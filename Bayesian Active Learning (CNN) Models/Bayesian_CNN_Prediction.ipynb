{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian CNN Prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV3SFE7R3CMW",
        "colab_type": "code",
        "outputId": "d4b1bc3c-6d16-46a7-9991-c359a400f2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCXZmlJ-2j5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adadelta, Adagrad, Adam\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from keras import backend as K\n",
        "#from tensorflow.keras import backend as k\n",
        "#from tensorflow.python.keras import backend as k\n",
        "from keras.layers.core import Lambda\n",
        "from six.moves import range\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "#from keras import backend as K\n",
        "import random\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.regularizers import l2\n",
        "import math\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "import pprint\n",
        "from six.moves import range\n",
        "import six\n",
        "import time\n",
        "import os\n",
        "import threading\n",
        "from tensorflow.python.framework import ops\n",
        "try:\n",
        "    import queue\n",
        "except ImportError:\n",
        "    import Queue as queue\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "#Importing 5' Data\n",
        "# Imports and functions from 5' Model\n",
        "#%matplotlib inline\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, Conv1D, SpatialDropout1D, MaxPooling1D, MaxPooling2D,Input, Flatten\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import six\n",
        "\n",
        "nuc_arr = ['A','C','G','T']\n",
        "#Function for calculating modified probability of splicing at SD1\n",
        "def prob_SD1 (sd1_freq, sd2_freq):\n",
        "    if (sd1_freq==0 and sd2_freq==0):\n",
        "        return 0.0\n",
        "    else:\n",
        "        return sd1_freq/(sd1_freq+sd2_freq)\n",
        "#Function converting nucleotide sequence to numerical array with 4 channels\n",
        "def seq_to_arr (seq):\n",
        "    seq_len = len(seq)\n",
        "    arr_rep = np.zeros((seq_len, len(nuc_arr)))\n",
        "    for i in range(seq_len):\n",
        "        arr_rep[i][nuc_arr.index(seq[i])] = 1\n",
        "    return arr_rep\n",
        "\n",
        "#Creating a modified dataset with only the necessary information\n",
        "#Storing model inputs and outputs\n",
        "reads_path = \"/content/gdrive/My Drive/Active Learning Research 2019/GSM1911086_A5SS_spliced_reads.txt\"\n",
        "seq_path = \"/content/gdrive/My Drive/Active Learning Research 2019/GSM1911085_A5SS_seq.txt\"\n",
        "s1_indx = 1\n",
        "s2_indx= 45\n",
        "seq_len = 101\n",
        "read_lines = []\n",
        "seq_lines = []\n",
        "data_table = []\n",
        "with open(reads_path) as f:\n",
        "    f.readline()\n",
        "    for line in f:\n",
        "        mod_line = line.split('\\t')\n",
        "        read_lines.append([mod_line[0], mod_line[s1_indx], mod_line[s2_indx]])\n",
        "with open(seq_path) as f:\n",
        "    f.readline()\n",
        "    for line in f:\n",
        "        mod_line = line.split('\\t')\n",
        "        seq_lines.append([mod_line[0], mod_line[1][:-1]])\n",
        "\n",
        "n = len(read_lines)\n",
        "prob_s1 = np.zeros(n)\n",
        "inputs = np.zeros((n,seq_len, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    prob_s1[i] = prob_SD1(float(read_lines[i][1]), float(read_lines[i][2]))\n",
        "    inputs[i] = seq_to_arr(seq_lines[i][1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzzxcs17zqVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8c7c5e91-c7ac-4bb3-d98c-0d45d0bb2e03"
      },
      "source": [
        "def permanent_dropout_layer(rate):\n",
        "    seed = np.random.randint(0,100)\n",
        "    return Lambda(lambda x: K.dropout(x,level=rate,seed=seed))\n",
        "\"\"\"\n",
        "def standardize_X(X):\n",
        "    if type(X) == list:\n",
        "        return X\n",
        "    else:\n",
        "        return [X]\n",
        "\n",
        "def permanent_spatial_dropout_layer(rate, noiseshape):\n",
        "    seed = np.random.randint(0,100)\n",
        "    return Lambda(lambda x: K.dropout(x,level=rate,noise_shape=noiseshape,seed=seed))\n",
        "    \n",
        "\n",
        "def predict_stochastic(model, X, batch_size=128, verbose=0):\n",
        "    '''Generate output predictions for the input samples\n",
        "    batch by batch.\n",
        "    # Arguments\n",
        "        X: the input data, as a numpy array.\n",
        "        batch_size: integer.\n",
        "        verbose: verbosity mode, 0 or 1.f\n",
        "    # Returns\n",
        "        A numpy array of predictions.\n",
        "    '''\n",
        "    X = standardize_X(X)\n",
        "    pred_Y = model.predict(X,batch_size=batch_size, verbose=verbose)\n",
        "    return np.array(pred_Y)\n",
        "\"\"\""
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef standardize_X(X):\\n    if type(X) == list:\\n        return X\\n    else:\\n        return [X]\\n\\ndef permanent_spatial_dropout_layer(rate, noiseshape):\\n    seed = np.random.randint(0,100)\\n    return Lambda(lambda x: K.dropout(x,level=rate,noise_shape=noiseshape,seed=seed))\\n    \\n\\ndef predict_stochastic(model, X, batch_size=128, verbose=0):\\n    '''Generate output predictions for the input samples\\n    batch by batch.\\n    # Arguments\\n        X: the input data, as a numpy array.\\n        batch_size: integer.\\n        verbose: verbosity mode, 0 or 1.f\\n    # Returns\\n        A numpy array of predictions.\\n    '''\\n    X = standardize_X(X)\\n    pred_Y = model.predict(X,batch_size=batch_size, verbose=verbose)\\n    return np.array(pred_Y)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za5uWd8Z0ADT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "Queries = 1\n",
        "\n",
        "all_rmse_rand = 0\n",
        "all_rmse_bald = 0\n",
        "batch_size = 256\n",
        "dropout_iterations = 100\n",
        "size_train = 768\n",
        "X = np.array(inputs)\n",
        "y = np.array(prob_s1)\n",
        "all_train_indices = np.load('/content/gdrive/My Drive/Active Learning Research 2019/Train-Test-Splits/trainindices1.npy')\n",
        "train_data_indices = all_train_indices[0:size_train].tolist()\n",
        "test_indices = np.load('/content/gdrive/My Drive/Active Learning Research 2019/Train-Test-Splits/testindices1.npy')\n",
        "X_train = X[train_data_indices]\n",
        "y_train = y[train_data_indices]\n",
        "X_test = X[test_indices]\n",
        "y_test = y[test_indices]\n",
        "\n",
        "#X_pool = X[ index_pool, :  ]\n",
        "#y_pool = y[ index_pool ]\n",
        "#normalise the dataset - X_train, y_train and X_test\n",
        "std_X_train = np.std(X_train, 0)\n",
        "std_X_train[ std_X_train == 0  ] = 1\n",
        "mean_X_train = np.mean(X_train, 0)\n",
        "mean_y_train = np.mean(y_train)\n",
        "std_y_train = np.std(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI0-qGxa1nTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "from keras import initializers\n",
        "from keras.engine import InputSpec\n",
        "from keras.layers import Dense, Lambda, Wrapper\n",
        "\n",
        "#From Concrete Dropout Paper, on GitHub\n",
        "class ConcreteDropout(Wrapper):\n",
        "    \"\"\"This wrapper allows to learn the dropout probability for any given input Dense layer.\n",
        "    ```python\n",
        "        # as the first layer in a model\n",
        "        model = Sequential()\n",
        "        model.add(ConcreteDropout(Dense(8), input_shape=(16)))\n",
        "        # now model.output_shape == (None, 8)\n",
        "        # subsequent layers: no need for input_shape\n",
        "        model.add(ConcreteDropout(Dense(32)))\n",
        "        # now model.output_shape == (None, 32)\n",
        "    \n",
        "    `ConcreteDropout` can be used with arbitrary layers which have 2D\n",
        "    kernels, not just `Dense`. However, Conv2D layers require different\n",
        "    weighing of the regulariser (use SpatialConcreteDropout instead).\n",
        "    # Arguments\n",
        "        layer: a layer instance.\n",
        "        weight_regularizer:\n",
        "            A positive number which satisfies\n",
        "                $weight_regularizer = l**2 / (\\tau * N)$\n",
        "            with prior lengthscale l, model precision $\\tau$ (inverse observation noise),\n",
        "            and N the number of instances in the dataset.\n",
        "            Note that kernel_regularizer is not needed.\n",
        "        dropout_regularizer:\n",
        "            A positive number which satisfies\n",
        "                $dropout_regularizer = 2 / (\\tau * N)$\n",
        "            with model precision $\\tau$ (inverse observation noise) and N the number of\n",
        "            instances in the dataset.\n",
        "            Note the relation between dropout_regularizer and weight_regularizer:\n",
        "                $weight_regularizer / dropout_regularizer = l**2 / 2$\n",
        "            with prior lengthscale l. Note also that the factor of two should be\n",
        "            ignored for cross-entropy loss, and used only for the eculedian loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, weight_regularizer=1e-6, dropout_regularizer=1e-5,\n",
        "                 init_min=0.1, init_max=0.1, is_mc_dropout=True, **kwargs):\n",
        "        assert 'kernel_regularizer' not in kwargs\n",
        "        super(ConcreteDropout, self).__init__(layer, **kwargs)\n",
        "        self.weight_regularizer = weight_regularizer\n",
        "        self.dropout_regularizer = dropout_regularizer\n",
        "        self.is_mc_dropout = is_mc_dropout\n",
        "        self.supports_masking = True\n",
        "        self.p_logit = None\n",
        "        self.p = None\n",
        "        self.init_min = np.log(init_min) - np.log(1. - init_min)\n",
        "        self.init_max = np.log(init_max) - np.log(1. - init_max)\n",
        "\n",
        "    def build(self, input_shape=None):\n",
        "        self.input_spec = InputSpec(shape=input_shape)\n",
        "        if not self.layer.built:\n",
        "            self.layer.build(input_shape)\n",
        "            self.layer.built = True\n",
        "        super(ConcreteDropout, self).build()  # this is very weird.. we must call super before we add new losses\n",
        "\n",
        "        # initialise p\n",
        "        self.p_logit = self.layer.add_weight(name='p_logit',\n",
        "                                            shape=(1,),\n",
        "                                            initializer=initializers.RandomUniform(self.init_min, self.init_max),\n",
        "                                            trainable=True)\n",
        "        self.p = K.sigmoid(self.p_logit[0])\n",
        "\n",
        "        # initialise regulariser / prior KL term\n",
        "        assert len(input_shape) == 2, 'this wrapper only supports Dense layers'\n",
        "        input_dim = np.prod(input_shape[-1])  # we drop only last dim\n",
        "        weight = self.layer.kernel\n",
        "        kernel_regularizer = self.weight_regularizer * K.sum(K.square(weight)) / (1. - self.p)\n",
        "        dropout_regularizer = self.p * K.log(self.p)\n",
        "        dropout_regularizer += (1. - self.p) * K.log(1. - self.p)\n",
        "        dropout_regularizer *= self.dropout_regularizer * input_dim\n",
        "        regularizer = K.sum(kernel_regularizer + dropout_regularizer)\n",
        "        self.layer.add_loss(regularizer)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.layer.compute_output_shape(input_shape)\n",
        "\n",
        "    def concrete_dropout(self, x):\n",
        "        '''\n",
        "        Concrete dropout - used at training time (gradients can be propagated)\n",
        "        :param x: input\n",
        "        :return:  approx. dropped out input\n",
        "        '''\n",
        "        eps = K.cast_to_floatx(K.epsilon())\n",
        "        temp = 0.1\n",
        "\n",
        "        unif_noise = K.random_uniform(shape=K.shape(x))\n",
        "        drop_prob = (\n",
        "            K.log(self.p + eps)\n",
        "            - K.log(1. - self.p + eps)\n",
        "            + K.log(unif_noise + eps)\n",
        "            - K.log(1. - unif_noise + eps)\n",
        "        )\n",
        "        drop_prob = K.sigmoid(drop_prob / temp)\n",
        "        random_tensor = 1. - drop_prob\n",
        "\n",
        "        retain_prob = 1. - self.p\n",
        "        x *= random_tensor\n",
        "        x /= retain_prob\n",
        "        return x\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if self.is_mc_dropout:\n",
        "            return self.layer.call(self.concrete_dropout(inputs))\n",
        "        else:\n",
        "            def relaxed_dropped_inputs():\n",
        "                return self.layer.call(self.concrete_dropout(inputs))\n",
        "            return K.in_train_phase(relaxed_dropped_inputs,\n",
        "                                    self.layer.call(inputs),\n",
        "                                    training=training)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5Z2sC4t90fP",
        "colab_type": "code",
        "outputId": "da4f1e0c-b4f8-4c9a-d809-d0a856d764d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "\n",
        "#Original model, without spatial or concrete dropout\n",
        "\n",
        "tau = 0.159708\n",
        "N = X_train.shape[0]\n",
        "dropout = 0.05\n",
        "lengthscale = 1e2\n",
        "Weight_Decay = (1 - dropout) / (2. * N * lengthscale**2 * tau)\n",
        "model = Sequential()\n",
        "model.add(Conv1D(seq_len,(4), strides=1, input_shape=(seq_len,len(nuc_arr)), activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(permanent_dropout_layer(dropout))\n",
        "model.add(Conv1D(seq_len//2, (4), strides=1, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(Flatten())\n",
        "model.add(permanent_dropout_layer(dropout))\n",
        "model.add(Dense(50, W_regularizer=l2(Weight_Decay), init='normal',  activation='relu'))\n",
        "model.add(permanent_dropout_layer(dropout))\n",
        "model.add(Dense(1, W_regularizer=l2(Weight_Decay), init='normal'))\n",
        "\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "\n",
        "#for layer in model.layers:\n",
        "#    print(layer.get_output_at(0).get_shape().as_list())\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=0)\n",
        "All_Dropout_Scores = np.zeros(shape=(X_test.shape[0],1))\n",
        "y_predicted = np.zeros(shape=(All_Dropout_Scores.shape[0]))\n",
        "for d in range(dropout_iterations):\n",
        "    dropout_score = predict_stochastic(model, X_test,batch_size=batch_size, verbose=0)\n",
        "    All_Dropout_Scores = np.append(All_Dropout_Scores,dropout_score,axis=1)\n",
        "for j in range(All_Dropout_Scores.shape[0]):\n",
        "    P_temp = All_Dropout_Scores[j,:]\n",
        "    P = np.delete(P_temp,0)\n",
        "    P_mean = np.mean(P)\n",
        "    y_predicted[j] = P_mean\n",
        "mse = np.mean((y_test - y_predicted)**2)\n",
        "print('r2 of test predictions: ' + str(r2_score(y_test, y_predicted)))\n",
        "print('Mean Squared Error: ' + str(mse))\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "r2 of test predictions: 0.4677616846996213\n",
            "Mean Squared Error: 0.07593692326508099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY9NRJRRGI-B",
        "colab_type": "code",
        "outputId": "67a46f2c-4049-469c-f5d6-ba32278f17f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#Version with Spatial Dropout after Conv Layers, but no Concrete Dropout\n",
        "\n",
        "import tensorflow as tf \n",
        "from keras.models import Model\n",
        "\n",
        "tau = 0.159708\n",
        "batchsize = 256\n",
        "N = X_train.shape[0]\n",
        "dropout = 0.1\n",
        "lengthscale = 1e2\n",
        "Weight_Decay = (1 - dropout) / (2. * N * lengthscale**2 * tau)\n",
        "inputlayer = Input(shape=(seq_len, len(nuc_arr)))\n",
        "x = Conv1D(seq_len,(4), strides=1, input_shape=(seq_len,len(nuc_arr)), activation='relu')(inputlayer)\n",
        "x = SpatialDropout1D(dropout)(x)\n",
        "x = MaxPooling1D(pool_size=3)(x)\n",
        "x = permanent_dropout_layer(dropout)(x)\n",
        "x = Conv1D(seq_len//2, (4), strides=1, activation='relu')(x)\n",
        "x = SpatialDropout1D(dropout)(x)\n",
        "x = MaxPooling1D(pool_size=3)(x)\n",
        "x = Flatten()(x)\n",
        "x = permanent_dropout_layer(dropout)(x)\n",
        "#x = Dense(50, activation='relu', kernel_initializer='random_normal', kernel_regularizer=keras.regularizers.l2(Weight_Decay))(x)\n",
        "x = Dense(50, W_regularizer=l2(Weight_Decay), init='normal',  activation='relu')(x)\n",
        "x = permanent_dropout_layer(dropout)(x)\n",
        "#outputlayer = Dense(1, kernel_initializer='random_normal',kernel_regularizer = keras.regularizers.l2(Weight_Decay))(x)\n",
        "outputlayer = Dense(1, W_regularizer=l2(Weight_Decay), init='normal')(x)\n",
        "\n",
        "model = Model(inputlayer, outputlayer)\n",
        "\n",
        "\n",
        "# for some model with dropout ...\n",
        "f = K.function([model.layers[0].input, K.learning_phase()],\n",
        "               [model.layers[-1].output])\n",
        "\n",
        "def predict_with_uncertainty(f, x, n_iter=100):\n",
        "    result = np.zeros((n_iter,x.shape[0]))\n",
        "    for i in range(n_iter):\n",
        "        predictions = np.array(f((x, 1))[0])[:,0]\n",
        "        result[i,:] = predictions\n",
        "    prediction = result.mean(axis=0)\n",
        "    uncertainty = result.std(axis=0)\n",
        "    return prediction, uncertainty    \n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=batchsize, verbose=0)\n",
        "predictions_with_uncertainty = predict_with_uncertainty(f, X_test, n_iter=100)\n",
        "y_predicted = predictions_with_uncertainty[0]\n",
        "y_uncertainty = predictions_with_uncertainty[1]\n",
        "mse = np.mean((y_test - y_predicted)**2)\n",
        "print('r2 of test predictions: ' + str(r2_score(y_test, y_predicted)))\n",
        "print('Mean Squared Error: ' + str(mse))\n",
        "print('uncertainties (std): ' + str(y_uncertainty))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "r2 of test predictions: 0.320351977765689\n",
            "Mean Squared Error: 0.09696855380008416\n",
            "uncertainties (std): [0.11393899 0.12971005 0.07174659 ... 0.11120183 0.04560007 0.02606147]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoAesYwrBeTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6c7e2043-7d41-468e-9d79-27ee55aa6481"
      },
      "source": [
        "#Version with Spatial and Concrete Dropout\n",
        "\n",
        "import tensorflow as tf \n",
        "from keras.models import Model\n",
        "\n",
        "tau = 0.159708\n",
        "batchsize = 128\n",
        "N = X_train.shape[0]\n",
        "dropout = 0.1\n",
        "lengthscale = 1e2\n",
        "Weight_Decay = (1 - dropout) / (2. * N * lengthscale**2 * tau)\n",
        "inputlayer = Input(shape=(seq_len, len(nuc_arr)))\n",
        "x = Conv1D(seq_len,(4), strides=1, input_shape=(seq_len,len(nuc_arr)), activation='relu')(inputlayer)\n",
        "x = SpatialDropout1D(dropout)(x)\n",
        "x = MaxPooling1D(pool_size=3)(x)\n",
        "x = permanent_dropout_layer(dropout)(x)\n",
        "x = Conv1D(seq_len//2, (4), strides=1, activation='relu')(x)\n",
        "x = SpatialDropout1D(dropout)(x)\n",
        "x = MaxPooling1D(pool_size=3)(x)\n",
        "x = Flatten()(x)\n",
        "x = permanent_dropout_layer(dropout)(x)\n",
        "x = ConcreteDropout(Dense(50, W_regularizer=l2(Weight_Decay), init='normal',  activation='relu'), input_shape=(450,))(x)\n",
        "outputlayer = ConcreteDropout(Dense(1, W_regularizer=l2(Weight_Decay), init='normal'), input_shape=(50,))(x)\n",
        "model = Model(inputlayer, outputlayer)\n",
        "\n",
        "# for some model with dropout ...\n",
        "f = K.function([model.layers[0].input, K.learning_phase()],\n",
        "               [model.layers[-1].output])\n",
        "\n",
        "def predict_with_uncertainty(f, x, n_iter=100):\n",
        "    result = np.zeros((n_iter,x.shape[0]))\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        predictions = np.array(f((x, 1))[0])[:,0]\n",
        "        result[i,:] = predictions\n",
        "    prediction = result.mean(axis=0)\n",
        "    uncertainty = result.std(axis=0)\n",
        "    return prediction, uncertainty    \n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=batchsize, verbose=0)\n",
        "predictions_with_uncertainty = predict_with_uncertainty(f, X_test, n_iter=100)\n",
        "y_predicted = predictions_with_uncertainty[0]\n",
        "y_uncertainty = predictions_with_uncertainty[1]\n",
        "mse = np.mean((y_test - y_predicted)**2)\n",
        "print('r2 of test predictions: ' + str(r2_score(y_test, y_predicted)))\n",
        "print('Mean Squared Error: ' + str(mse))\n",
        "print('uncertainties (std): ' + str(y_uncertainty))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "r2 of test predictions: 0.4786600685352127\n",
            "Mean Squared Error: 0.07438200000374148\n",
            "uncertainties (std): [0.19067245 0.14921673 0.12157874 ... 0.13805234 0.09350427 0.032944  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j-e2nVsrLTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
