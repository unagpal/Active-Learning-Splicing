{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian CNN Prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV3SFE7R3CMW",
        "colab_type": "code",
        "outputId": "fc9dcc29-b1d6-4727-f078-baced75bf46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCXZmlJ-2j5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00837aa5-e5b8-4e1c-dcfd-2f476253f3ac"
      },
      "source": [
        "#Imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adadelta, Adagrad, Adam\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from keras import backend as K\n",
        "#from tensorflow.keras import backend as k\n",
        "#from tensorflow.python.keras import backend as k\n",
        "from keras.layers.core import Lambda\n",
        "from six.moves import range\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "#from keras import backend as K\n",
        "import random\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.regularizers import l2\n",
        "import math\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "import pprint\n",
        "from six.moves import range\n",
        "import six\n",
        "import time\n",
        "import os\n",
        "import threading\n",
        "from tensorflow.python.framework import ops\n",
        "try:\n",
        "    import queue\n",
        "except ImportError:\n",
        "    import Queue as queue\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "#Importing 5' Data\n",
        "# Imports and functions from 5' Model\n",
        "#%matplotlib inline\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,Input, Flatten\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import six\n",
        "\n",
        "nuc_arr = ['A','C','G','T']\n",
        "#Function for calculating modified probability of splicing at SD1\n",
        "def prob_SD1 (sd1_freq, sd2_freq):\n",
        "    if (sd1_freq==0 and sd2_freq==0):\n",
        "        return 0.0\n",
        "    else:\n",
        "        return sd1_freq/(sd1_freq+sd2_freq)\n",
        "#Function converting nucleotide sequence to numerical array with 4 channels\n",
        "def seq_to_arr (seq):\n",
        "    seq_len = len(seq)\n",
        "    arr_rep = np.zeros((seq_len, len(nuc_arr)))\n",
        "    for i in range(seq_len):\n",
        "        arr_rep[i][nuc_arr.index(seq[i])] = 1\n",
        "    return arr_rep\n",
        "\n",
        "#Creating a modified dataset with only the necessary information\n",
        "#Storing model inputs and outputs\n",
        "reads_path = \"/content/gdrive/My Drive/Active Learning Research 2019/GSM1911086_A5SS_spliced_reads.txt\"\n",
        "seq_path = \"/content/gdrive/My Drive/Active Learning Research 2019/GSM1911085_A5SS_seq.txt\"\n",
        "s1_indx = 1\n",
        "s2_indx= 45\n",
        "seq_len = 101\n",
        "read_lines = []\n",
        "seq_lines = []\n",
        "data_table = []\n",
        "with open(reads_path) as f:\n",
        "    f.readline()\n",
        "    for line in f:\n",
        "        mod_line = line.split('\\t')\n",
        "        read_lines.append([mod_line[0], mod_line[s1_indx], mod_line[s2_indx]])\n",
        "with open(seq_path) as f:\n",
        "    f.readline()\n",
        "    for line in f:\n",
        "        mod_line = line.split('\\t')\n",
        "        seq_lines.append([mod_line[0], mod_line[1][:-1]])\n",
        "\n",
        "n = len(read_lines)\n",
        "prob_s1 = np.zeros(n)\n",
        "inputs = np.zeros((n,seq_len, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    prob_s1[i] = prob_SD1(float(read_lines[i][1]), float(read_lines[i][2]))\n",
        "    inputs[i] = seq_to_arr(seq_lines[i][1])\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzzxcs17zqVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standardize_X(X):\n",
        "    if type(X) == list:\n",
        "        return X\n",
        "    else:\n",
        "        return [X]\n",
        "      \n",
        "def permanent_dropout_layer(rate, seed):\n",
        "    return Lambda(lambda x: K.dropout(x,level=rate,seed=seed))\n",
        "\n",
        "def predict_stochastic(model, X, batch_size=128, verbose=0):\n",
        "    '''Generate output predictions for the input samples\n",
        "    batch by batch.\n",
        "    # Arguments\n",
        "        X: the input data, as a numpy array.\n",
        "        batch_size: integer.\n",
        "        verbose: verbosity mode, 0 or 1.f\n",
        "    # Returns\n",
        "        A numpy array of predictions.\n",
        "    '''\n",
        "    X = standardize_X(X)\n",
        "    pred_Y = model.predict(X,batch_size=batch_size, verbose=verbose)\n",
        "    return np.array(pred_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za5uWd8Z0ADT",
        "colab_type": "code",
        "outputId": "230186a7-151b-46fe-e04a-b378cc9b3432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "Queries = 1\n",
        "\n",
        "all_rmse_rand = 0\n",
        "all_rmse_bald = 0\n",
        "batch_size = 256\n",
        "dropout_iterations = 100\n",
        "X = np.array(inputs)\n",
        "y = np.array(prob_s1)\n",
        "all_train_indices = np.load('/content/gdrive/My Drive/Active Learning Research 2019/Train-Test-Splits/trainindices1.npy')\n",
        "size_train = 1000\n",
        "train_data_indices = all_train_indices[0:size_train].tolist()\n",
        "test_indices = np.load('/content/gdrive/My Drive/Active Learning Research 2019/Train-Test-Splits/testindices1.npy')\n",
        "X_train = X[train_data_indices]\n",
        "y_train = y[train_data_indices]\n",
        "X_test = X[test_indices]\n",
        "y_test = y[test_indices]\n",
        "\n",
        "#X_pool = X[ index_pool, :  ]\n",
        "#y_pool = y[ index_pool ]\n",
        "#normalise the dataset - X_train, y_train and X_test\n",
        "std_X_train = np.std(X_train, 0)\n",
        "std_X_train[ std_X_train == 0  ] = 1\n",
        "mean_X_train = np.mean(X_train, 0)\n",
        "mean_y_train = np.mean(y_train)\n",
        "std_y_train = np.std(y_train)\n",
        "\n",
        "\n",
        "#X_test = (X_test - mean_X_train) / std_X_train\n",
        "\n",
        "tau = 0.159708\n",
        "N = X_train.shape[0]\n",
        "dropout = 0.05\n",
        "seed = 38\n",
        "lengthscale = 1e2\n",
        "Weight_Decay = (1 - dropout) / (2. * N * lengthscale**2 * tau)\n",
        "model = Sequential()\n",
        "model.add(Conv1D(seq_len,(4), strides=1, input_shape=(seq_len,len(nuc_arr)), activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(permanent_dropout_layer(dropout,seed))\n",
        "model.add(Conv1D(seq_len//2, (4), strides=1, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(Flatten())\n",
        "model.add(permanent_dropout_layer(dropout,seed))\n",
        "model.add(Dense(50, W_regularizer=l2(Weight_Decay), init='normal',  activation='relu'))\n",
        "model.add(permanent_dropout_layer(dropout,seed))\n",
        "model.add(Dense(1, W_regularizer=l2(Weight_Decay), init='normal'))\n",
        "\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=0)\n",
        "All_Dropout_Scores = np.zeros(shape=(X_test.shape[0],1))\n",
        "y_predicted = np.zeros(shape=(All_Dropout_Scores.shape[0]))\n",
        "for d in range(dropout_iterations):\n",
        "    dropout_score = predict_stochastic(model, X_test,batch_size=batch_size, verbose=0)\n",
        "    All_Dropout_Scores = np.append(All_Dropout_Scores,dropout_score,axis=1)\n",
        "for j in range(All_Dropout_Scores.shape[0]):\n",
        "    P_temp = All_Dropout_Scores[j,:]\n",
        "    P = np.delete(P_temp,0)\n",
        "    P_mean = np.mean(P)\n",
        "    y_predicted[j] = P_mean\n",
        "mse = np.mean((y_test - y_predicted)**2)\n",
        "print('r2 of test predictions: ' + str(r2_score(y_test, y_predicted)))\n",
        "print('Mean Squared Error: ' + str(mse))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "r2 of test predictions: 0.49131683092128275\n",
            "Mean Squared Error: 0.07257620067200239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI0-qGxa1nTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}