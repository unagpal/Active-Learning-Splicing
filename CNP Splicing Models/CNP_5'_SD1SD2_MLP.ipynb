{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNP_5'_SD1SD2_MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4mBSzbcRdz5",
        "colab_type": "code",
        "outputId": "c8ca9205-721e-4af1-c3c8-f5eefce6a812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LdXHOzdSF8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "%matplotlib inline\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6BPpGE6TTon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports and functions from 5' Model\n",
        "%matplotlib inline\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D,Input, Flatten\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "nuc_arr = ['A','C','G','T']\n",
        "#Function for calculating modified probability of splicing at SD1\n",
        "def prob_SD1 (sd1_freq, sd2_freq): \n",
        "    if (sd1_freq==0 and sd2_freq==0):\n",
        "        return 0.0 \n",
        "    else:\n",
        "        return sd1_freq/(sd1_freq+sd2_freq)\n",
        "#Function converting nucleotide sequence to numerical array with 4 channels\n",
        "def seq_to_arr (seq):\n",
        "    seq_len = len(seq)\n",
        "    arr_rep = np.zeros((seq_len, len(nuc_arr))) \n",
        "    for i in range(seq_len):\n",
        "        arr_rep[i][nuc_arr.index(seq[i])] = 1 \n",
        "    return arr_rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_qGRzOmTWfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating a modified dataset with only the necessary information\n",
        "#Storing model inputs and outputs\n",
        "reads_path = \"/content/gdrive/My Drive/Active Learning Research 2019/GSM1911086_A5SS_spliced_reads.txt\"\n",
        "seq_path = \"/content/gdrive/My Drive/Active Learning Research 2019/GSM1911085_A5SS_seq.txt\"\n",
        "s1_indx = 1\n",
        "s2_indx= 45\n",
        "seq_len = 101\n",
        "read_lines = []\n",
        "seq_lines = []\n",
        "data_table = []\n",
        "with open(reads_path) as f:\n",
        "    f.readline() \n",
        "    for line in f:\n",
        "        mod_line = line.split('\\t')\n",
        "        read_lines.append([mod_line[0], mod_line[s1_indx], mod_line[s2_indx]])\n",
        "with open(seq_path) as f:\n",
        "    f.readline() \n",
        "    for line in f:\n",
        "        mod_line = line.split('\\t') \n",
        "        seq_lines.append([mod_line[0], mod_line[1][:-1]])\n",
        "        \n",
        "n = len(read_lines)\n",
        "prob_s1 = np.zeros(n)\n",
        "inputs = np.zeros((n,seq_len, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    prob_s1[i] = prob_SD1(float(read_lines[i][1]), float(read_lines[i][2]))\n",
        "    inputs[i] = seq_to_arr(seq_lines[i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UARyXCPYZDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The CNP takes as input a `CNPRegressionDescription` namedtuple with fields:\n",
        "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
        "#   `target_y`: a tesor containing the ground truth for the targets to be\n",
        "#     predicted\n",
        "#   `num_total_points`: A vector containing a scalar that describes the total\n",
        "#     number of datapoints used (context + target)\n",
        "#   `num_context_points`: A vector containing a scalar that describes the number\n",
        "#     of datapoints used as context\n",
        "# The GPCurvesReader returns the newly sampled data in this format at each\n",
        "# iteration\n",
        "\n",
        "CNPRegressionDescription = collections.namedtuple(\n",
        "    \"CNPRegressionDescription\",\n",
        "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
        "\n",
        "\n",
        "class GPCurvesReader(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                batch_size,\n",
        "                max_num_context,\n",
        "                x_train,\n",
        "                x_test,\n",
        "                y_train,\n",
        "                y_test,\n",
        "                x_size=404,\n",
        "                y_size=1,\n",
        "                l1_scale=0.4,\n",
        "                sigma_scale=1.0,\n",
        "                testing=False):\n",
        "\n",
        "        self._batch_size = batch_size\n",
        "        self._max_num_context = max_num_context\n",
        "        self._x_size = x_size\n",
        "        self._y_size = y_size\n",
        "        self._x_train = x_train\n",
        "        self._x_test = x_test\n",
        "        self._y_train = y_train\n",
        "        self._y_test = y_test\n",
        "        self._l1_scale = l1_scale\n",
        "        self._sigma_scale = sigma_scale\n",
        "        self._testing = testing\n",
        "\n",
        "    def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
        "\n",
        "        num_total_points = tf.shape(xdata)[1]\n",
        "\n",
        "        # Expand and take the difference\n",
        "        xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
        "        xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
        "        diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
        "\n",
        "        # [B, y_size, num_total_points, num_total_points, x_size]\n",
        "        norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
        "\n",
        "        norm = tf.reduce_sum(\n",
        "            norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
        "\n",
        "        # [B, y_size, num_total_points, num_total_points]\n",
        "        kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
        "\n",
        "        # Add some noise to the diagonal to make the cholesky work.\n",
        "        kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
        "\n",
        "        return kernel\n",
        "    def generate_curves(self):\n",
        "        \"\"\"Builds the op delivering the data.\n",
        "\n",
        "        Generated functions are `float32` with x values between -2 and 2.\n",
        "\n",
        "        Returns:\n",
        "          A `CNPRegressionDescription` namedtuple.\n",
        "        \"\"\"\n",
        "        \n",
        "        #print(num_context)\n",
        "        # If we are testing we want to have more targets and have them evenly\n",
        "        # distributed in order to plot the function.\n",
        "        # When testing, we want context points randomly sampled from training data\n",
        "        # and target points randomly sampled from testing data\n",
        "        if self._testing:\n",
        "            num_testing_points = tf.Tensor.get_shape(self._y_test)[0]\n",
        "            num_training_points = tf.Tensor.get_shape(self._y_train)[0]\n",
        "            num_context = num_training_points\n",
        "            # Select the targets (all training data)\n",
        "            indices_test = np.arange(int(num_testing_points))\n",
        "            target_x_values = [tf.gather(self._x_test, tf.convert_to_tensor(indices_test))]\n",
        "            target_y_values = [tf.gather(self._y_test, tf.convert_to_tensor(indices_test))]\n",
        "            target_x = tf.stack(target_x_values)\n",
        "            target_y_values = tf.stack(target_y_values)\n",
        "            target_y = target_y_values\n",
        "            #target_y = tf.expand_dims(target_y_values, axis=2)\n",
        "            \n",
        "            # Select the observations (all testing data)\n",
        "            indices_train = np.arange(int(num_training_points))\n",
        "            context_x_values = [tf.gather(self._x_train, tf.convert_to_tensor(indices_train))]\n",
        "            context_y_values = [tf.gather(self._y_train, tf.convert_to_tensor(indices_train))]\n",
        "            context_x = tf.stack(context_x_values)\n",
        "            context_y_values = tf.stack(context_y_values)\n",
        "            context_y = context_y_values\n",
        "            #context_y = tf.expand_dims(context_y_values, axis=2)\n",
        "            \n",
        "            \n",
        "            #context_x = tf.gather(x_values, idx[:int(num_context)], axis=1)\n",
        "            #context_y = tf.gather(y_values, idx[:int(num_context)], axis=1)\n",
        "        # During training the number of target points and their x-positions are\n",
        "        # selected at random\n",
        "        else:\n",
        "            num_context = self._max_num_context\n",
        "            num_target = self._max_num_context\n",
        "            num_total_points = int(num_context + num_target)\n",
        "            x_values = []\n",
        "            y_values = []\n",
        "            num_training_points = tf.Tensor.get_shape(self._y_train)[0]\n",
        "            #print(num_context)\n",
        "            #print(num_target)\n",
        "            #print(num_total_points)\n",
        "            for batch_num in range(self._batch_size):\n",
        "                indices = np.random.choice(num_training_points,num_total_points,replace=False)\n",
        "                x_values_batch = tf.gather(self._x_train, tf.convert_to_tensor(indices))\n",
        "                y_values_batch = tf.gather(self._y_train, tf.convert_to_tensor(indices))\n",
        "                x_values.append(x_values_batch)\n",
        "                y_values.append(y_values_batch)\n",
        "            x_values = tf.stack(x_values)\n",
        "            y_values = tf.stack(y_values)\n",
        "            #y_values = tf.expand_dims(y_values, axis=2)\n",
        "            #print(tf.shape(y_values))\n",
        "            #x_values = tf.random_uniform(\n",
        "            #  [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
        "            # Select the targets which will consist of the context points as well as\n",
        "            # some new target points\n",
        "            target_x = x_values[:, :num_total_points, :]\n",
        "            target_y = y_values[:, :num_total_points, :]\n",
        "            # Select the observations\n",
        "            context_x = x_values[:, :int(num_context), :]\n",
        "            context_y = y_values[:, :int(num_context), :]\n",
        "\n",
        "        query = ((context_x, context_y), target_x)\n",
        "        return CNPRegressionDescription(\n",
        "            query=query,\n",
        "            target_y=target_y,\n",
        "            num_total_points=tf.shape(target_x)[1],\n",
        "            num_context_points=num_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHpGDucHagqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicEncoder(object):\n",
        "  \"\"\"The Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes):\n",
        "    \"\"\"CNP encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "\n",
        "  def __call__(self, context_x, context_y, num_context_points):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      context_x: Tensor of size bs x observations x m_ch. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      context_y: Tensor of size bs x observations x d_ch. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "      num_context_points: A tensor containing a single scalar that indicates the\n",
        "          number of context_points provided in this iteration.\n",
        "\n",
        "    Returns:\n",
        "      representation: The encoded representation averaged over all context \n",
        "          points.\n",
        "    \"\"\"\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
        "    # Get the shapes of the input and reshape to parallelise across observations\n",
        "    batch_size, _, filter_size = encoder_input.shape.as_list()\n",
        "    hidden = tf.reshape(encoder_input, (batch_size * int(num_context_points), -1))\n",
        "    hidden.set_shape((None, filter_size))\n",
        "    \n",
        "    # Pass through MLP\n",
        "    with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
        "      for i, size in enumerate(self._output_sizes[:-1]):\n",
        "        hidden = tf.nn.relu(\n",
        "            tf.layers.dense(hidden, size, name=\"Encoder_layer_{}\".format(i)))\n",
        "\n",
        "      # Last layer without a ReLu\n",
        "      hidden = tf.layers.dense(\n",
        "          hidden, self._output_sizes[-1], name=\"Encoder_layer_{}\".format(i + 1))\n",
        "\n",
        "    # Bring back into original shape\n",
        "    hidden = tf.reshape(hidden, (batch_size, int(num_context_points), size))\n",
        "\n",
        "    # Aggregator: take the mean over all points\n",
        "    representation = tf.reduce_mean(hidden, axis=1)\n",
        "    \n",
        "    return representation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrPLUZifcug5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicDecoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes):\n",
        "    \"\"\"CNP decoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
        "          as defined in `basic.Linear`.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "\n",
        "  def __call__(self, representation, target_x, num_total_points):\n",
        "    \"\"\"Decodes the individual targets.\n",
        "\n",
        "    Args:\n",
        "      representation: The encoded representation of the context\n",
        "      target_x: The x locations for the target query\n",
        "      num_total_points: The number of target points.\n",
        "\n",
        "    Returns:\n",
        "      dist: A multivariate Gaussian over the target points.\n",
        "      mu: The mean of the multivariate Gaussian.\n",
        "      sigma: The standard deviation of the multivariate Gaussian.\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate the representation and the target_x\n",
        "    representation = tf.tile(\n",
        "        tf.expand_dims(representation, axis=1), [1, num_total_points, 1])\n",
        "    input = tf.concat([representation, target_x], axis=-1)\n",
        "\n",
        "    # Get the shapes of the input and reshape to parallelise across observations\n",
        "    batch_size, _, filter_size = input.shape.as_list()\n",
        "    hidden = tf.reshape(input, (batch_size * num_total_points, -1))\n",
        "    hidden.set_shape((None, filter_size))\n",
        "\n",
        "    # Pass through MLP\n",
        "    with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
        "      for i, size in enumerate(self._output_sizes[:-1]):\n",
        "        hidden = tf.nn.relu(\n",
        "            tf.layers.dense(hidden, size, name=\"Decoder_layer_{}\".format(i)))\n",
        "\n",
        "      # Last layer without a ReLu\n",
        "      hidden = tf.layers.dense(\n",
        "          hidden, self._output_sizes[-1], name=\"Decoder_layer_{}\".format(i + 1))\n",
        "\n",
        "    # Bring back into original shape\n",
        "    hidden = tf.reshape(hidden, (batch_size, num_total_points, -1))\n",
        "\n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "\n",
        "    # Get the distribution\n",
        "    dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
        "        loc=mu, scale_diag=sigma)\n",
        "\n",
        "    return dist, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD2b4mRicwYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeterministicModel(object):\n",
        "  \"\"\"The CNP model.\"\"\"\n",
        "\n",
        "  def __init__(self, encoder_output_sizes, decoder_output_sizes):\n",
        "    \"\"\"Initialises the model.\n",
        "\n",
        "    Args:\n",
        "      encoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
        "          the encoder. The last one is the size of the representation r.\n",
        "      decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
        "          the decoder. The last element should correspond to the dimension of\n",
        "          the y * 2 (it encodes both mean and variance concatenated)\n",
        "    \"\"\"\n",
        "    self._encoder = DeterministicEncoder(encoder_output_sizes)\n",
        "    self._decoder = DeterministicDecoder(decoder_output_sizes)\n",
        "\n",
        "  def __call__(self, query, num_total_points, num_contexts, target_y=None):\n",
        "    \"\"\"Returns the predicted mean and variance at the target points.\n",
        "\n",
        "    Args:\n",
        "      query: Array containing ((context_x, context_y), target_x) where:\n",
        "          context_x: Array of shape batch_size x num_context x 1 contains the \n",
        "              x values of the context points.\n",
        "          context_y: Array of shape batch_size x num_context x 1 contains the \n",
        "              y values of the context points.\n",
        "          target_x: Array of shape batch_size x num_target x 1 contains the\n",
        "              x values of the target points.\n",
        "      target_y: The ground truth y values of the target y. An array of \n",
        "          shape batchsize x num_targets x 1.\n",
        "      num_total_points: Number of target points.\n",
        "\n",
        "    Returns:\n",
        "      log_p: The log_probability of the target_y given the predicted\n",
        "      distribution.\n",
        "      mu: The mean of the predicted distribution.\n",
        "      sigma: The variance of the predicted distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    (context_x, context_y), target_x = query\n",
        "\n",
        "    # Pass query through the encoder and the decoder\n",
        "    representation = self._encoder(context_x, context_y, num_contexts)\n",
        "    dist, mu, sigma = self._decoder(representation, target_x, num_total_points)\n",
        "\n",
        "    # If we want to calculate the log_prob for training we will make use of the\n",
        "    # target_y. At test time the target_y is not available so we return None\n",
        "    if target_y is not None:\n",
        "      log_p = dist.log_prob(target_y)\n",
        "    else:\n",
        "      log_p = None\n",
        "\n",
        "    return log_p, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maXGTjLXcyeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_functions(target_x, target_y, context_x, context_y, pred_y, var):\n",
        "  \"\"\"Plots the predicted mean and variance and the context points.\n",
        "  \n",
        "  Args: \n",
        "    target_x: An array of shape batchsize x number_targets x 1 that contains the\n",
        "        x values of the target points.\n",
        "    target_y: An array of shape batchsize x number_targets x 1 that contains the\n",
        "        y values of the target points.\n",
        "    context_x: An array of shape batchsize x number_context x 1 that contains \n",
        "        the x values of the context points.\n",
        "    context_y: An array of shape batchsize x number_context x 1 that contains \n",
        "        the y values of the context points.\n",
        "    pred_y: An array of shape batchsize x number_targets x 1  that contains the\n",
        "        predicted means of the y values at the target points in target_x.\n",
        "    pred_y: An array of shape batchsize x number_targets x 1  that contains the\n",
        "        predicted variance of the y values at the target points in target_x.\n",
        "  \"\"\"\n",
        "  # Plot everything\n",
        "  plt.plot(target_x[0], pred_y[0], 'b', linewidth=2)\n",
        "  plt.plot(target_x[0], target_y[0], 'k:', linewidth=2)\n",
        "  plt.plot(context_x[0], context_y[0], 'ko', markersize=10)\n",
        "  plt.fill_between(\n",
        "      target_x[0, :, 0],\n",
        "      pred_y[0, :, 0] - var[0, :, 0],\n",
        "      pred_y[0, :, 0] + var[0, :, 0],\n",
        "      alpha=0.2,\n",
        "      facecolor='#65c9f7',\n",
        "      interpolate=True)\n",
        "\n",
        "  # Make the plot pretty\n",
        "  plt.yticks([-2, 0, 2], fontsize=16)\n",
        "  plt.xticks([-2, 0, 2], fontsize=16)\n",
        "  plt.ylim([-2, 2])\n",
        "  plt.grid(False)\n",
        "  ax = plt.gca()\n",
        "  ax.set_facecolor('white')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys-E-DMNdFno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Resetting Graphs\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvvkA9z7dGwP",
        "colab_type": "code",
        "outputId": "71e34c32-2f81-4c24-be3f-3ec9b031aabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Randomized Train-Test Split\n",
        "train_indices_1 = np.load(\"/content/gdrive/My Drive/Active Learning Research 2019/Train-Test-Splits/trainindices1.npy\")\n",
        "test_indices_1 = np.load(\"/content/gdrive/My Drive/Active Learning Research 2019/Train-Test-Splits/testindices1.npy\")\n",
        "X_train_1 = inputs[train_indices_1]\n",
        "X_test_1 = inputs[test_indices_1]\n",
        "y_train_1 = prob_s1[train_indices_1]\n",
        "y_test_1 = prob_s1[test_indices_1]\n",
        "#X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(inputs, prob_s1,test_size=0.1, random_state=40) \n",
        "#Compressing each input X to 1D for dimensional consistency with MLP Encoder\n",
        "X_train_lst = np.array([data_point.flatten() for data_point in X_train_1])\n",
        "X_train_1 = tf.constant(X_train_lst)\n",
        "X_test_lst = np.array([test_point.flatten() for test_point in X_test_1])\n",
        "X_test_1 = tf.constant(X_test_lst)\n",
        "y_train_lst = np.array([[training_y] for training_y in y_train_1])\n",
        "y_train_1 = tf.constant(y_train_lst)\n",
        "y_test_1 = tf.convert_to_tensor(y_test_1, dtype=tf.float64)\n",
        "#y_test_1 = tf.constant(y_test_1)\n",
        "print(tf.shape(X_train_1))\n",
        "print(tf.shape(y_test_1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
            "Tensor(\"Shape_1:0\", shape=(1,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9VHYn9mdRFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train and Test Datasets\n",
        "TRAINING_ITERATIONS=3000\n",
        "PRINT_AFTER=1\n",
        "#TRAINING_ITERATIONS = int(2e4)\n",
        "MAX_CONTEXT_POINTS = 2000\n",
        "\n",
        "dataset_train_5_prime = GPCurvesReader(\n",
        "    batch_size=4, x_train=X_train_1, x_test=X_test_1, y_train=y_train_1, y_test=y_test_1, max_num_context=MAX_CONTEXT_POINTS)\n",
        "data_train = dataset_train_5_prime.generate_curves()\n",
        "dataset_test_5_prime = GPCurvesReader(\n",
        "    batch_size=1, x_train=X_train_1, x_test=X_test_1, y_train=y_train_1, y_test=y_test_1, max_num_context=MAX_CONTEXT_POINTS,testing=True)\n",
        "data_test = dataset_test_5_prime.generate_curves()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSXCSsgadTJI",
        "colab_type": "code",
        "outputId": "f1b4d6be-cb0f-44ab-8764-9ad280ff0e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51835
        }
      },
      "source": [
        "# Sizes of the layers of the MLPs for the encoder and decoder\n",
        "# The final output layer of the decoder outputs two values, one for the mean and\n",
        "# one for the variance of the prediction at the target location\n",
        "encoder_output_sizes = [128, 128, 128, 128]\n",
        "decoder_output_sizes = [128, 128, 2]\n",
        "\n",
        "# Define the model\n",
        "model = DeterministicModel(encoder_output_sizes, decoder_output_sizes)\n",
        "\n",
        "# Define the loss\n",
        "log_prob, _, _ = model(data_train.query, data_train.num_total_points,\n",
        "                       data_train.num_context_points, data_train.target_y)\n",
        "\n",
        "loss = -tf.reduce_mean(log_prob)\n",
        "\n",
        "# Get the predicted mean and variance at the target points for the testing set\n",
        "_, mu, sigma = model(data_test.query, data_test.num_total_points,\n",
        "                     data_test.num_context_points)\n",
        "\n",
        "\n",
        "# Set up the optimizer and train step\n",
        "optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "train_step = optimizer.minimize(loss)\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for it in range(TRAINING_ITERATIONS):\n",
        "    sess.run([train_step])\n",
        "\n",
        "    # Plot the predictions in `PLOT_AFTER` intervals\n",
        "    if it % PRINT_AFTER == 0:\n",
        "      loss_value, pred_y, var, target_y, whole_query = sess.run(\n",
        "          [loss, mu, sigma, data_test.target_y, data_test.query])\n",
        "\n",
        "      (context_x, context_y), target_x = whole_query\n",
        "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, loss: 0.8734886079766493\n",
            "Iteration: 1, loss: 0.8244736921168431\n",
            "Iteration: 2, loss: 0.7822820476889528\n",
            "Iteration: 3, loss: 0.747035132305987\n",
            "Iteration: 4, loss: 0.7185566531015679\n",
            "Iteration: 5, loss: 0.6963092859427287\n",
            "Iteration: 6, loss: 0.6792776411133825\n",
            "Iteration: 7, loss: 0.6660143883818367\n",
            "Iteration: 8, loss: 0.6547609196155764\n",
            "Iteration: 9, loss: 0.6437029612216816\n",
            "Iteration: 10, loss: 0.6313916741548378\n",
            "Iteration: 11, loss: 0.6170383301830598\n",
            "Iteration: 12, loss: 0.6006567402281943\n",
            "Iteration: 13, loss: 0.5829266549789308\n",
            "Iteration: 14, loss: 0.5650327317530437\n",
            "Iteration: 15, loss: 0.5483377023954064\n",
            "Iteration: 16, loss: 0.5340727644168454\n",
            "Iteration: 17, loss: 0.5229501106401256\n",
            "Iteration: 18, loss: 0.514948365846081\n",
            "Iteration: 19, loss: 0.5091915827626338\n",
            "Iteration: 20, loss: 0.5043267136700557\n",
            "Iteration: 21, loss: 0.49909933032038845\n",
            "Iteration: 22, loss: 0.49300916717215204\n",
            "Iteration: 23, loss: 0.48661883285332125\n",
            "Iteration: 24, loss: 0.48124913048527995\n",
            "Iteration: 25, loss: 0.47813745820372117\n",
            "Iteration: 26, loss: 0.4773783550845441\n",
            "Iteration: 27, loss: 0.4773545637876603\n",
            "Iteration: 28, loss: 0.47583930435057176\n",
            "Iteration: 29, loss: 0.4720617127872422\n",
            "Iteration: 30, loss: 0.4669713542155531\n",
            "Iteration: 31, loss: 0.46217973533181816\n",
            "Iteration: 32, loss: 0.4585716988805864\n",
            "Iteration: 33, loss: 0.45587975948461884\n",
            "Iteration: 34, loss: 0.4532120014461883\n",
            "Iteration: 35, loss: 0.4499437596755299\n",
            "Iteration: 36, loss: 0.44612649163248624\n",
            "Iteration: 37, loss: 0.44231635748711107\n",
            "Iteration: 38, loss: 0.43907704418777677\n",
            "Iteration: 39, loss: 0.43666060579395494\n",
            "Iteration: 40, loss: 0.4348082704197061\n",
            "Iteration: 41, loss: 0.4330095258633761\n",
            "Iteration: 42, loss: 0.43093193448787515\n",
            "Iteration: 43, loss: 0.428563398896624\n",
            "Iteration: 44, loss: 0.4261408046079552\n",
            "Iteration: 45, loss: 0.42389575784480543\n",
            "Iteration: 46, loss: 0.42189384069566793\n",
            "Iteration: 47, loss: 0.42001035306410367\n",
            "Iteration: 48, loss: 0.4180028907688164\n",
            "Iteration: 49, loss: 0.41575494541490077\n",
            "Iteration: 50, loss: 0.4133134709341121\n",
            "Iteration: 51, loss: 0.41085599188995287\n",
            "Iteration: 52, loss: 0.4085491049077384\n",
            "Iteration: 53, loss: 0.4063988177266329\n",
            "Iteration: 54, loss: 0.404278281007705\n",
            "Iteration: 55, loss: 0.4020574696083503\n",
            "Iteration: 56, loss: 0.39975176125345846\n",
            "Iteration: 57, loss: 0.3974740684914075\n",
            "Iteration: 58, loss: 0.3953345476997987\n",
            "Iteration: 59, loss: 0.39330127032249085\n",
            "Iteration: 60, loss: 0.39124295242391305\n",
            "Iteration: 61, loss: 0.38905273900658593\n",
            "Iteration: 62, loss: 0.3867862264079986\n",
            "Iteration: 63, loss: 0.3845659917035248\n",
            "Iteration: 64, loss: 0.3824010862521634\n",
            "Iteration: 65, loss: 0.38022360906575964\n",
            "Iteration: 66, loss: 0.37795258856766206\n",
            "Iteration: 67, loss: 0.3756225567738648\n",
            "Iteration: 68, loss: 0.3733228064932568\n",
            "Iteration: 69, loss: 0.37106789464711254\n",
            "Iteration: 70, loss: 0.36878956201673874\n",
            "Iteration: 71, loss: 0.36643414966284704\n",
            "Iteration: 72, loss: 0.36407742611344424\n",
            "Iteration: 73, loss: 0.36177669553879704\n",
            "Iteration: 74, loss: 0.3594914294901445\n",
            "Iteration: 75, loss: 0.3571614591713078\n",
            "Iteration: 76, loss: 0.3547864551611723\n",
            "Iteration: 77, loss: 0.3524358607346875\n",
            "Iteration: 78, loss: 0.35008659339518977\n",
            "Iteration: 79, loss: 0.34768313612422025\n",
            "Iteration: 80, loss: 0.34526293177829387\n",
            "Iteration: 81, loss: 0.34286195094667465\n",
            "Iteration: 82, loss: 0.34045437627608305\n",
            "Iteration: 83, loss: 0.33802078666537444\n",
            "Iteration: 84, loss: 0.33560178781185745\n",
            "Iteration: 85, loss: 0.3332068934651975\n",
            "Iteration: 86, loss: 0.33078883002504156\n",
            "Iteration: 87, loss: 0.328353720277684\n",
            "Iteration: 88, loss: 0.32594831799588087\n",
            "Iteration: 89, loss: 0.32354992348294204\n",
            "Iteration: 90, loss: 0.3211510265148303\n",
            "Iteration: 91, loss: 0.31880148163846295\n",
            "Iteration: 92, loss: 0.3164832640872373\n",
            "Iteration: 93, loss: 0.3141952673922493\n",
            "Iteration: 94, loss: 0.3119609179372908\n",
            "Iteration: 95, loss: 0.3097572905856151\n",
            "Iteration: 96, loss: 0.3075881183694388\n",
            "Iteration: 97, loss: 0.3054705108402611\n",
            "Iteration: 98, loss: 0.3033602872257748\n",
            "Iteration: 99, loss: 0.30127891517222555\n",
            "Iteration: 100, loss: 0.2992351842481699\n",
            "Iteration: 101, loss: 0.2972076702743528\n",
            "Iteration: 102, loss: 0.2952221663013905\n",
            "Iteration: 103, loss: 0.29327405359230396\n",
            "Iteration: 104, loss: 0.29136529863496324\n",
            "Iteration: 105, loss: 0.28950275251703494\n",
            "Iteration: 106, loss: 0.28768230091140634\n",
            "Iteration: 107, loss: 0.2859123789697895\n",
            "Iteration: 108, loss: 0.284174596562917\n",
            "Iteration: 109, loss: 0.28248289054493764\n",
            "Iteration: 110, loss: 0.2808220581586604\n",
            "Iteration: 111, loss: 0.2792043066392964\n",
            "Iteration: 112, loss: 0.2776143902244879\n",
            "Iteration: 113, loss: 0.2760580473527622\n",
            "Iteration: 114, loss: 0.2745322510935886\n",
            "Iteration: 115, loss: 0.27304920351585943\n",
            "Iteration: 116, loss: 0.27160903333855013\n",
            "Iteration: 117, loss: 0.2702158244575141\n",
            "Iteration: 118, loss: 0.2688609525263555\n",
            "Iteration: 119, loss: 0.2675418398890161\n",
            "Iteration: 120, loss: 0.26624469648071364\n",
            "Iteration: 121, loss: 0.26498051699243536\n",
            "Iteration: 122, loss: 0.2637448018597086\n",
            "Iteration: 123, loss: 0.2625461439186126\n",
            "Iteration: 124, loss: 0.26137651266500234\n",
            "Iteration: 125, loss: 0.2602324457311031\n",
            "Iteration: 126, loss: 0.2591152064953323\n",
            "Iteration: 127, loss: 0.2580266089147312\n",
            "Iteration: 128, loss: 0.25697070672027583\n",
            "Iteration: 129, loss: 0.2559410753348735\n",
            "Iteration: 130, loss: 0.25493442659310034\n",
            "Iteration: 131, loss: 0.25394713276462705\n",
            "Iteration: 132, loss: 0.2529810365685233\n",
            "Iteration: 133, loss: 0.2520401693829195\n",
            "Iteration: 134, loss: 0.2511188332240679\n",
            "Iteration: 135, loss: 0.25021420461368404\n",
            "Iteration: 136, loss: 0.24932593360815597\n",
            "Iteration: 137, loss: 0.2484566198221909\n",
            "Iteration: 138, loss: 0.24759862619621686\n",
            "Iteration: 139, loss: 0.24674945257113234\n",
            "Iteration: 140, loss: 0.24590949856944133\n",
            "Iteration: 141, loss: 0.24507837373643132\n",
            "Iteration: 142, loss: 0.2442545541008176\n",
            "Iteration: 143, loss: 0.2434387225508126\n",
            "Iteration: 144, loss: 0.24263577922022977\n",
            "Iteration: 145, loss: 0.24183943266322916\n",
            "Iteration: 146, loss: 0.24104530445257022\n",
            "Iteration: 147, loss: 0.24025941213895802\n",
            "Iteration: 148, loss: 0.23947861789152697\n",
            "Iteration: 149, loss: 0.2386992675217704\n",
            "Iteration: 150, loss: 0.23792406088415302\n",
            "Iteration: 151, loss: 0.2371533753522976\n",
            "Iteration: 152, loss: 0.2363861300442441\n",
            "Iteration: 153, loss: 0.23561819671354273\n",
            "Iteration: 154, loss: 0.2348501736743301\n",
            "Iteration: 155, loss: 0.23408464257087233\n",
            "Iteration: 156, loss: 0.23332871762757684\n",
            "Iteration: 157, loss: 0.23257240183216557\n",
            "Iteration: 158, loss: 0.23182008569500812\n",
            "Iteration: 159, loss: 0.2310703704915024\n",
            "Iteration: 160, loss: 0.23033171203535327\n",
            "Iteration: 161, loss: 0.22959586938767046\n",
            "Iteration: 162, loss: 0.2288611735994089\n",
            "Iteration: 163, loss: 0.22812737462223714\n",
            "Iteration: 164, loss: 0.2273905638547311\n",
            "Iteration: 165, loss: 0.22665323329577047\n",
            "Iteration: 166, loss: 0.2259144595113872\n",
            "Iteration: 167, loss: 0.22518211692588494\n",
            "Iteration: 168, loss: 0.22445523188352984\n",
            "Iteration: 169, loss: 0.22372524227547083\n",
            "Iteration: 170, loss: 0.22299649841192196\n",
            "Iteration: 171, loss: 0.22227460967470464\n",
            "Iteration: 172, loss: 0.22155646040874188\n",
            "Iteration: 173, loss: 0.22083159511445818\n",
            "Iteration: 174, loss: 0.22011185781144882\n",
            "Iteration: 175, loss: 0.21939417800923033\n",
            "Iteration: 176, loss: 0.21867563065957882\n",
            "Iteration: 177, loss: 0.21795809431998908\n",
            "Iteration: 178, loss: 0.2172409469940902\n",
            "Iteration: 179, loss: 0.2165266674371046\n",
            "Iteration: 180, loss: 0.2158135983838101\n",
            "Iteration: 181, loss: 0.21510274628186074\n",
            "Iteration: 182, loss: 0.2143967373685336\n",
            "Iteration: 183, loss: 0.2136880507589353\n",
            "Iteration: 184, loss: 0.21297735616706678\n",
            "Iteration: 185, loss: 0.21226498257300294\n",
            "Iteration: 186, loss: 0.21154832474745167\n",
            "Iteration: 187, loss: 0.21083450526173128\n",
            "Iteration: 188, loss: 0.2101243353230972\n",
            "Iteration: 189, loss: 0.20942488800432638\n",
            "Iteration: 190, loss: 0.2087227898101073\n",
            "Iteration: 191, loss: 0.20802413698474578\n",
            "Iteration: 192, loss: 0.2073224675350422\n",
            "Iteration: 193, loss: 0.20661374508825628\n",
            "Iteration: 194, loss: 0.2058986161893855\n",
            "Iteration: 195, loss: 0.20518072450634556\n",
            "Iteration: 196, loss: 0.20447218390260744\n",
            "Iteration: 197, loss: 0.203772044210642\n",
            "Iteration: 198, loss: 0.20308213809741232\n",
            "Iteration: 199, loss: 0.20238356388936365\n",
            "Iteration: 200, loss: 0.20168296122109505\n",
            "Iteration: 201, loss: 0.20097960846705892\n",
            "Iteration: 202, loss: 0.2002646899274937\n",
            "Iteration: 203, loss: 0.19955500820056318\n",
            "Iteration: 204, loss: 0.1988440716650286\n",
            "Iteration: 205, loss: 0.19813756310220546\n",
            "Iteration: 206, loss: 0.1974344033719708\n",
            "Iteration: 207, loss: 0.19672902204088857\n",
            "Iteration: 208, loss: 0.19602514793341988\n",
            "Iteration: 209, loss: 0.19531887055400693\n",
            "Iteration: 210, loss: 0.19461557108469343\n",
            "Iteration: 211, loss: 0.1939106277078126\n",
            "Iteration: 212, loss: 0.19320747766771953\n",
            "Iteration: 213, loss: 0.19250331617698763\n",
            "Iteration: 214, loss: 0.19179577767918302\n",
            "Iteration: 215, loss: 0.19108711401485579\n",
            "Iteration: 216, loss: 0.1903798588187423\n",
            "Iteration: 217, loss: 0.18967161783077535\n",
            "Iteration: 218, loss: 0.1889581554815825\n",
            "Iteration: 219, loss: 0.18824590882290979\n",
            "Iteration: 220, loss: 0.18753206922246993\n",
            "Iteration: 221, loss: 0.18682026293419\n",
            "Iteration: 222, loss: 0.18610146720184\n",
            "Iteration: 223, loss: 0.18538172998154417\n",
            "Iteration: 224, loss: 0.18465981646728932\n",
            "Iteration: 225, loss: 0.18393319379458586\n",
            "Iteration: 226, loss: 0.183208366306049\n",
            "Iteration: 227, loss: 0.18248196658657043\n",
            "Iteration: 228, loss: 0.18174972488057337\n",
            "Iteration: 229, loss: 0.18102406944030036\n",
            "Iteration: 230, loss: 0.18029316303376192\n",
            "Iteration: 231, loss: 0.1795534880917414\n",
            "Iteration: 232, loss: 0.17882110591917363\n",
            "Iteration: 233, loss: 0.1780832020064309\n",
            "Iteration: 234, loss: 0.17734808447748632\n",
            "Iteration: 235, loss: 0.17661803451714883\n",
            "Iteration: 236, loss: 0.17589377216279803\n",
            "Iteration: 237, loss: 0.17518238125801644\n",
            "Iteration: 238, loss: 0.17449246567331714\n",
            "Iteration: 239, loss: 0.17379256821720887\n",
            "Iteration: 240, loss: 0.17307783815353636\n",
            "Iteration: 241, loss: 0.17229717238535908\n",
            "Iteration: 242, loss: 0.17146888641631153\n",
            "Iteration: 243, loss: 0.1706608599702042\n",
            "Iteration: 244, loss: 0.16989957314673104\n",
            "Iteration: 245, loss: 0.16918563213893548\n",
            "Iteration: 246, loss: 0.16848155851076915\n",
            "Iteration: 247, loss: 0.16774846918203615\n",
            "Iteration: 248, loss: 0.16697041750826738\n",
            "Iteration: 249, loss: 0.16617006543725993\n",
            "Iteration: 250, loss: 0.16538730906543087\n",
            "Iteration: 251, loss: 0.16462983195437422\n",
            "Iteration: 252, loss: 0.163893867057536\n",
            "Iteration: 253, loss: 0.1631661287952221\n",
            "Iteration: 254, loss: 0.1624203125815347\n",
            "Iteration: 255, loss: 0.1616485628241135\n",
            "Iteration: 256, loss: 0.1608609064776424\n",
            "Iteration: 257, loss: 0.1600640301245661\n",
            "Iteration: 258, loss: 0.15928488137067068\n",
            "Iteration: 259, loss: 0.15851955159901152\n",
            "Iteration: 260, loss: 0.15776575755689412\n",
            "Iteration: 261, loss: 0.15701881740999854\n",
            "Iteration: 262, loss: 0.1562572189080885\n",
            "Iteration: 263, loss: 0.15549425286646074\n",
            "Iteration: 264, loss: 0.15471569296127188\n",
            "Iteration: 265, loss: 0.15393732028876145\n",
            "Iteration: 266, loss: 0.1531519990763996\n",
            "Iteration: 267, loss: 0.1523511066835705\n",
            "Iteration: 268, loss: 0.15156169575613562\n",
            "Iteration: 269, loss: 0.15075987399302687\n",
            "Iteration: 270, loss: 0.1499690756477816\n",
            "Iteration: 271, loss: 0.14917630430605788\n",
            "Iteration: 272, loss: 0.1483863387396924\n",
            "Iteration: 273, loss: 0.14759668520924477\n",
            "Iteration: 274, loss: 0.14681022985001943\n",
            "Iteration: 275, loss: 0.14602894333487001\n",
            "Iteration: 276, loss: 0.14525898971959844\n",
            "Iteration: 277, loss: 0.14451093842932067\n",
            "Iteration: 278, loss: 0.14378652066775255\n",
            "Iteration: 279, loss: 0.14309577315570055\n",
            "Iteration: 280, loss: 0.14239951075528898\n",
            "Iteration: 281, loss: 0.14163084570273154\n",
            "Iteration: 282, loss: 0.1407153986010715\n",
            "Iteration: 283, loss: 0.13972903708700132\n",
            "Iteration: 284, loss: 0.1388039835809241\n",
            "Iteration: 285, loss: 0.1380285184111859\n",
            "Iteration: 286, loss: 0.13733938818553482\n",
            "Iteration: 287, loss: 0.1366241354697773\n",
            "Iteration: 288, loss: 0.1357989675521729\n",
            "Iteration: 289, loss: 0.1348688898445662\n",
            "Iteration: 290, loss: 0.13395996024844534\n",
            "Iteration: 291, loss: 0.13313609885365169\n",
            "Iteration: 292, loss: 0.1323858780170361\n",
            "Iteration: 293, loss: 0.13162525508387932\n",
            "Iteration: 294, loss: 0.130809652872183\n",
            "Iteration: 295, loss: 0.12993502428686082\n",
            "Iteration: 296, loss: 0.12904508709498907\n",
            "Iteration: 297, loss: 0.12819028284071135\n",
            "Iteration: 298, loss: 0.1273717582245324\n",
            "Iteration: 299, loss: 0.12657717688210424\n",
            "Iteration: 300, loss: 0.12576914638710585\n",
            "Iteration: 301, loss: 0.12494645464429571\n",
            "Iteration: 302, loss: 0.12409428504402902\n",
            "Iteration: 303, loss: 0.12321752660642622\n",
            "Iteration: 304, loss: 0.12235306487271844\n",
            "Iteration: 305, loss: 0.12148324278414636\n",
            "Iteration: 306, loss: 0.12063729528882204\n",
            "Iteration: 307, loss: 0.11979621627980185\n",
            "Iteration: 308, loss: 0.11895480046275392\n",
            "Iteration: 309, loss: 0.11812829713000259\n",
            "Iteration: 310, loss: 0.11728950940655057\n",
            "Iteration: 311, loss: 0.11646315465321609\n",
            "Iteration: 312, loss: 0.11563233912733775\n",
            "Iteration: 313, loss: 0.11480199081445058\n",
            "Iteration: 314, loss: 0.11396462261205577\n",
            "Iteration: 315, loss: 0.11312145781038975\n",
            "Iteration: 316, loss: 0.11225705417023368\n",
            "Iteration: 317, loss: 0.11136884522253289\n",
            "Iteration: 318, loss: 0.11044368831062595\n",
            "Iteration: 319, loss: 0.10952110043511434\n",
            "Iteration: 320, loss: 0.10858143024189232\n",
            "Iteration: 321, loss: 0.10767842283427202\n",
            "Iteration: 322, loss: 0.10677986405134489\n",
            "Iteration: 323, loss: 0.10590023437668787\n",
            "Iteration: 324, loss: 0.10502588873389246\n",
            "Iteration: 325, loss: 0.10416030086049022\n",
            "Iteration: 326, loss: 0.10331075411586818\n",
            "Iteration: 327, loss: 0.10248225148787785\n",
            "Iteration: 328, loss: 0.10168452770178862\n",
            "Iteration: 329, loss: 0.10094047445749768\n",
            "Iteration: 330, loss: 0.10021327400682503\n",
            "Iteration: 331, loss: 0.09946956799689466\n",
            "Iteration: 332, loss: 0.09858209475786787\n",
            "Iteration: 333, loss: 0.09750233655609265\n",
            "Iteration: 334, loss: 0.09630433956841904\n",
            "Iteration: 335, loss: 0.09522279936601936\n",
            "Iteration: 336, loss: 0.09436384491210524\n",
            "Iteration: 337, loss: 0.09362842526126881\n",
            "Iteration: 338, loss: 0.09288790243972918\n",
            "Iteration: 339, loss: 0.09201143664006388\n",
            "Iteration: 340, loss: 0.09099670782053382\n",
            "Iteration: 341, loss: 0.08993554580398086\n",
            "Iteration: 342, loss: 0.08892224110684623\n",
            "Iteration: 343, loss: 0.08804837663658989\n",
            "Iteration: 344, loss: 0.0872276951261145\n",
            "Iteration: 345, loss: 0.08640140815139341\n",
            "Iteration: 346, loss: 0.08550424115530002\n",
            "Iteration: 347, loss: 0.08453531909390552\n",
            "Iteration: 348, loss: 0.08353577001430003\n",
            "Iteration: 349, loss: 0.08253977416284823\n",
            "Iteration: 350, loss: 0.08158747688340602\n",
            "Iteration: 351, loss: 0.08067175631519571\n",
            "Iteration: 352, loss: 0.07977889422413864\n",
            "Iteration: 353, loss: 0.07890605851315721\n",
            "Iteration: 354, loss: 0.0780183233998848\n",
            "Iteration: 355, loss: 0.0771229752367449\n",
            "Iteration: 356, loss: 0.0761994502483216\n",
            "Iteration: 357, loss: 0.07526267412698236\n",
            "Iteration: 358, loss: 0.07431387426396222\n",
            "Iteration: 359, loss: 0.07334154822791575\n",
            "Iteration: 360, loss: 0.07237736858435499\n",
            "Iteration: 361, loss: 0.07140818054461542\n",
            "Iteration: 362, loss: 0.07044051454764641\n",
            "Iteration: 363, loss: 0.06947544001391287\n",
            "Iteration: 364, loss: 0.06852823719905506\n",
            "Iteration: 365, loss: 0.06758166623905106\n",
            "Iteration: 366, loss: 0.06664818539623099\n",
            "Iteration: 367, loss: 0.0657178190079608\n",
            "Iteration: 368, loss: 0.0648218305902856\n",
            "Iteration: 369, loss: 0.06396466301879986\n",
            "Iteration: 370, loss: 0.0631919081647085\n",
            "Iteration: 371, loss: 0.06259113214438587\n",
            "Iteration: 372, loss: 0.062170342328117884\n",
            "Iteration: 373, loss: 0.0617646596680677\n",
            "Iteration: 374, loss: 0.06089708517475333\n",
            "Iteration: 375, loss: 0.05912045504411721\n",
            "Iteration: 376, loss: 0.05728360720400748\n",
            "Iteration: 377, loss: 0.056323260552286925\n",
            "Iteration: 378, loss: 0.05603117003614152\n",
            "Iteration: 379, loss: 0.05551171297820816\n",
            "Iteration: 380, loss: 0.0542071313563916\n",
            "Iteration: 381, loss: 0.052651057890666014\n",
            "Iteration: 382, loss: 0.05168154891839926\n",
            "Iteration: 383, loss: 0.051209709566337816\n",
            "Iteration: 384, loss: 0.0504807527144805\n",
            "Iteration: 385, loss: 0.04921228505144034\n",
            "Iteration: 386, loss: 0.047948706330319564\n",
            "Iteration: 387, loss: 0.04710276439361389\n",
            "Iteration: 388, loss: 0.04645072625328527\n",
            "Iteration: 389, loss: 0.04558824782892078\n",
            "Iteration: 390, loss: 0.04441862653657941\n",
            "Iteration: 391, loss: 0.04328524079475135\n",
            "Iteration: 392, loss: 0.042422431827964444\n",
            "Iteration: 393, loss: 0.04165645552450338\n",
            "Iteration: 394, loss: 0.04077422824333216\n",
            "Iteration: 395, loss: 0.03972743462676166\n",
            "Iteration: 396, loss: 0.038640476146252475\n",
            "Iteration: 397, loss: 0.037675433951859794\n",
            "Iteration: 398, loss: 0.03680501545932298\n",
            "Iteration: 399, loss: 0.03595150764476483\n",
            "Iteration: 400, loss: 0.0350173130064968\n",
            "Iteration: 401, loss: 0.03401070340544983\n",
            "Iteration: 402, loss: 0.032987950296217206\n",
            "Iteration: 403, loss: 0.031985938006517715\n",
            "Iteration: 404, loss: 0.031030888579951458\n",
            "Iteration: 405, loss: 0.03010036137016521\n",
            "Iteration: 406, loss: 0.029186920239173037\n",
            "Iteration: 407, loss: 0.028269253687953414\n",
            "Iteration: 408, loss: 0.027342788737833777\n",
            "Iteration: 409, loss: 0.026394860480925913\n",
            "Iteration: 410, loss: 0.025419524754534437\n",
            "Iteration: 411, loss: 0.024443223616331233\n",
            "Iteration: 412, loss: 0.023440561712288012\n",
            "Iteration: 413, loss: 0.02246251476945789\n",
            "Iteration: 414, loss: 0.021468172566982864\n",
            "Iteration: 415, loss: 0.02049873566506821\n",
            "Iteration: 416, loss: 0.019511204270679577\n",
            "Iteration: 417, loss: 0.018553879035888052\n",
            "Iteration: 418, loss: 0.017586904718572115\n",
            "Iteration: 419, loss: 0.016645169147936772\n",
            "Iteration: 420, loss: 0.01573575282796989\n",
            "Iteration: 421, loss: 0.014894795762726482\n",
            "Iteration: 422, loss: 0.014161969319413024\n",
            "Iteration: 423, loss: 0.013564123710332719\n",
            "Iteration: 424, loss: 0.013154053887751184\n",
            "Iteration: 425, loss: 0.0126161339508611\n",
            "Iteration: 426, loss: 0.011693827200137879\n",
            "Iteration: 427, loss: 0.009996882516546922\n",
            "Iteration: 428, loss: 0.00811371749134229\n",
            "Iteration: 429, loss: 0.006788817968583985\n",
            "Iteration: 430, loss: 0.006222673483661592\n",
            "Iteration: 431, loss: 0.005927831239997694\n",
            "Iteration: 432, loss: 0.005191262974227854\n",
            "Iteration: 433, loss: 0.0038474446871219724\n",
            "Iteration: 434, loss: 0.0022594480003212203\n",
            "Iteration: 435, loss: 0.0010791572249472315\n",
            "Iteration: 436, loss: 0.0004290111713907825\n",
            "Iteration: 437, loss: -0.00012626586090963587\n",
            "Iteration: 438, loss: -0.0010171060926322272\n",
            "Iteration: 439, loss: -0.0022917876083325206\n",
            "Iteration: 440, loss: -0.0035982071024529983\n",
            "Iteration: 441, loss: -0.004638579450106057\n",
            "Iteration: 442, loss: -0.005403104547258254\n",
            "Iteration: 443, loss: -0.006136038279035077\n",
            "Iteration: 444, loss: -0.007037963718176506\n",
            "Iteration: 445, loss: -0.00813352299535384\n",
            "Iteration: 446, loss: -0.009305673692706797\n",
            "Iteration: 447, loss: -0.010382757784578306\n",
            "Iteration: 448, loss: -0.011316297154811519\n",
            "Iteration: 449, loss: -0.012167928257053899\n",
            "Iteration: 450, loss: -0.012987102829502242\n",
            "Iteration: 451, loss: -0.013872290546539084\n",
            "Iteration: 452, loss: -0.01484281888302037\n",
            "Iteration: 453, loss: -0.015873195632436523\n",
            "Iteration: 454, loss: -0.016951096335767017\n",
            "Iteration: 455, loss: -0.0180281150311042\n",
            "Iteration: 456, loss: -0.019036273308066422\n",
            "Iteration: 457, loss: -0.020007395515414608\n",
            "Iteration: 458, loss: -0.020932329864270932\n",
            "Iteration: 459, loss: -0.021839837422141295\n",
            "Iteration: 460, loss: -0.022709745556785017\n",
            "Iteration: 461, loss: -0.023558081517651163\n",
            "Iteration: 462, loss: -0.0243604300879331\n",
            "Iteration: 463, loss: -0.02513396058358014\n",
            "Iteration: 464, loss: -0.025890787726215458\n",
            "Iteration: 465, loss: -0.026631418984147844\n",
            "Iteration: 466, loss: -0.027542839462898375\n",
            "Iteration: 467, loss: -0.028613922753773773\n",
            "Iteration: 468, loss: -0.029885359150519413\n",
            "Iteration: 469, loss: -0.031185565282579576\n",
            "Iteration: 470, loss: -0.03239274734207088\n",
            "Iteration: 471, loss: -0.033436820076319\n",
            "Iteration: 472, loss: -0.03431671037305288\n",
            "Iteration: 473, loss: -0.03510206531337299\n",
            "Iteration: 474, loss: -0.03582054100103418\n",
            "Iteration: 475, loss: -0.03654094845049816\n",
            "Iteration: 476, loss: -0.03724525305394109\n",
            "Iteration: 477, loss: -0.03811258168633309\n",
            "Iteration: 478, loss: -0.03905843521439561\n",
            "Iteration: 479, loss: -0.04023223514350451\n",
            "Iteration: 480, loss: -0.04146597371788878\n",
            "Iteration: 481, loss: -0.042700645786087905\n",
            "Iteration: 482, loss: -0.04381703968824124\n",
            "Iteration: 483, loss: -0.04479117826376021\n",
            "Iteration: 484, loss: -0.045662296728466105\n",
            "Iteration: 485, loss: -0.046446980159578\n",
            "Iteration: 486, loss: -0.04720477595604412\n",
            "Iteration: 487, loss: -0.047904928092619196\n",
            "Iteration: 488, loss: -0.048644136853376256\n",
            "Iteration: 489, loss: -0.04936625312890746\n",
            "Iteration: 490, loss: -0.05021570277438079\n",
            "Iteration: 491, loss: -0.051181583756766146\n",
            "Iteration: 492, loss: -0.05234885726501318\n",
            "Iteration: 493, loss: -0.05364135941529176\n",
            "Iteration: 494, loss: -0.054878050576926095\n",
            "Iteration: 495, loss: -0.05601420105880421\n",
            "Iteration: 496, loss: -0.056960088186118654\n",
            "Iteration: 497, loss: -0.05780700215619215\n",
            "Iteration: 498, loss: -0.05856574804570093\n",
            "Iteration: 499, loss: -0.05928484216305897\n",
            "Iteration: 500, loss: -0.059941939842558384\n",
            "Iteration: 501, loss: -0.060610950819520414\n",
            "Iteration: 502, loss: -0.061359608733964635\n",
            "Iteration: 503, loss: -0.062271463499701814\n",
            "Iteration: 504, loss: -0.0633171572997347\n",
            "Iteration: 505, loss: -0.06454113507951215\n",
            "Iteration: 506, loss: -0.06579820983402365\n",
            "Iteration: 507, loss: -0.06697827177458555\n",
            "Iteration: 508, loss: -0.06803107801266116\n",
            "Iteration: 509, loss: -0.06895015838330869\n",
            "Iteration: 510, loss: -0.06975578571336051\n",
            "Iteration: 511, loss: -0.07050655747583386\n",
            "Iteration: 512, loss: -0.07121586702821198\n",
            "Iteration: 513, loss: -0.07191431179813619\n",
            "Iteration: 514, loss: -0.07263987359795607\n",
            "Iteration: 515, loss: -0.07338501828168603\n",
            "Iteration: 516, loss: -0.0742767047933953\n",
            "Iteration: 517, loss: -0.0752619033808042\n",
            "Iteration: 518, loss: -0.07639686919968823\n",
            "Iteration: 519, loss: -0.07757314678486024\n",
            "Iteration: 520, loss: -0.07872132858447166\n",
            "Iteration: 521, loss: -0.07980658173623564\n",
            "Iteration: 522, loss: -0.08077921594696394\n",
            "Iteration: 523, loss: -0.08169882147573242\n",
            "Iteration: 524, loss: -0.08254481771972157\n",
            "Iteration: 525, loss: -0.08335402762543866\n",
            "Iteration: 526, loss: -0.08408450116085173\n",
            "Iteration: 527, loss: -0.0847325207717915\n",
            "Iteration: 528, loss: -0.08524376253824921\n",
            "Iteration: 529, loss: -0.0856946672642968\n",
            "Iteration: 530, loss: -0.0860316015940666\n",
            "Iteration: 531, loss: -0.08656704009710417\n",
            "Iteration: 532, loss: -0.08745946841854095\n",
            "Iteration: 533, loss: -0.08901445295057132\n",
            "Iteration: 534, loss: -0.09084364182988797\n",
            "Iteration: 535, loss: -0.09237428004978918\n",
            "Iteration: 536, loss: -0.09331336004272218\n",
            "Iteration: 537, loss: -0.09370719038755582\n",
            "Iteration: 538, loss: -0.09399589029656961\n",
            "Iteration: 539, loss: -0.09447271924327603\n",
            "Iteration: 540, loss: -0.09543781446178827\n",
            "Iteration: 541, loss: -0.0968279539085037\n",
            "Iteration: 542, loss: -0.09833123832538886\n",
            "Iteration: 543, loss: -0.09950620180901312\n",
            "Iteration: 544, loss: -0.10026453519058778\n",
            "Iteration: 545, loss: -0.10076666492832373\n",
            "Iteration: 546, loss: -0.1012392806560357\n",
            "Iteration: 547, loss: -0.10196625908640962\n",
            "Iteration: 548, loss: -0.10296202439999985\n",
            "Iteration: 549, loss: -0.10417792853837987\n",
            "Iteration: 550, loss: -0.10540333838056629\n",
            "Iteration: 551, loss: -0.10647423205217592\n",
            "Iteration: 552, loss: -0.10736216212598912\n",
            "Iteration: 553, loss: -0.10808192351819365\n",
            "Iteration: 554, loss: -0.10873088450061695\n",
            "Iteration: 555, loss: -0.10934575692261272\n",
            "Iteration: 556, loss: -0.11000984703082524\n",
            "Iteration: 557, loss: -0.11074841500121045\n",
            "Iteration: 558, loss: -0.11165925999467728\n",
            "Iteration: 559, loss: -0.11267732063420617\n",
            "Iteration: 560, loss: -0.11376443684884124\n",
            "Iteration: 561, loss: -0.1148582746784351\n",
            "Iteration: 562, loss: -0.11590935195259613\n",
            "Iteration: 563, loss: -0.11686722513332451\n",
            "Iteration: 564, loss: -0.1177619003191947\n",
            "Iteration: 565, loss: -0.11860537985169164\n",
            "Iteration: 566, loss: -0.11938545518231981\n",
            "Iteration: 567, loss: -0.12013553042391072\n",
            "Iteration: 568, loss: -0.12080722520547195\n",
            "Iteration: 569, loss: -0.12138038683447845\n",
            "Iteration: 570, loss: -0.1218473877668452\n",
            "Iteration: 571, loss: -0.12218047297604996\n",
            "Iteration: 572, loss: -0.12241604578898352\n",
            "Iteration: 573, loss: -0.12284353954992926\n",
            "Iteration: 574, loss: -0.12359637855412857\n",
            "Iteration: 575, loss: -0.12509850959934965\n",
            "Iteration: 576, loss: -0.12691650213084507\n",
            "Iteration: 577, loss: -0.1286176003212928\n",
            "Iteration: 578, loss: -0.12968543865510102\n",
            "Iteration: 579, loss: -0.13015774019369047\n",
            "Iteration: 580, loss: -0.13036454204490666\n",
            "Iteration: 581, loss: -0.13061751629030832\n",
            "Iteration: 582, loss: -0.13137277267351596\n",
            "Iteration: 583, loss: -0.13255663153611696\n",
            "Iteration: 584, loss: -0.1340614820768471\n",
            "Iteration: 585, loss: -0.1353972524512599\n",
            "Iteration: 586, loss: -0.13631603503750778\n",
            "Iteration: 587, loss: -0.1368764702768605\n",
            "Iteration: 588, loss: -0.137296509296814\n",
            "Iteration: 589, loss: -0.1377959632339781\n",
            "Iteration: 590, loss: -0.1384867957598217\n",
            "Iteration: 591, loss: -0.13948245568290776\n",
            "Iteration: 592, loss: -0.14064392821009045\n",
            "Iteration: 593, loss: -0.1418271176394864\n",
            "Iteration: 594, loss: -0.14287629163949425\n",
            "Iteration: 595, loss: -0.1437330450479613\n",
            "Iteration: 596, loss: -0.14444005813172758\n",
            "Iteration: 597, loss: -0.14505336596196972\n",
            "Iteration: 598, loss: -0.14562597988796971\n",
            "Iteration: 599, loss: -0.14622167455123475\n",
            "Iteration: 600, loss: -0.1468901111736019\n",
            "Iteration: 601, loss: -0.14762391183984316\n",
            "Iteration: 602, loss: -0.1484712362579156\n",
            "Iteration: 603, loss: -0.14943125702780186\n",
            "Iteration: 604, loss: -0.15044258507904026\n",
            "Iteration: 605, loss: -0.15146199704083793\n",
            "Iteration: 606, loss: -0.15246116065936355\n",
            "Iteration: 607, loss: -0.15343951449369456\n",
            "Iteration: 608, loss: -0.15431206574216413\n",
            "Iteration: 609, loss: -0.15518549984662625\n",
            "Iteration: 610, loss: -0.15601048077603724\n",
            "Iteration: 611, loss: -0.15680371563542464\n",
            "Iteration: 612, loss: -0.1575997856459828\n",
            "Iteration: 613, loss: -0.15832894220276889\n",
            "Iteration: 614, loss: -0.15898968640436892\n",
            "Iteration: 615, loss: -0.15953591070969877\n",
            "Iteration: 616, loss: -0.1598227752722087\n",
            "Iteration: 617, loss: -0.1597992749751473\n",
            "Iteration: 618, loss: -0.15916904988830433\n",
            "Iteration: 619, loss: -0.1584241169623083\n",
            "Iteration: 620, loss: -0.15817331144478208\n",
            "Iteration: 621, loss: -0.16025277929021325\n",
            "Iteration: 622, loss: -0.1636930203736408\n",
            "Iteration: 623, loss: -0.16641215369067056\n",
            "Iteration: 624, loss: -0.16673191563844206\n",
            "Iteration: 625, loss: -0.1656484709537815\n",
            "Iteration: 626, loss: -0.16539656925736632\n",
            "Iteration: 627, loss: -0.1669180303563204\n",
            "Iteration: 628, loss: -0.16959049129695178\n",
            "Iteration: 629, loss: -0.17117719098670653\n",
            "Iteration: 630, loss: -0.17113499657879191\n",
            "Iteration: 631, loss: -0.17081140120613247\n",
            "Iteration: 632, loss: -0.17148204433420927\n",
            "Iteration: 633, loss: -0.1733072366915162\n",
            "Iteration: 634, loss: -0.17495010598699354\n",
            "Iteration: 635, loss: -0.1755802308993101\n",
            "Iteration: 636, loss: -0.17563039856312873\n",
            "Iteration: 637, loss: -0.1760064196831975\n",
            "Iteration: 638, loss: -0.1771567080920652\n",
            "Iteration: 639, loss: -0.17855725890744417\n",
            "Iteration: 640, loss: -0.17961905963826774\n",
            "Iteration: 641, loss: -0.18013506995427908\n",
            "Iteration: 642, loss: -0.18045933637453285\n",
            "Iteration: 643, loss: -0.18106821912580523\n",
            "Iteration: 644, loss: -0.18204549198291872\n",
            "Iteration: 645, loss: -0.1831888959187716\n",
            "Iteration: 646, loss: -0.18419427155074225\n",
            "Iteration: 647, loss: -0.18491605033039857\n",
            "Iteration: 648, loss: -0.18547896198393163\n",
            "Iteration: 649, loss: -0.18603206319895935\n",
            "Iteration: 650, loss: -0.18675930586396267\n",
            "Iteration: 651, loss: -0.18764840131707003\n",
            "Iteration: 652, loss: -0.1885909228324867\n",
            "Iteration: 653, loss: -0.18951483904957153\n",
            "Iteration: 654, loss: -0.19034689739088778\n",
            "Iteration: 655, loss: -0.19109729478556012\n",
            "Iteration: 656, loss: -0.19177744074426256\n",
            "Iteration: 657, loss: -0.19243928361064808\n",
            "Iteration: 658, loss: -0.19308742545801633\n",
            "Iteration: 659, loss: -0.19380208187808284\n",
            "Iteration: 660, loss: -0.1944899725413848\n",
            "Iteration: 661, loss: -0.19522107833298324\n",
            "Iteration: 662, loss: -0.19598487003332843\n",
            "Iteration: 663, loss: -0.1967339814662218\n",
            "Iteration: 664, loss: -0.19752931495297396\n",
            "Iteration: 665, loss: -0.19832416564068595\n",
            "Iteration: 666, loss: -0.19912977981282473\n",
            "Iteration: 667, loss: -0.19992349416274965\n",
            "Iteration: 668, loss: -0.20069072764167178\n",
            "Iteration: 669, loss: -0.20146656330778623\n",
            "Iteration: 670, loss: -0.2021891780163027\n",
            "Iteration: 671, loss: -0.2029058034705003\n",
            "Iteration: 672, loss: -0.20356082544033746\n",
            "Iteration: 673, loss: -0.20417668161989577\n",
            "Iteration: 674, loss: -0.20470652030749922\n",
            "Iteration: 675, loss: -0.20516417062356254\n",
            "Iteration: 676, loss: -0.20548304690668898\n",
            "Iteration: 677, loss: -0.20581217878825936\n",
            "Iteration: 678, loss: -0.2060504293322545\n",
            "Iteration: 679, loss: -0.20664178441591555\n",
            "Iteration: 680, loss: -0.20757237348598975\n",
            "Iteration: 681, loss: -0.20904199752379293\n",
            "Iteration: 682, loss: -0.2106993764502255\n",
            "Iteration: 683, loss: -0.21222549436503418\n",
            "Iteration: 684, loss: -0.21333963256846344\n",
            "Iteration: 685, loss: -0.2139403517176335\n",
            "Iteration: 686, loss: -0.21430428766073684\n",
            "Iteration: 687, loss: -0.2145338860482138\n",
            "Iteration: 688, loss: -0.21483004837522188\n",
            "Iteration: 689, loss: -0.21529389027979146\n",
            "Iteration: 690, loss: -0.21609976421240357\n",
            "Iteration: 691, loss: -0.2171418887539644\n",
            "Iteration: 692, loss: -0.2184460721364525\n",
            "Iteration: 693, loss: -0.21969311345155296\n",
            "Iteration: 694, loss: -0.22076039453748944\n",
            "Iteration: 695, loss: -0.22159637049319605\n",
            "Iteration: 696, loss: -0.22223379348374953\n",
            "Iteration: 697, loss: -0.22275747984840127\n",
            "Iteration: 698, loss: -0.22319289580806487\n",
            "Iteration: 699, loss: -0.22361712643161263\n",
            "Iteration: 700, loss: -0.22397954868355435\n",
            "Iteration: 701, loss: -0.2244294843971909\n",
            "Iteration: 702, loss: -0.22495699379632828\n",
            "Iteration: 703, loss: -0.2256601400741184\n",
            "Iteration: 704, loss: -0.226559457824677\n",
            "Iteration: 705, loss: -0.22766233139937167\n",
            "Iteration: 706, loss: -0.22883002143392844\n",
            "Iteration: 707, loss: -0.23001184486728515\n",
            "Iteration: 708, loss: -0.2310624099456073\n",
            "Iteration: 709, loss: -0.23195997306253413\n",
            "Iteration: 710, loss: -0.23271874321501923\n",
            "Iteration: 711, loss: -0.23336268275856498\n",
            "Iteration: 712, loss: -0.23391753821205927\n",
            "Iteration: 713, loss: -0.23435171345429867\n",
            "Iteration: 714, loss: -0.2346940024626561\n",
            "Iteration: 715, loss: -0.23488265938019878\n",
            "Iteration: 716, loss: -0.2349734560746041\n",
            "Iteration: 717, loss: -0.23494273259523368\n",
            "Iteration: 718, loss: -0.23521410292404157\n",
            "Iteration: 719, loss: -0.23577641443473688\n",
            "Iteration: 720, loss: -0.2372311527198853\n",
            "Iteration: 721, loss: -0.2390364097202899\n",
            "Iteration: 722, loss: -0.24080268467192417\n",
            "Iteration: 723, loss: -0.24207013183268067\n",
            "Iteration: 724, loss: -0.242703302348941\n",
            "Iteration: 725, loss: -0.24288932249993292\n",
            "Iteration: 726, loss: -0.2428828405100372\n",
            "Iteration: 727, loss: -0.24298454366473823\n",
            "Iteration: 728, loss: -0.24338502358133018\n",
            "Iteration: 729, loss: -0.24435204044214578\n",
            "Iteration: 730, loss: -0.24566358854404186\n",
            "Iteration: 731, loss: -0.24710520070312506\n",
            "Iteration: 732, loss: -0.24832250174524123\n",
            "Iteration: 733, loss: -0.24915514141371203\n",
            "Iteration: 734, loss: -0.24966614075231175\n",
            "Iteration: 735, loss: -0.24999781374057567\n",
            "Iteration: 736, loss: -0.2503305710319051\n",
            "Iteration: 737, loss: -0.2507313389522671\n",
            "Iteration: 738, loss: -0.2513689541912824\n",
            "Iteration: 739, loss: -0.2521534975625342\n",
            "Iteration: 740, loss: -0.2531055498789854\n",
            "Iteration: 741, loss: -0.2540982067008376\n",
            "Iteration: 742, loss: -0.25509587982665555\n",
            "Iteration: 743, loss: -0.2560203983811596\n",
            "Iteration: 744, loss: -0.25681507454962677\n",
            "Iteration: 745, loss: -0.2575720387307365\n",
            "Iteration: 746, loss: -0.25820179467822374\n",
            "Iteration: 747, loss: -0.2588368424316636\n",
            "Iteration: 748, loss: -0.2593658680885555\n",
            "Iteration: 749, loss: -0.2598872564492992\n",
            "Iteration: 750, loss: -0.2602633133481942\n",
            "Iteration: 751, loss: -0.2604992966570681\n",
            "Iteration: 752, loss: -0.26046569346148907\n",
            "Iteration: 753, loss: -0.2602536577282513\n",
            "Iteration: 754, loss: -0.25983344317747636\n",
            "Iteration: 755, loss: -0.2600417260255383\n",
            "Iteration: 756, loss: -0.2609000829683122\n",
            "Iteration: 757, loss: -0.2628136014894678\n",
            "Iteration: 758, loss: -0.2650503212425448\n",
            "Iteration: 759, loss: -0.2668911443522594\n",
            "Iteration: 760, loss: -0.2678612229853636\n",
            "Iteration: 761, loss: -0.26805397655199364\n",
            "Iteration: 762, loss: -0.26784191487461284\n",
            "Iteration: 763, loss: -0.26767170247243544\n",
            "Iteration: 764, loss: -0.26800488398346867\n",
            "Iteration: 765, loss: -0.2689310851272818\n",
            "Iteration: 766, loss: -0.27046016234741344\n",
            "Iteration: 767, loss: -0.2720820984731544\n",
            "Iteration: 768, loss: -0.27321952849952447\n",
            "Iteration: 769, loss: -0.2737945616764091\n",
            "Iteration: 770, loss: -0.27397201778304153\n",
            "Iteration: 771, loss: -0.27406733415412116\n",
            "Iteration: 772, loss: -0.27436428633833715\n",
            "Iteration: 773, loss: -0.27503693302454685\n",
            "Iteration: 774, loss: -0.27602781191091924\n",
            "Iteration: 775, loss: -0.2771888695182084\n",
            "Iteration: 776, loss: -0.2783295430213884\n",
            "Iteration: 777, loss: -0.27925801730329103\n",
            "Iteration: 778, loss: -0.2799676340344297\n",
            "Iteration: 779, loss: -0.28045102448660497\n",
            "Iteration: 780, loss: -0.28086059328442264\n",
            "Iteration: 781, loss: -0.28120302595230273\n",
            "Iteration: 782, loss: -0.2816003405309325\n",
            "Iteration: 783, loss: -0.28210147115387624\n",
            "Iteration: 784, loss: -0.2826803871008866\n",
            "Iteration: 785, loss: -0.2834108829934975\n",
            "Iteration: 786, loss: -0.28424137102842845\n",
            "Iteration: 787, loss: -0.28511118738437785\n",
            "Iteration: 788, loss: -0.28605215528870365\n",
            "Iteration: 789, loss: -0.28696678530612735\n",
            "Iteration: 790, loss: -0.28780932650673585\n",
            "Iteration: 791, loss: -0.288618502514393\n",
            "Iteration: 792, loss: -0.28937231272782005\n",
            "Iteration: 793, loss: -0.2900875576957412\n",
            "Iteration: 794, loss: -0.2908024980499264\n",
            "Iteration: 795, loss: -0.29147574914677704\n",
            "Iteration: 796, loss: -0.292162830397887\n",
            "Iteration: 797, loss: -0.29284600556598034\n",
            "Iteration: 798, loss: -0.2935106152747177\n",
            "Iteration: 799, loss: -0.2941674521181787\n",
            "Iteration: 800, loss: -0.2948061707162324\n",
            "Iteration: 801, loss: -0.29535129636002044\n",
            "Iteration: 802, loss: -0.2957425965224534\n",
            "Iteration: 803, loss: -0.2957887407406352\n",
            "Iteration: 804, loss: -0.2951205587469522\n",
            "Iteration: 805, loss: -0.29363052362914016\n",
            "Iteration: 806, loss: -0.2908464147864629\n",
            "Iteration: 807, loss: -0.28866421064955994\n",
            "Iteration: 808, loss: -0.2881549776339482\n",
            "Iteration: 809, loss: -0.29332372328052353\n",
            "Iteration: 810, loss: -0.29955728201700677\n",
            "Iteration: 811, loss: -0.30182051342169264\n",
            "Iteration: 812, loss: -0.29932825644089767\n",
            "Iteration: 813, loss: -0.29642057267156474\n",
            "Iteration: 814, loss: -0.2979025981322305\n",
            "Iteration: 815, loss: -0.3022494935287497\n",
            "Iteration: 816, loss: -0.3051971688050627\n",
            "Iteration: 817, loss: -0.30447978384149343\n",
            "Iteration: 818, loss: -0.3026927271398805\n",
            "Iteration: 819, loss: -0.3035208291506696\n",
            "Iteration: 820, loss: -0.30643847466492574\n",
            "Iteration: 821, loss: -0.30837309776468985\n",
            "Iteration: 822, loss: -0.30811298363905965\n",
            "Iteration: 823, loss: -0.30730452372389455\n",
            "Iteration: 824, loss: -0.3080395585280656\n",
            "Iteration: 825, loss: -0.31007662711119005\n",
            "Iteration: 826, loss: -0.3115435641243302\n",
            "Iteration: 827, loss: -0.31172900416416705\n",
            "Iteration: 828, loss: -0.31149512645480754\n",
            "Iteration: 829, loss: -0.3120563560936174\n",
            "Iteration: 830, loss: -0.31343812091587225\n",
            "Iteration: 831, loss: -0.31467782369376013\n",
            "Iteration: 832, loss: -0.31522705239980553\n",
            "Iteration: 833, loss: -0.31531228233882463\n",
            "Iteration: 834, loss: -0.315657622075394\n",
            "Iteration: 835, loss: -0.3165423830778694\n",
            "Iteration: 836, loss: -0.3176464050251397\n",
            "Iteration: 837, loss: -0.3185529591127665\n",
            "Iteration: 838, loss: -0.31908537025108313\n",
            "Iteration: 839, loss: -0.31946184102191455\n",
            "Iteration: 840, loss: -0.3199458212447381\n",
            "Iteration: 841, loss: -0.3206151453221626\n",
            "Iteration: 842, loss: -0.3214850401626432\n",
            "Iteration: 843, loss: -0.32229620447392904\n",
            "Iteration: 844, loss: -0.3230186300341119\n",
            "Iteration: 845, loss: -0.32360998131091273\n",
            "Iteration: 846, loss: -0.3241396907301207\n",
            "Iteration: 847, loss: -0.3246815213408794\n",
            "Iteration: 848, loss: -0.32527192960118484\n",
            "Iteration: 849, loss: -0.3259350204787006\n",
            "Iteration: 850, loss: -0.32664725195145344\n",
            "Iteration: 851, loss: -0.32736531725555995\n",
            "Iteration: 852, loss: -0.3280758716748805\n",
            "Iteration: 853, loss: -0.3287536103455978\n",
            "Iteration: 854, loss: -0.3294105377797086\n",
            "Iteration: 855, loss: -0.3300399559683305\n",
            "Iteration: 856, loss: -0.3306648074866188\n",
            "Iteration: 857, loss: -0.33125604015188453\n",
            "Iteration: 858, loss: -0.3318335403063025\n",
            "Iteration: 859, loss: -0.3323480449452357\n",
            "Iteration: 860, loss: -0.3328407056277355\n",
            "Iteration: 861, loss: -0.33324375322437993\n",
            "Iteration: 862, loss: -0.3335738756815721\n",
            "Iteration: 863, loss: -0.33383821122000107\n",
            "Iteration: 864, loss: -0.33404950192792443\n",
            "Iteration: 865, loss: -0.3342578611437662\n",
            "Iteration: 866, loss: -0.33446753923234473\n",
            "Iteration: 867, loss: -0.33504141088822065\n",
            "Iteration: 868, loss: -0.335849868409756\n",
            "Iteration: 869, loss: -0.3369962953179826\n",
            "Iteration: 870, loss: -0.3383135638002517\n",
            "Iteration: 871, loss: -0.33956917240513756\n",
            "Iteration: 872, loss: -0.3406857733984094\n",
            "Iteration: 873, loss: -0.3415304840369869\n",
            "Iteration: 874, loss: -0.3421536964019312\n",
            "Iteration: 875, loss: -0.3426265755102613\n",
            "Iteration: 876, loss: -0.34296153446174504\n",
            "Iteration: 877, loss: -0.3432140801578466\n",
            "Iteration: 878, loss: -0.3434071141248246\n",
            "Iteration: 879, loss: -0.34358788150569025\n",
            "Iteration: 880, loss: -0.3437983564567169\n",
            "Iteration: 881, loss: -0.34404684125697227\n",
            "Iteration: 882, loss: -0.344567457596013\n",
            "Iteration: 883, loss: -0.3452547749077296\n",
            "Iteration: 884, loss: -0.3463144548847147\n",
            "Iteration: 885, loss: -0.34751763914988193\n",
            "Iteration: 886, loss: -0.34881062205003494\n",
            "Iteration: 887, loss: -0.3499780834104388\n",
            "Iteration: 888, loss: -0.35092535036782135\n",
            "Iteration: 889, loss: -0.35166299111358\n",
            "Iteration: 890, loss: -0.35218739773946733\n",
            "Iteration: 891, loss: -0.35262186810215757\n",
            "Iteration: 892, loss: -0.35293087616487034\n",
            "Iteration: 893, loss: -0.35316894979885466\n",
            "Iteration: 894, loss: -0.3533053688741592\n",
            "Iteration: 895, loss: -0.3533836303434339\n",
            "Iteration: 896, loss: -0.3533338996353441\n",
            "Iteration: 897, loss: -0.35340886984104586\n",
            "Iteration: 898, loss: -0.3535915578518587\n",
            "Iteration: 899, loss: -0.3542963819707827\n",
            "Iteration: 900, loss: -0.35534971598014437\n",
            "Iteration: 901, loss: -0.35685943825815986\n",
            "Iteration: 902, loss: -0.3584383516386253\n",
            "Iteration: 903, loss: -0.3598806419669007\n",
            "Iteration: 904, loss: -0.3609071680390352\n",
            "Iteration: 905, loss: -0.36149394175581095\n",
            "Iteration: 906, loss: -0.361771852075934\n",
            "Iteration: 907, loss: -0.36182855092207833\n",
            "Iteration: 908, loss: -0.36179880284862337\n",
            "Iteration: 909, loss: -0.36173087633172124\n",
            "Iteration: 910, loss: -0.3619534489355318\n",
            "Iteration: 911, loss: -0.3623667982027472\n",
            "Iteration: 912, loss: -0.3632293298652517\n",
            "Iteration: 913, loss: -0.3644077217955196\n",
            "Iteration: 914, loss: -0.3657713401548148\n",
            "Iteration: 915, loss: -0.3670479573961453\n",
            "Iteration: 916, loss: -0.3681097171792088\n",
            "Iteration: 917, loss: -0.3688983347643552\n",
            "Iteration: 918, loss: -0.3694578384152845\n",
            "Iteration: 919, loss: -0.36986325068784165\n",
            "Iteration: 920, loss: -0.3701537750581454\n",
            "Iteration: 921, loss: -0.3703840873486281\n",
            "Iteration: 922, loss: -0.37054662637300906\n",
            "Iteration: 923, loss: -0.370649664828002\n",
            "Iteration: 924, loss: -0.37067689452540953\n",
            "Iteration: 925, loss: -0.3708818132751436\n",
            "Iteration: 926, loss: -0.3711501681411655\n",
            "Iteration: 927, loss: -0.37191195732320614\n",
            "Iteration: 928, loss: -0.3728871593859869\n",
            "Iteration: 929, loss: -0.3741898452553705\n",
            "Iteration: 930, loss: -0.37557255473079004\n",
            "Iteration: 931, loss: -0.3768378928531923\n",
            "Iteration: 932, loss: -0.37787284054669634\n",
            "Iteration: 933, loss: -0.3786428186610198\n",
            "Iteration: 934, loss: -0.3791934463229392\n",
            "Iteration: 935, loss: -0.379591339165058\n",
            "Iteration: 936, loss: -0.3798714452171725\n",
            "Iteration: 937, loss: -0.3800079900748465\n",
            "Iteration: 938, loss: -0.3800951574680351\n",
            "Iteration: 939, loss: -0.3800490466852972\n",
            "Iteration: 940, loss: -0.3800557014509495\n",
            "Iteration: 941, loss: -0.38007270443184327\n",
            "Iteration: 942, loss: -0.3804679522911859\n",
            "Iteration: 943, loss: -0.3810325046669782\n",
            "Iteration: 944, loss: -0.38223776069353393\n",
            "Iteration: 945, loss: -0.38362801370415595\n",
            "Iteration: 946, loss: -0.38522762776974756\n",
            "Iteration: 947, loss: -0.38660843028193326\n",
            "Iteration: 948, loss: -0.3875566591025186\n",
            "Iteration: 949, loss: -0.38816949689537095\n",
            "Iteration: 950, loss: -0.38847362498584254\n",
            "Iteration: 951, loss: -0.3886339514038492\n",
            "Iteration: 952, loss: -0.38868041274002535\n",
            "Iteration: 953, loss: -0.38875634789822633\n",
            "Iteration: 954, loss: -0.3888489380035978\n",
            "Iteration: 955, loss: -0.3891879061966174\n",
            "Iteration: 956, loss: -0.3896895623033071\n",
            "Iteration: 957, loss: -0.39057668147137237\n",
            "Iteration: 958, loss: -0.3915963817580433\n",
            "Iteration: 959, loss: -0.39280579321452036\n",
            "Iteration: 960, loss: -0.3939537236512421\n",
            "Iteration: 961, loss: -0.3949783089540438\n",
            "Iteration: 962, loss: -0.39582170376125214\n",
            "Iteration: 963, loss: -0.3964728775302952\n",
            "Iteration: 964, loss: -0.3970027211142402\n",
            "Iteration: 965, loss: -0.3974262864450214\n",
            "Iteration: 966, loss: -0.397742038715302\n",
            "Iteration: 967, loss: -0.3979515120326452\n",
            "Iteration: 968, loss: -0.39802782158260697\n",
            "Iteration: 969, loss: -0.39789475108274575\n",
            "Iteration: 970, loss: -0.3976494547412892\n",
            "Iteration: 971, loss: -0.3971080707727095\n",
            "Iteration: 972, loss: -0.39685956457649646\n",
            "Iteration: 973, loss: -0.39684687343821506\n",
            "Iteration: 974, loss: -0.3978422396626836\n",
            "Iteration: 975, loss: -0.39950313449211833\n",
            "Iteration: 976, loss: -0.4018365144531331\n",
            "Iteration: 977, loss: -0.40386489930569414\n",
            "Iteration: 978, loss: -0.40516668828930885\n",
            "Iteration: 979, loss: -0.4056132791628192\n",
            "Iteration: 980, loss: -0.40544206202717\n",
            "Iteration: 981, loss: -0.40501503086018953\n",
            "Iteration: 982, loss: -0.4046333868125034\n",
            "Iteration: 983, loss: -0.4047481816139741\n",
            "Iteration: 984, loss: -0.4053484803910669\n",
            "Iteration: 985, loss: -0.40674764919155976\n",
            "Iteration: 986, loss: -0.40839466824677456\n",
            "Iteration: 987, loss: -0.40991012182103315\n",
            "Iteration: 988, loss: -0.41095565871467066\n",
            "Iteration: 989, loss: -0.41150629968594227\n",
            "Iteration: 990, loss: -0.41165292242621376\n",
            "Iteration: 991, loss: -0.41164045931098736\n",
            "Iteration: 992, loss: -0.41166481073050376\n",
            "Iteration: 993, loss: -0.41181417706764467\n",
            "Iteration: 994, loss: -0.41228636004431596\n",
            "Iteration: 995, loss: -0.41302886909280145\n",
            "Iteration: 996, loss: -0.41404161773882553\n",
            "Iteration: 997, loss: -0.41510211514850287\n",
            "Iteration: 998, loss: -0.41616032758815996\n",
            "Iteration: 999, loss: -0.4170973399163494\n",
            "Iteration: 1000, loss: -0.41785101572864375\n",
            "Iteration: 1001, loss: -0.4184951537457014\n",
            "Iteration: 1002, loss: -0.4190479838170355\n",
            "Iteration: 1003, loss: -0.4195133626814429\n",
            "Iteration: 1004, loss: -0.4199248269193315\n",
            "Iteration: 1005, loss: -0.4202579632538201\n",
            "Iteration: 1006, loss: -0.42043727174669066\n",
            "Iteration: 1007, loss: -0.42048697114163647\n",
            "Iteration: 1008, loss: -0.4202372877777543\n",
            "Iteration: 1009, loss: -0.4198031428336199\n",
            "Iteration: 1010, loss: -0.41902777845074246\n",
            "Iteration: 1011, loss: -0.4185064021741291\n",
            "Iteration: 1012, loss: -0.41822954439026694\n",
            "Iteration: 1013, loss: -0.41946167428395686\n",
            "Iteration: 1014, loss: -0.4215743776843694\n",
            "Iteration: 1015, loss: -0.4243718394678093\n",
            "Iteration: 1016, loss: -0.4265487266616849\n",
            "Iteration: 1017, loss: -0.42755860556936987\n",
            "Iteration: 1018, loss: -0.4275373615561487\n",
            "Iteration: 1019, loss: -0.42690175785331075\n",
            "Iteration: 1020, loss: -0.42624347429719867\n",
            "Iteration: 1021, loss: -0.4258897160179414\n",
            "Iteration: 1022, loss: -0.4264544534264687\n",
            "Iteration: 1023, loss: -0.4278084726014989\n",
            "Iteration: 1024, loss: -0.4297676529616742\n",
            "Iteration: 1025, loss: -0.4314758463123003\n",
            "Iteration: 1026, loss: -0.43255002474780496\n",
            "Iteration: 1027, loss: -0.43295392194277\n",
            "Iteration: 1028, loss: -0.43290253545650775\n",
            "Iteration: 1029, loss: -0.43273197463657903\n",
            "Iteration: 1030, loss: -0.4326653553715743\n",
            "Iteration: 1031, loss: -0.4329260074342286\n",
            "Iteration: 1032, loss: -0.4335969794690603\n",
            "Iteration: 1033, loss: -0.4346776239417604\n",
            "Iteration: 1034, loss: -0.43593699357564103\n",
            "Iteration: 1035, loss: -0.4370864917100149\n",
            "Iteration: 1036, loss: -0.4380139563329717\n",
            "Iteration: 1037, loss: -0.4386509024426477\n",
            "Iteration: 1038, loss: -0.4390922284381572\n",
            "Iteration: 1039, loss: -0.43940065073689655\n",
            "Iteration: 1040, loss: -0.43957554933722476\n",
            "Iteration: 1041, loss: -0.43965463024879875\n",
            "Iteration: 1042, loss: -0.43973109408043876\n",
            "Iteration: 1043, loss: -0.4398813336584592\n",
            "Iteration: 1044, loss: -0.44012570937794954\n",
            "Iteration: 1045, loss: -0.4404555531885146\n",
            "Iteration: 1046, loss: -0.4410654713338473\n",
            "Iteration: 1047, loss: -0.44173734796847686\n",
            "Iteration: 1048, loss: -0.44266264858914756\n",
            "Iteration: 1049, loss: -0.443686919445369\n",
            "Iteration: 1050, loss: -0.4447394537214046\n",
            "Iteration: 1051, loss: -0.4457627284289682\n",
            "Iteration: 1052, loss: -0.4466398120881705\n",
            "Iteration: 1053, loss: -0.44740199267024033\n",
            "Iteration: 1054, loss: -0.44802760037178857\n",
            "Iteration: 1055, loss: -0.4485957320498741\n",
            "Iteration: 1056, loss: -0.4490961097091479\n",
            "Iteration: 1057, loss: -0.4495343698433378\n",
            "Iteration: 1058, loss: -0.44989194316921244\n",
            "Iteration: 1059, loss: -0.45012843847299655\n",
            "Iteration: 1060, loss: -0.4501746776138159\n",
            "Iteration: 1061, loss: -0.4499398795935705\n",
            "Iteration: 1062, loss: -0.4492711171710038\n",
            "Iteration: 1063, loss: -0.4481987574691044\n",
            "Iteration: 1064, loss: -0.4466782909713181\n",
            "Iteration: 1065, loss: -0.4458599385186474\n",
            "Iteration: 1066, loss: -0.44586990553936723\n",
            "Iteration: 1067, loss: -0.4482353513097064\n",
            "Iteration: 1068, loss: -0.451669843783918\n",
            "Iteration: 1069, loss: -0.45502286814959414\n",
            "Iteration: 1070, loss: -0.45672498495035846\n",
            "Iteration: 1071, loss: -0.45654698124199733\n",
            "Iteration: 1072, loss: -0.4552717614557578\n",
            "Iteration: 1073, loss: -0.45392356738141443\n",
            "Iteration: 1074, loss: -0.4536276337875687\n",
            "Iteration: 1075, loss: -0.4547171034554535\n",
            "Iteration: 1076, loss: -0.4571537286516642\n",
            "Iteration: 1077, loss: -0.45955920420615665\n",
            "Iteration: 1078, loss: -0.46097405269987335\n",
            "Iteration: 1079, loss: -0.46117290403787337\n",
            "Iteration: 1080, loss: -0.4606245770861898\n",
            "Iteration: 1081, loss: -0.46010947979759625\n",
            "Iteration: 1082, loss: -0.4601766354694756\n",
            "Iteration: 1083, loss: -0.46108105591828197\n",
            "Iteration: 1084, loss: -0.46252508472102827\n",
            "Iteration: 1085, loss: -0.46405721127322275\n",
            "Iteration: 1086, loss: -0.4651939337482853\n",
            "Iteration: 1087, loss: -0.4657117153292238\n",
            "Iteration: 1088, loss: -0.46579663771177116\n",
            "Iteration: 1089, loss: -0.46570868185162057\n",
            "Iteration: 1090, loss: -0.4657308912385838\n",
            "Iteration: 1091, loss: -0.4660439659925497\n",
            "Iteration: 1092, loss: -0.46670692220184334\n",
            "Iteration: 1093, loss: -0.4676306640609018\n",
            "Iteration: 1094, loss: -0.46867449864402566\n",
            "Iteration: 1095, loss: -0.4696553862237306\n",
            "Iteration: 1096, loss: -0.4704579755613371\n",
            "Iteration: 1097, loss: -0.471115293286672\n",
            "Iteration: 1098, loss: -0.47158551004887045\n",
            "Iteration: 1099, loss: -0.4719574112249459\n",
            "Iteration: 1100, loss: -0.4722799621136755\n",
            "Iteration: 1101, loss: -0.4725625913594501\n",
            "Iteration: 1102, loss: -0.47280020088438546\n",
            "Iteration: 1103, loss: -0.47301071514657533\n",
            "Iteration: 1104, loss: -0.47317352738003704\n",
            "Iteration: 1105, loss: -0.47335547884202395\n",
            "Iteration: 1106, loss: -0.47345272514077247\n",
            "Iteration: 1107, loss: -0.4736910452555216\n",
            "Iteration: 1108, loss: -0.47396157431945496\n",
            "Iteration: 1109, loss: -0.47451717541529204\n",
            "Iteration: 1110, loss: -0.4751047981756721\n",
            "Iteration: 1111, loss: -0.47595935075804996\n",
            "Iteration: 1112, loss: -0.47689542936526347\n",
            "Iteration: 1113, loss: -0.4779870160801564\n",
            "Iteration: 1114, loss: -0.4790904522717406\n",
            "Iteration: 1115, loss: -0.4800747597498678\n",
            "Iteration: 1116, loss: -0.48090786843202293\n",
            "Iteration: 1117, loss: -0.48162062038849013\n",
            "Iteration: 1118, loss: -0.48220138991842815\n",
            "Iteration: 1119, loss: -0.48273694083308716\n",
            "Iteration: 1120, loss: -0.48318528785045883\n",
            "Iteration: 1121, loss: -0.4835838471373387\n",
            "Iteration: 1122, loss: -0.48392410191766383\n",
            "Iteration: 1123, loss: -0.4841202776067002\n",
            "Iteration: 1124, loss: -0.4842085508887428\n",
            "Iteration: 1125, loss: -0.4840202233623107\n",
            "Iteration: 1126, loss: -0.48354854537926883\n",
            "Iteration: 1127, loss: -0.4825429954620622\n",
            "Iteration: 1128, loss: -0.48146215478489\n",
            "Iteration: 1129, loss: -0.48027638657157357\n",
            "Iteration: 1130, loss: -0.4802560629347858\n",
            "Iteration: 1131, loss: -0.4814381826589995\n",
            "Iteration: 1132, loss: -0.4842680297440921\n",
            "Iteration: 1133, loss: -0.4876224086375871\n",
            "Iteration: 1134, loss: -0.4899356033002055\n",
            "Iteration: 1135, loss: -0.4909751569916191\n",
            "Iteration: 1136, loss: -0.49060236354771214\n",
            "Iteration: 1137, loss: -0.4897155495018899\n",
            "Iteration: 1138, loss: -0.488945418621881\n",
            "Iteration: 1139, loss: -0.48890256388933545\n",
            "Iteration: 1140, loss: -0.4897948098423085\n",
            "Iteration: 1141, loss: -0.49135220231375115\n",
            "Iteration: 1142, loss: -0.4932693527276814\n",
            "Iteration: 1143, loss: -0.4947254207121364\n",
            "Iteration: 1144, loss: -0.4956162582454706\n",
            "Iteration: 1145, loss: -0.49583649546825037\n",
            "Iteration: 1146, loss: -0.49581768935852666\n",
            "Iteration: 1147, loss: -0.495678723569127\n",
            "Iteration: 1148, loss: -0.49579326652564404\n",
            "Iteration: 1149, loss: -0.4961991359083181\n",
            "Iteration: 1150, loss: -0.49695440547265934\n",
            "Iteration: 1151, loss: -0.49791876209459496\n",
            "Iteration: 1152, loss: -0.4988914145660934\n",
            "Iteration: 1153, loss: -0.499851426363154\n",
            "Iteration: 1154, loss: -0.5006006284527538\n",
            "Iteration: 1155, loss: -0.5012515626701651\n",
            "Iteration: 1156, loss: -0.5017474898088689\n",
            "Iteration: 1157, loss: -0.5021537331179766\n",
            "Iteration: 1158, loss: -0.502506677720867\n",
            "Iteration: 1159, loss: -0.502825324334109\n",
            "Iteration: 1160, loss: -0.5030848298523797\n",
            "Iteration: 1161, loss: -0.5032925772920013\n",
            "Iteration: 1162, loss: -0.5034275477452511\n",
            "Iteration: 1163, loss: -0.5035198765761291\n",
            "Iteration: 1164, loss: -0.5035384663484095\n",
            "Iteration: 1165, loss: -0.5035208841234473\n",
            "Iteration: 1166, loss: -0.5034302311899085\n",
            "Iteration: 1167, loss: -0.5034838728231116\n",
            "Iteration: 1168, loss: -0.5036034844180977\n",
            "Iteration: 1169, loss: -0.5042156187795005\n",
            "Iteration: 1170, loss: -0.5051336216115575\n",
            "Iteration: 1171, loss: -0.5064959315988363\n",
            "Iteration: 1172, loss: -0.5080165841407384\n",
            "Iteration: 1173, loss: -0.5093814759535387\n",
            "Iteration: 1174, loss: -0.5105650973218027\n",
            "Iteration: 1175, loss: -0.5113811436331337\n",
            "Iteration: 1176, loss: -0.5119432181064376\n",
            "Iteration: 1177, loss: -0.5122515719845433\n",
            "Iteration: 1178, loss: -0.5124266655166666\n",
            "Iteration: 1179, loss: -0.5124426281104492\n",
            "Iteration: 1180, loss: -0.5123953340795608\n",
            "Iteration: 1181, loss: -0.5121842205887434\n",
            "Iteration: 1182, loss: -0.5119142305481967\n",
            "Iteration: 1183, loss: -0.511568346100016\n",
            "Iteration: 1184, loss: -0.5114395963887236\n",
            "Iteration: 1185, loss: -0.5115184656857186\n",
            "Iteration: 1186, loss: -0.5121889233767044\n",
            "Iteration: 1187, loss: -0.5132746022605047\n",
            "Iteration: 1188, loss: -0.5149078424810516\n",
            "Iteration: 1189, loss: -0.5166711698218938\n",
            "Iteration: 1190, loss: -0.5182047969721156\n",
            "Iteration: 1191, loss: -0.5193051626176663\n",
            "Iteration: 1192, loss: -0.5198955829076515\n",
            "Iteration: 1193, loss: -0.520104828880295\n",
            "Iteration: 1194, loss: -0.5200718330329214\n",
            "Iteration: 1195, loss: -0.5198895594750034\n",
            "Iteration: 1196, loss: -0.5196382137325816\n",
            "Iteration: 1197, loss: -0.5195065954868006\n",
            "Iteration: 1198, loss: -0.5194368413759031\n",
            "Iteration: 1199, loss: -0.5197165431919669\n",
            "Iteration: 1200, loss: -0.520223690951449\n",
            "Iteration: 1201, loss: -0.5212310724101348\n",
            "Iteration: 1202, loss: -0.5224600567708713\n",
            "Iteration: 1203, loss: -0.523841543099349\n",
            "Iteration: 1204, loss: -0.5250699998980742\n",
            "Iteration: 1205, loss: -0.5260639977475257\n",
            "Iteration: 1206, loss: -0.5267780471899576\n",
            "Iteration: 1207, loss: -0.5272493778724001\n",
            "Iteration: 1208, loss: -0.5275446108301496\n",
            "Iteration: 1209, loss: -0.5276902382853561\n",
            "Iteration: 1210, loss: -0.5277231304788741\n",
            "Iteration: 1211, loss: -0.5276111400486471\n",
            "Iteration: 1212, loss: -0.5274010815729109\n",
            "Iteration: 1213, loss: -0.5270541700999547\n",
            "Iteration: 1214, loss: -0.526653467130803\n",
            "Iteration: 1215, loss: -0.5262849652565121\n",
            "Iteration: 1216, loss: -0.5262569522127488\n",
            "Iteration: 1217, loss: -0.5266460905830596\n",
            "Iteration: 1218, loss: -0.5278380155643217\n",
            "Iteration: 1219, loss: -0.5294999267335183\n",
            "Iteration: 1220, loss: -0.531476862752897\n",
            "Iteration: 1221, loss: -0.5332077407451973\n",
            "Iteration: 1222, loss: -0.5343556152423149\n",
            "Iteration: 1223, loss: -0.5349508824569628\n",
            "Iteration: 1224, loss: -0.5350460553856342\n",
            "Iteration: 1225, loss: -0.5348969519918817\n",
            "Iteration: 1226, loss: -0.534631406685423\n",
            "Iteration: 1227, loss: -0.5343949458714298\n",
            "Iteration: 1228, loss: -0.534251863186023\n",
            "Iteration: 1229, loss: -0.5344087981806731\n",
            "Iteration: 1230, loss: -0.5348539511266902\n",
            "Iteration: 1231, loss: -0.5357378628599787\n",
            "Iteration: 1232, loss: -0.5369106987641171\n",
            "Iteration: 1233, loss: -0.5382232703740075\n",
            "Iteration: 1234, loss: -0.5394466303568581\n",
            "Iteration: 1235, loss: -0.5404294056555715\n",
            "Iteration: 1236, loss: -0.5411548662157943\n",
            "Iteration: 1237, loss: -0.5416350908804143\n",
            "Iteration: 1238, loss: -0.5419294532639296\n",
            "Iteration: 1239, loss: -0.5421075701405411\n",
            "Iteration: 1240, loss: -0.5421757391886274\n",
            "Iteration: 1241, loss: -0.542170981970419\n",
            "Iteration: 1242, loss: -0.5420771537747434\n",
            "Iteration: 1243, loss: -0.5418596828735872\n",
            "Iteration: 1244, loss: -0.5416507900778477\n",
            "Iteration: 1245, loss: -0.5414066145348659\n",
            "Iteration: 1246, loss: -0.5414125382991061\n",
            "Iteration: 1247, loss: -0.5416333378467422\n",
            "Iteration: 1248, loss: -0.5424041136364924\n",
            "Iteration: 1249, loss: -0.543499010033247\n",
            "Iteration: 1250, loss: -0.5450217898729965\n",
            "Iteration: 1251, loss: -0.5466014877492883\n",
            "Iteration: 1252, loss: -0.5479648475480787\n",
            "Iteration: 1253, loss: -0.5490130341488362\n",
            "Iteration: 1254, loss: -0.5496420038114691\n",
            "Iteration: 1255, loss: -0.5499948121445052\n",
            "Iteration: 1256, loss: -0.5501163635902212\n",
            "Iteration: 1257, loss: -0.5500924541712986\n",
            "Iteration: 1258, loss: -0.5499755987865044\n",
            "Iteration: 1259, loss: -0.5497753332744563\n",
            "Iteration: 1260, loss: -0.5496347923707351\n",
            "Iteration: 1261, loss: -0.5495815786355629\n",
            "Iteration: 1262, loss: -0.5496487614357619\n",
            "Iteration: 1263, loss: -0.5499963915693828\n",
            "Iteration: 1264, loss: -0.5505678698078007\n",
            "Iteration: 1265, loss: -0.5516093469658251\n",
            "Iteration: 1266, loss: -0.5528254105472408\n",
            "Iteration: 1267, loss: -0.5541159324535843\n",
            "Iteration: 1268, loss: -0.5553249381118817\n",
            "Iteration: 1269, loss: -0.5563037182819229\n",
            "Iteration: 1270, loss: -0.5570442122677679\n",
            "Iteration: 1271, loss: -0.5575327485080691\n",
            "Iteration: 1272, loss: -0.5578483031616271\n",
            "Iteration: 1273, loss: -0.5580154483737679\n",
            "Iteration: 1274, loss: -0.5580647961946845\n",
            "Iteration: 1275, loss: -0.5579836824998516\n",
            "Iteration: 1276, loss: -0.5577457517776241\n",
            "Iteration: 1277, loss: -0.557340309042914\n",
            "Iteration: 1278, loss: -0.5568448293672745\n",
            "Iteration: 1279, loss: -0.5562308398655175\n",
            "Iteration: 1280, loss: -0.5560119979513797\n",
            "Iteration: 1281, loss: -0.5560891028216622\n",
            "Iteration: 1282, loss: -0.5570357336214324\n",
            "Iteration: 1283, loss: -0.5585566064390075\n",
            "Iteration: 1284, loss: -0.5605378926958642\n",
            "Iteration: 1285, loss: -0.5624819457748531\n",
            "Iteration: 1286, loss: -0.5639751268552065\n",
            "Iteration: 1287, loss: -0.5648253497518042\n",
            "Iteration: 1288, loss: -0.565058664004345\n",
            "Iteration: 1289, loss: -0.5649395537148318\n",
            "Iteration: 1290, loss: -0.5645402706971174\n",
            "Iteration: 1291, loss: -0.5642114705379444\n",
            "Iteration: 1292, loss: -0.5639371007964451\n",
            "Iteration: 1293, loss: -0.5640617914295797\n",
            "Iteration: 1294, loss: -0.5645009632283517\n",
            "Iteration: 1295, loss: -0.565390472343595\n",
            "Iteration: 1296, loss: -0.5665243439912814\n",
            "Iteration: 1297, loss: -0.5678749347018284\n",
            "Iteration: 1298, loss: -0.5690777092214316\n",
            "Iteration: 1299, loss: -0.5700174617840613\n",
            "Iteration: 1300, loss: -0.5706716740496749\n",
            "Iteration: 1301, loss: -0.5710904179717796\n",
            "Iteration: 1302, loss: -0.5713187498391138\n",
            "Iteration: 1303, loss: -0.5714448611209457\n",
            "Iteration: 1304, loss: -0.5714277363202004\n",
            "Iteration: 1305, loss: -0.5713834356052417\n",
            "Iteration: 1306, loss: -0.5712496925810217\n",
            "Iteration: 1307, loss: -0.5710641054693733\n",
            "Iteration: 1308, loss: -0.5708434719379935\n",
            "Iteration: 1309, loss: -0.5706295462455646\n",
            "Iteration: 1310, loss: -0.570669802731946\n",
            "Iteration: 1311, loss: -0.5710271485224289\n",
            "Iteration: 1312, loss: -0.571937675483554\n",
            "Iteration: 1313, loss: -0.5731989067004206\n",
            "Iteration: 1314, loss: -0.5746742631718019\n",
            "Iteration: 1315, loss: -0.5760883620367367\n",
            "Iteration: 1316, loss: -0.5773162965431851\n",
            "Iteration: 1317, loss: -0.5782145146093175\n",
            "Iteration: 1318, loss: -0.5787708177357694\n",
            "Iteration: 1319, loss: -0.5790902677200913\n",
            "Iteration: 1320, loss: -0.5792313732869955\n",
            "Iteration: 1321, loss: -0.5792440360332701\n",
            "Iteration: 1322, loss: -0.579205159793712\n",
            "Iteration: 1323, loss: -0.5790271172151398\n",
            "Iteration: 1324, loss: -0.5788022075624502\n",
            "Iteration: 1325, loss: -0.5785612714049012\n",
            "Iteration: 1326, loss: -0.5783190285391985\n",
            "Iteration: 1327, loss: -0.5782518579137749\n",
            "Iteration: 1328, loss: -0.5783549224328527\n",
            "Iteration: 1329, loss: -0.5789501737916201\n",
            "Iteration: 1330, loss: -0.5799778149263046\n",
            "Iteration: 1331, loss: -0.5814521682026275\n",
            "Iteration: 1332, loss: -0.583025796056343\n",
            "Iteration: 1333, loss: -0.5844468194725114\n",
            "Iteration: 1334, loss: -0.5855577616758295\n",
            "Iteration: 1335, loss: -0.5862815581722659\n",
            "Iteration: 1336, loss: -0.5866860356448622\n",
            "Iteration: 1337, loss: -0.5868405149944763\n",
            "Iteration: 1338, loss: -0.5868358209358501\n",
            "Iteration: 1339, loss: -0.5867110720614739\n",
            "Iteration: 1340, loss: -0.586509176138303\n",
            "Iteration: 1341, loss: -0.5862091999521695\n",
            "Iteration: 1342, loss: -0.5859503414742958\n",
            "Iteration: 1343, loss: -0.5857091504732148\n",
            "Iteration: 1344, loss: -0.5857196449108812\n",
            "Iteration: 1345, loss: -0.585930128319119\n",
            "Iteration: 1346, loss: -0.5866628748811568\n",
            "Iteration: 1347, loss: -0.5877598156651438\n",
            "Iteration: 1348, loss: -0.5892408519366865\n",
            "Iteration: 1349, loss: -0.5907333562524576\n",
            "Iteration: 1350, loss: -0.5920729071678361\n",
            "Iteration: 1351, loss: -0.5930705697542221\n",
            "Iteration: 1352, loss: -0.5937074282800661\n",
            "Iteration: 1353, loss: -0.5940267564485616\n",
            "Iteration: 1354, loss: -0.5941302513695605\n",
            "Iteration: 1355, loss: -0.5940779061640371\n",
            "Iteration: 1356, loss: -0.5939276826804674\n",
            "Iteration: 1357, loss: -0.5937208161946895\n",
            "Iteration: 1358, loss: -0.5933872634388168\n",
            "Iteration: 1359, loss: -0.5930904929965135\n",
            "Iteration: 1360, loss: -0.592814856462325\n",
            "Iteration: 1361, loss: -0.5928538611643469\n",
            "Iteration: 1362, loss: -0.5931176427517105\n",
            "Iteration: 1363, loss: -0.5939256028337688\n",
            "Iteration: 1364, loss: -0.5951703691993385\n",
            "Iteration: 1365, loss: -0.5967138151150205\n",
            "Iteration: 1366, loss: -0.5982286293969719\n",
            "Iteration: 1367, loss: -0.5995208030540666\n",
            "Iteration: 1368, loss: -0.6004725587037856\n",
            "Iteration: 1369, loss: -0.6010107881040668\n",
            "Iteration: 1370, loss: -0.601258760281255\n",
            "Iteration: 1371, loss: -0.6012981138937495\n",
            "Iteration: 1372, loss: -0.6012110745064566\n",
            "Iteration: 1373, loss: -0.6010267173821299\n",
            "Iteration: 1374, loss: -0.6007925078491511\n",
            "Iteration: 1375, loss: -0.6004735247658229\n",
            "Iteration: 1376, loss: -0.6002731769306645\n",
            "Iteration: 1377, loss: -0.6002082934077159\n",
            "Iteration: 1378, loss: -0.6004815274316709\n",
            "Iteration: 1379, loss: -0.6009453584990188\n",
            "Iteration: 1380, loss: -0.6018872941046951\n",
            "Iteration: 1381, loss: -0.603071029869208\n",
            "Iteration: 1382, loss: -0.6044938524222154\n",
            "Iteration: 1383, loss: -0.6057936284371293\n",
            "Iteration: 1384, loss: -0.6069225672300069\n",
            "Iteration: 1385, loss: -0.6077496952494822\n",
            "Iteration: 1386, loss: -0.6082896221292288\n",
            "Iteration: 1387, loss: -0.6086013183064115\n",
            "Iteration: 1388, loss: -0.6087435027580055\n",
            "Iteration: 1389, loss: -0.6087379993580712\n",
            "Iteration: 1390, loss: -0.608580061906553\n",
            "Iteration: 1391, loss: -0.6083308113039407\n",
            "Iteration: 1392, loss: -0.6079284567920026\n",
            "Iteration: 1393, loss: -0.6075104130430964\n",
            "Iteration: 1394, loss: -0.607068315334709\n",
            "Iteration: 1395, loss: -0.6067949460986438\n",
            "Iteration: 1396, loss: -0.6067175547402337\n",
            "Iteration: 1397, loss: -0.6072971979549329\n",
            "Iteration: 1398, loss: -0.6083683058674408\n",
            "Iteration: 1399, loss: -0.6100946148948853\n",
            "Iteration: 1400, loss: -0.6119468871741203\n",
            "Iteration: 1401, loss: -0.6136166626998647\n",
            "Iteration: 1402, loss: -0.6148181864587461\n",
            "Iteration: 1403, loss: -0.6155043067792075\n",
            "Iteration: 1404, loss: -0.6157613980868628\n",
            "Iteration: 1405, loss: -0.6157134961626923\n",
            "Iteration: 1406, loss: -0.6154779487226827\n",
            "Iteration: 1407, loss: -0.6151201092892602\n",
            "Iteration: 1408, loss: -0.6147449418899558\n",
            "Iteration: 1409, loss: -0.6143583900772159\n",
            "Iteration: 1410, loss: -0.6141927267857268\n",
            "Iteration: 1411, loss: -0.6142317746287556\n",
            "Iteration: 1412, loss: -0.6147861835788632\n",
            "Iteration: 1413, loss: -0.6157330507891657\n",
            "Iteration: 1414, loss: -0.6170484311831067\n",
            "Iteration: 1415, loss: -0.6184604232304192\n",
            "Iteration: 1416, loss: -0.619786923551358\n",
            "Iteration: 1417, loss: -0.6208930681126843\n",
            "Iteration: 1418, loss: -0.621684270179461\n",
            "Iteration: 1419, loss: -0.6221799620806234\n",
            "Iteration: 1420, loss: -0.622495969427026\n",
            "Iteration: 1421, loss: -0.6226426156285799\n",
            "Iteration: 1422, loss: -0.6226940629377726\n",
            "Iteration: 1423, loss: -0.6226065423558481\n",
            "Iteration: 1424, loss: -0.6224629896643955\n",
            "Iteration: 1425, loss: -0.6221943355902884\n",
            "Iteration: 1426, loss: -0.6218383909790718\n",
            "Iteration: 1427, loss: -0.6214175522224086\n",
            "Iteration: 1428, loss: -0.6209270821254601\n",
            "Iteration: 1429, loss: -0.6207351248826676\n",
            "Iteration: 1430, loss: -0.6208588935910972\n",
            "Iteration: 1431, loss: -0.6216474896559409\n",
            "Iteration: 1432, loss: -0.6229072787880356\n",
            "Iteration: 1433, loss: -0.6246216865437036\n",
            "Iteration: 1434, loss: -0.6264440871397828\n",
            "Iteration: 1435, loss: -0.6279934112986969\n",
            "Iteration: 1436, loss: -0.6290808221367283\n",
            "Iteration: 1437, loss: -0.6296513051928178\n",
            "Iteration: 1438, loss: -0.6298485421772697\n",
            "Iteration: 1439, loss: -0.6297638868791771\n",
            "Iteration: 1440, loss: -0.6295543402267902\n",
            "Iteration: 1441, loss: -0.6292027302514005\n",
            "Iteration: 1442, loss: -0.6288770450659957\n",
            "Iteration: 1443, loss: -0.628514448585476\n",
            "Iteration: 1444, loss: -0.6284802682496642\n",
            "Iteration: 1445, loss: -0.6286143382683553\n",
            "Iteration: 1446, loss: -0.6292453592185994\n",
            "Iteration: 1447, loss: -0.6300818337392569\n",
            "Iteration: 1448, loss: -0.631304651552543\n",
            "Iteration: 1449, loss: -0.6325670961148513\n",
            "Iteration: 1450, loss: -0.6338094934700638\n",
            "Iteration: 1451, loss: -0.6348600250746785\n",
            "Iteration: 1452, loss: -0.6356960394648415\n",
            "Iteration: 1453, loss: -0.6362684717618954\n",
            "Iteration: 1454, loss: -0.6366569979430321\n",
            "Iteration: 1455, loss: -0.6368755862354649\n",
            "Iteration: 1456, loss: -0.6369947888039418\n",
            "Iteration: 1457, loss: -0.6370006666215968\n",
            "Iteration: 1458, loss: -0.6368660938627281\n",
            "Iteration: 1459, loss: -0.6366092230644087\n",
            "Iteration: 1460, loss: -0.6362192167128216\n",
            "Iteration: 1461, loss: -0.6356633014824722\n",
            "Iteration: 1462, loss: -0.6350270753207293\n",
            "Iteration: 1463, loss: -0.6345617785244201\n",
            "Iteration: 1464, loss: -0.634183892294523\n",
            "Iteration: 1465, loss: -0.6346234788941292\n",
            "Iteration: 1466, loss: -0.6356765632312578\n",
            "Iteration: 1467, loss: -0.6376393339070947\n",
            "Iteration: 1468, loss: -0.6397899140281345\n",
            "Iteration: 1469, loss: -0.6416830647875572\n",
            "Iteration: 1470, loss: -0.6429848800403996\n",
            "Iteration: 1471, loss: -0.6436377289333672\n",
            "Iteration: 1472, loss: -0.6438057511920778\n",
            "Iteration: 1473, loss: -0.6436374335461633\n",
            "Iteration: 1474, loss: -0.6432718014654859\n",
            "Iteration: 1475, loss: -0.6428177423368758\n",
            "Iteration: 1476, loss: -0.6424756023463879\n",
            "Iteration: 1477, loss: -0.6421710131477392\n",
            "Iteration: 1478, loss: -0.6422801486802636\n",
            "Iteration: 1479, loss: -0.64267311805444\n",
            "Iteration: 1480, loss: -0.6436530253475701\n",
            "Iteration: 1481, loss: -0.6448295578660611\n",
            "Iteration: 1482, loss: -0.6462361555243039\n",
            "Iteration: 1483, loss: -0.6475091355339639\n",
            "Iteration: 1484, loss: -0.6485531499026312\n",
            "Iteration: 1485, loss: -0.6492650105020664\n",
            "Iteration: 1486, loss: -0.6497331056958132\n",
            "Iteration: 1487, loss: -0.649982447983844\n",
            "Iteration: 1488, loss: -0.6500892696876883\n",
            "Iteration: 1489, loss: -0.6500452807041931\n",
            "Iteration: 1490, loss: -0.6498961831404725\n",
            "Iteration: 1491, loss: -0.6496528823122895\n",
            "Iteration: 1492, loss: -0.6492975356095116\n",
            "Iteration: 1493, loss: -0.6489099507821381\n",
            "Iteration: 1494, loss: -0.6484338872930928\n",
            "Iteration: 1495, loss: -0.6481978777550116\n",
            "Iteration: 1496, loss: -0.6481529619686048\n",
            "Iteration: 1497, loss: -0.6487297134824773\n",
            "Iteration: 1498, loss: -0.6496602503259715\n",
            "Iteration: 1499, loss: -0.6511314356246544\n",
            "Iteration: 1500, loss: -0.6527565329295546\n",
            "Iteration: 1501, loss: -0.6543428146486253\n",
            "Iteration: 1502, loss: -0.6555915040827486\n",
            "Iteration: 1503, loss: -0.6564700665498914\n",
            "Iteration: 1504, loss: -0.6569463187475401\n",
            "Iteration: 1505, loss: -0.6571210004784663\n",
            "Iteration: 1506, loss: -0.6570612300485279\n",
            "Iteration: 1507, loss: -0.6568427738732614\n",
            "Iteration: 1508, loss: -0.6565503894947842\n",
            "Iteration: 1509, loss: -0.656139380299559\n",
            "Iteration: 1510, loss: -0.6558244513290138\n",
            "Iteration: 1511, loss: -0.6555366685212496\n",
            "Iteration: 1512, loss: -0.6555257901831094\n",
            "Iteration: 1513, loss: -0.6557948913484163\n",
            "Iteration: 1514, loss: -0.6565891207067757\n",
            "Iteration: 1515, loss: -0.6576906980954343\n",
            "Iteration: 1516, loss: -0.6590690770498593\n",
            "Iteration: 1517, loss: -0.6604331575184887\n",
            "Iteration: 1518, loss: -0.6616383555642735\n",
            "Iteration: 1519, loss: -0.6625794095996821\n",
            "Iteration: 1520, loss: -0.6632725992144871\n",
            "Iteration: 1521, loss: -0.6636921362614461\n",
            "Iteration: 1522, loss: -0.6640055679438512\n",
            "Iteration: 1523, loss: -0.6641228208877499\n",
            "Iteration: 1524, loss: -0.6642164874043167\n",
            "Iteration: 1525, loss: -0.6641355131412188\n",
            "Iteration: 1526, loss: -0.664026864651768\n",
            "Iteration: 1527, loss: -0.6637433078981533\n",
            "Iteration: 1528, loss: -0.6633373987271388\n",
            "Iteration: 1529, loss: -0.6627984324584137\n",
            "Iteration: 1530, loss: -0.6622441599534018\n",
            "Iteration: 1531, loss: -0.6618564449079113\n",
            "Iteration: 1532, loss: -0.6616631650861698\n",
            "Iteration: 1533, loss: -0.6621481796325943\n",
            "Iteration: 1534, loss: -0.6631633122233197\n",
            "Iteration: 1535, loss: -0.6648645086079972\n",
            "Iteration: 1536, loss: -0.6667505307183252\n",
            "Iteration: 1537, loss: -0.6685262162590827\n",
            "Iteration: 1538, loss: -0.6698994034064752\n",
            "Iteration: 1539, loss: -0.6707635416512215\n",
            "Iteration: 1540, loss: -0.6711630333093181\n",
            "Iteration: 1541, loss: -0.6711962753861617\n",
            "Iteration: 1542, loss: -0.6710084817363254\n",
            "Iteration: 1543, loss: -0.6706615470464079\n",
            "Iteration: 1544, loss: -0.6702859014344931\n",
            "Iteration: 1545, loss: -0.6698870346859699\n",
            "Iteration: 1546, loss: -0.6696510234489825\n",
            "Iteration: 1547, loss: -0.6695320709605439\n",
            "Iteration: 1548, loss: -0.669773642159068\n",
            "Iteration: 1549, loss: -0.6702982571480328\n",
            "Iteration: 1550, loss: -0.6713720182347356\n",
            "Iteration: 1551, loss: -0.672563976670632\n",
            "Iteration: 1552, loss: -0.6738801352417298\n",
            "Iteration: 1553, loss: -0.6750961351730745\n",
            "Iteration: 1554, loss: -0.6761304639954014\n",
            "Iteration: 1555, loss: -0.6769354725652745\n",
            "Iteration: 1556, loss: -0.6774740533196711\n",
            "Iteration: 1557, loss: -0.6778473664347906\n",
            "Iteration: 1558, loss: -0.678068292041846\n",
            "Iteration: 1559, loss: -0.6781974627381735\n",
            "Iteration: 1560, loss: -0.678170671421861\n",
            "Iteration: 1561, loss: -0.6780398545868634\n",
            "Iteration: 1562, loss: -0.677763065752297\n",
            "Iteration: 1563, loss: -0.6773203642040353\n",
            "Iteration: 1564, loss: -0.6766266925405857\n",
            "Iteration: 1565, loss: -0.6758070708180369\n",
            "Iteration: 1566, loss: -0.6747984401468905\n",
            "Iteration: 1567, loss: -0.6742711044598834\n",
            "Iteration: 1568, loss: -0.6742909530078082\n",
            "Iteration: 1569, loss: -0.6753254825757301\n",
            "Iteration: 1570, loss: -0.6771534925090044\n",
            "Iteration: 1571, loss: -0.679615400444856\n",
            "Iteration: 1572, loss: -0.6819459865848131\n",
            "Iteration: 1573, loss: -0.6836014370155882\n",
            "Iteration: 1574, loss: -0.6844042109304801\n",
            "Iteration: 1575, loss: -0.6844745955016683\n",
            "Iteration: 1576, loss: -0.684036196131999\n",
            "Iteration: 1577, loss: -0.6834129779626532\n",
            "Iteration: 1578, loss: -0.6827498707576576\n",
            "Iteration: 1579, loss: -0.6823460121745616\n",
            "Iteration: 1580, loss: -0.6823070367800339\n",
            "Iteration: 1581, loss: -0.6826752820681752\n",
            "Iteration: 1582, loss: -0.6836227762334269\n",
            "Iteration: 1583, loss: -0.6849842267193702\n",
            "Iteration: 1584, loss: -0.6864808355536419\n",
            "Iteration: 1585, loss: -0.6877934265391413\n",
            "Iteration: 1586, loss: -0.6887804483676038\n",
            "Iteration: 1587, loss: -0.6893961035675942\n",
            "Iteration: 1588, loss: -0.6896630844633355\n",
            "Iteration: 1589, loss: -0.6897449937606989\n",
            "Iteration: 1590, loss: -0.6896532654545083\n",
            "Iteration: 1591, loss: -0.6894911240074509\n",
            "Iteration: 1592, loss: -0.6892854041699613\n",
            "Iteration: 1593, loss: -0.6890712823641189\n",
            "Iteration: 1594, loss: -0.6888220538116655\n",
            "Iteration: 1595, loss: -0.688764602347982\n",
            "Iteration: 1596, loss: -0.6887599162531718\n",
            "Iteration: 1597, loss: -0.6890214749462129\n",
            "Iteration: 1598, loss: -0.6894438731060818\n",
            "Iteration: 1599, loss: -0.690186006969944\n",
            "Iteration: 1600, loss: -0.6910811360246244\n",
            "Iteration: 1601, loss: -0.6922105738688882\n",
            "Iteration: 1602, loss: -0.693344941699675\n",
            "Iteration: 1603, loss: -0.6943771561295747\n",
            "Iteration: 1604, loss: -0.6952746899590387\n",
            "Iteration: 1605, loss: -0.6959919210123027\n",
            "Iteration: 1606, loss: -0.6965497300990052\n",
            "Iteration: 1607, loss: -0.6969623826898987\n",
            "Iteration: 1608, loss: -0.6972669436372855\n",
            "Iteration: 1609, loss: -0.697479694418485\n",
            "Iteration: 1610, loss: -0.6976298538210224\n",
            "Iteration: 1611, loss: -0.6976675752153025\n",
            "Iteration: 1612, loss: -0.6975939354379166\n",
            "Iteration: 1613, loss: -0.6973372091918775\n",
            "Iteration: 1614, loss: -0.6968865531086755\n",
            "Iteration: 1615, loss: -0.6960657261611705\n",
            "Iteration: 1616, loss: -0.6949876400925445\n",
            "Iteration: 1617, loss: -0.6936741791948373\n",
            "Iteration: 1618, loss: -0.6925162171309769\n",
            "Iteration: 1619, loss: -0.6916754876356319\n",
            "Iteration: 1620, loss: -0.6922380691777499\n",
            "Iteration: 1621, loss: -0.6939272727146331\n",
            "Iteration: 1622, loss: -0.6968277688115246\n",
            "Iteration: 1623, loss: -0.6999258200027249\n",
            "Iteration: 1624, loss: -0.7023330674733587\n",
            "Iteration: 1625, loss: -0.7035932827235127\n",
            "Iteration: 1626, loss: -0.7036815913453588\n",
            "Iteration: 1627, loss: -0.7029931591781221\n",
            "Iteration: 1628, loss: -0.7019937541771393\n",
            "Iteration: 1629, loss: -0.7011412980009163\n",
            "Iteration: 1630, loss: -0.70070979613473\n",
            "Iteration: 1631, loss: -0.7009237797718291\n",
            "Iteration: 1632, loss: -0.7018285238805443\n",
            "Iteration: 1633, loss: -0.7033152052684277\n",
            "Iteration: 1634, loss: -0.7050212222173227\n",
            "Iteration: 1635, loss: -0.7065021583430314\n",
            "Iteration: 1636, loss: -0.7075095776392982\n",
            "Iteration: 1637, loss: -0.7079871161240405\n",
            "Iteration: 1638, loss: -0.7080447604497762\n",
            "Iteration: 1639, loss: -0.7078525238707223\n",
            "Iteration: 1640, loss: -0.7075555660481159\n",
            "Iteration: 1641, loss: -0.7072851984860891\n",
            "Iteration: 1642, loss: -0.707125980700294\n",
            "Iteration: 1643, loss: -0.7072051711899766\n",
            "Iteration: 1644, loss: -0.7075730133960895\n",
            "Iteration: 1645, loss: -0.708154901507019\n",
            "Iteration: 1646, loss: -0.7089280347123309\n",
            "Iteration: 1647, loss: -0.7098039915570937\n",
            "Iteration: 1648, loss: -0.7106842319190251\n",
            "Iteration: 1649, loss: -0.7114744763768597\n",
            "Iteration: 1650, loss: -0.7121809502286263\n",
            "Iteration: 1651, loss: -0.7127963043102855\n",
            "Iteration: 1652, loss: -0.7133390870191867\n",
            "Iteration: 1653, loss: -0.7138042541932769\n",
            "Iteration: 1654, loss: -0.7142267512772948\n",
            "Iteration: 1655, loss: -0.7145941190604673\n",
            "Iteration: 1656, loss: -0.7149629419811133\n",
            "Iteration: 1657, loss: -0.715284712778204\n",
            "Iteration: 1658, loss: -0.7155906638921115\n",
            "Iteration: 1659, loss: -0.7158204996191273\n",
            "Iteration: 1660, loss: -0.7159869634097017\n",
            "Iteration: 1661, loss: -0.7159353017432275\n",
            "Iteration: 1662, loss: -0.7156405580518868\n",
            "Iteration: 1663, loss: -0.7148769203254254\n",
            "Iteration: 1664, loss: -0.7134641893394857\n",
            "Iteration: 1665, loss: -0.7113583197636164\n",
            "Iteration: 1666, loss: -0.708644803653809\n",
            "Iteration: 1667, loss: -0.70609793423623\n",
            "Iteration: 1668, loss: -0.7043660883782522\n",
            "Iteration: 1669, loss: -0.7052610151380632\n",
            "Iteration: 1670, loss: -0.7088255813181915\n",
            "Iteration: 1671, loss: -0.7142486515278592\n",
            "Iteration: 1672, loss: -0.7189167625056692\n",
            "Iteration: 1673, loss: -0.7210009122718751\n",
            "Iteration: 1674, loss: -0.720445393944913\n",
            "Iteration: 1675, loss: -0.7182481932239319\n",
            "Iteration: 1676, loss: -0.7159945489318225\n",
            "Iteration: 1677, loss: -0.7148460601668785\n",
            "Iteration: 1678, loss: -0.7157113910534785\n",
            "Iteration: 1679, loss: -0.7181612548624725\n",
            "Iteration: 1680, loss: -0.7212175239866896\n",
            "Iteration: 1681, loss: -0.7234412051979319\n",
            "Iteration: 1682, loss: -0.7242306937045598\n",
            "Iteration: 1683, loss: -0.7237416313816197\n",
            "Iteration: 1684, loss: -0.7226884003682943\n",
            "Iteration: 1685, loss: -0.7218474244015548\n",
            "Iteration: 1686, loss: -0.7218188947040995\n",
            "Iteration: 1687, loss: -0.7227689356600576\n",
            "Iteration: 1688, loss: -0.7242216350736821\n",
            "Iteration: 1689, loss: -0.7258017741485714\n",
            "Iteration: 1690, loss: -0.7269518304327599\n",
            "Iteration: 1691, loss: -0.7275165311203077\n",
            "Iteration: 1692, loss: -0.7275706916866939\n",
            "Iteration: 1693, loss: -0.7273679284628052\n",
            "Iteration: 1694, loss: -0.727154212800614\n",
            "Iteration: 1695, loss: -0.7270389329131131\n",
            "Iteration: 1696, loss: -0.7272664177427016\n",
            "Iteration: 1697, loss: -0.7277850682370604\n",
            "Iteration: 1698, loss: -0.7284806496139377\n",
            "Iteration: 1699, loss: -0.7292813143579795\n",
            "Iteration: 1700, loss: -0.7300703900582696\n",
            "Iteration: 1701, loss: -0.7307511783679594\n",
            "Iteration: 1702, loss: -0.7313243803857095\n",
            "Iteration: 1703, loss: -0.731806111907234\n",
            "Iteration: 1704, loss: -0.7321552424712925\n",
            "Iteration: 1705, loss: -0.7325043140332611\n",
            "Iteration: 1706, loss: -0.7327663596660673\n",
            "Iteration: 1707, loss: -0.7330271405301776\n",
            "Iteration: 1708, loss: -0.7332278490516385\n",
            "Iteration: 1709, loss: -0.7333596809744698\n",
            "Iteration: 1710, loss: -0.7334008402772302\n",
            "Iteration: 1711, loss: -0.7333331006988448\n",
            "Iteration: 1712, loss: -0.7331025778929049\n",
            "Iteration: 1713, loss: -0.7326596054554753\n",
            "Iteration: 1714, loss: -0.7320273952954425\n",
            "Iteration: 1715, loss: -0.7312911570079875\n",
            "Iteration: 1716, loss: -0.730476810819419\n",
            "Iteration: 1717, loss: -0.7300059816046339\n",
            "Iteration: 1718, loss: -0.7299038545767144\n",
            "Iteration: 1719, loss: -0.7307255041178591\n",
            "Iteration: 1720, loss: -0.7321666429717912\n",
            "Iteration: 1721, loss: -0.7341116125470207\n",
            "Iteration: 1722, loss: -0.736094333598368\n",
            "Iteration: 1723, loss: -0.7377824450490166\n",
            "Iteration: 1724, loss: -0.7389539076787461\n",
            "Iteration: 1725, loss: -0.7395848465687762\n",
            "Iteration: 1726, loss: -0.7398190471432579\n",
            "Iteration: 1727, loss: -0.7397360354499687\n",
            "Iteration: 1728, loss: -0.7394466861481976\n",
            "Iteration: 1729, loss: -0.7390404501499953\n",
            "Iteration: 1730, loss: -0.7385928157118785\n",
            "Iteration: 1731, loss: -0.7382109392953562\n",
            "Iteration: 1732, loss: -0.7379741900969017\n",
            "Iteration: 1733, loss: -0.7379183852087894\n",
            "Iteration: 1734, loss: -0.7381652377369537\n",
            "Iteration: 1735, loss: -0.7387690324468512\n",
            "Iteration: 1736, loss: -0.7397317832041926\n",
            "Iteration: 1737, loss: -0.7409008972399213\n",
            "Iteration: 1738, loss: -0.7421534439277754\n",
            "Iteration: 1739, loss: -0.7433090529537157\n",
            "Iteration: 1740, loss: -0.7442456254613076\n",
            "Iteration: 1741, loss: -0.7449890733371549\n",
            "Iteration: 1742, loss: -0.7455085227301856\n",
            "Iteration: 1743, loss: -0.7459090335667402\n",
            "Iteration: 1744, loss: -0.7461672075209299\n",
            "Iteration: 1745, loss: -0.746361866807631\n",
            "Iteration: 1746, loss: -0.7464596522536426\n",
            "Iteration: 1747, loss: -0.7464763643976844\n",
            "Iteration: 1748, loss: -0.7463418211408549\n",
            "Iteration: 1749, loss: -0.746114495826601\n",
            "Iteration: 1750, loss: -0.7455576489088198\n",
            "Iteration: 1751, loss: -0.7448389158200023\n",
            "Iteration: 1752, loss: -0.7438113686673453\n",
            "Iteration: 1753, loss: -0.742799279582441\n",
            "Iteration: 1754, loss: -0.741736559683717\n",
            "Iteration: 1755, loss: -0.7413415232132632\n",
            "Iteration: 1756, loss: -0.7417577557052363\n",
            "Iteration: 1757, loss: -0.7434002030923864\n",
            "Iteration: 1758, loss: -0.7457110772876911\n",
            "Iteration: 1759, loss: -0.7482811283567284\n",
            "Iteration: 1760, loss: -0.7504397892829741\n",
            "Iteration: 1761, loss: -0.7517817529139145\n",
            "Iteration: 1762, loss: -0.7523966342732975\n",
            "Iteration: 1763, loss: -0.7523663175802922\n",
            "Iteration: 1764, loss: -0.7519028161533557\n",
            "Iteration: 1765, loss: -0.7512696612578422\n",
            "Iteration: 1766, loss: -0.750636175699803\n",
            "Iteration: 1767, loss: -0.7501745238487021\n",
            "Iteration: 1768, loss: -0.7501015409085279\n",
            "Iteration: 1769, loss: -0.750428656663176\n",
            "Iteration: 1770, loss: -0.7512221240864144\n",
            "Iteration: 1771, loss: -0.7523198430227763\n",
            "Iteration: 1772, loss: -0.7535763767431429\n",
            "Iteration: 1773, loss: -0.7548010948732162\n",
            "Iteration: 1774, loss: -0.7558318210832852\n",
            "Iteration: 1775, loss: -0.7566234224536585\n",
            "Iteration: 1776, loss: -0.7571544441846838\n",
            "Iteration: 1777, loss: -0.7574899188014436\n",
            "Iteration: 1778, loss: -0.7576621794603179\n",
            "Iteration: 1779, loss: -0.7577093536397967\n",
            "Iteration: 1780, loss: -0.7576460819044398\n",
            "Iteration: 1781, loss: -0.7574440188513293\n",
            "Iteration: 1782, loss: -0.7571116841640401\n",
            "Iteration: 1783, loss: -0.7566781317129011\n",
            "Iteration: 1784, loss: -0.7561946057098198\n",
            "Iteration: 1785, loss: -0.7556621057804777\n",
            "Iteration: 1786, loss: -0.7552658064073123\n",
            "Iteration: 1787, loss: -0.7550761933559831\n",
            "Iteration: 1788, loss: -0.7550732833407994\n",
            "Iteration: 1789, loss: -0.7556360579535263\n",
            "Iteration: 1790, loss: -0.7567142957172281\n",
            "Iteration: 1791, loss: -0.7581983234693817\n",
            "Iteration: 1792, loss: -0.7597651001239029\n",
            "Iteration: 1793, loss: -0.7612673983064608\n",
            "Iteration: 1794, loss: -0.7624830149744973\n",
            "Iteration: 1795, loss: -0.763322972065843\n",
            "Iteration: 1796, loss: -0.7638405799128427\n",
            "Iteration: 1797, loss: -0.7641005970745297\n",
            "Iteration: 1798, loss: -0.7641881001999109\n",
            "Iteration: 1799, loss: -0.764138491953594\n",
            "Iteration: 1800, loss: -0.7639632523920613\n",
            "Iteration: 1801, loss: -0.7637166629933406\n",
            "Iteration: 1802, loss: -0.7633690450575358\n",
            "Iteration: 1803, loss: -0.7629529595236447\n",
            "Iteration: 1804, loss: -0.7625359219397687\n",
            "Iteration: 1805, loss: -0.7620388930413415\n",
            "Iteration: 1806, loss: -0.7618206163935963\n",
            "Iteration: 1807, loss: -0.7618998563932226\n",
            "Iteration: 1808, loss: -0.7623424614718176\n",
            "Iteration: 1809, loss: -0.7630922789492798\n",
            "Iteration: 1810, loss: -0.7642722280000283\n",
            "Iteration: 1811, loss: -0.7654686115126033\n",
            "Iteration: 1812, loss: -0.7667924456897396\n",
            "Iteration: 1813, loss: -0.768093420065745\n",
            "Iteration: 1814, loss: -0.769166160064837\n",
            "Iteration: 1815, loss: -0.7699106479724571\n",
            "Iteration: 1816, loss: -0.7704430679930077\n",
            "Iteration: 1817, loss: -0.7707207555780791\n",
            "Iteration: 1818, loss: -0.7708970608816667\n",
            "Iteration: 1819, loss: -0.7709375556128866\n",
            "Iteration: 1820, loss: -0.770863415015247\n",
            "Iteration: 1821, loss: -0.770665200448391\n",
            "Iteration: 1822, loss: -0.7703566080036129\n",
            "Iteration: 1823, loss: -0.7698831072835831\n",
            "Iteration: 1824, loss: -0.7693255096652162\n",
            "Iteration: 1825, loss: -0.7686979751843941\n",
            "Iteration: 1826, loss: -0.7681031133870474\n",
            "Iteration: 1827, loss: -0.767647329352305\n",
            "Iteration: 1828, loss: -0.7675000154215753\n",
            "Iteration: 1829, loss: -0.7680304898497335\n",
            "Iteration: 1830, loss: -0.7690876301720824\n",
            "Iteration: 1831, loss: -0.770565314403145\n",
            "Iteration: 1832, loss: -0.7722548543447252\n",
            "Iteration: 1833, loss: -0.7739508548079578\n",
            "Iteration: 1834, loss: -0.7753199652363769\n",
            "Iteration: 1835, loss: -0.7762863830275307\n",
            "Iteration: 1836, loss: -0.7769229340964378\n",
            "Iteration: 1837, loss: -0.777116259787421\n",
            "Iteration: 1838, loss: -0.7771884291765786\n",
            "Iteration: 1839, loss: -0.7770236049781164\n",
            "Iteration: 1840, loss: -0.776795915226917\n",
            "Iteration: 1841, loss: -0.7764995420852217\n",
            "Iteration: 1842, loss: -0.7761761678102489\n",
            "Iteration: 1843, loss: -0.7758621858934016\n",
            "Iteration: 1844, loss: -0.7756056557732383\n",
            "Iteration: 1845, loss: -0.7754030200337626\n",
            "Iteration: 1846, loss: -0.7754642153867725\n",
            "Iteration: 1847, loss: -0.7758310307212016\n",
            "Iteration: 1848, loss: -0.7765747223427482\n",
            "Iteration: 1849, loss: -0.7774646881740289\n",
            "Iteration: 1850, loss: -0.7785367118007676\n",
            "Iteration: 1851, loss: -0.7796646136291104\n",
            "Iteration: 1852, loss: -0.7807039921068948\n",
            "Iteration: 1853, loss: -0.7816456857086236\n",
            "Iteration: 1854, loss: -0.7824014934779746\n",
            "Iteration: 1855, loss: -0.7829571444141379\n",
            "Iteration: 1856, loss: -0.7833730165771203\n",
            "Iteration: 1857, loss: -0.7836977068587275\n",
            "Iteration: 1858, loss: -0.7839480731827712\n",
            "Iteration: 1859, loss: -0.7841479685713033\n",
            "Iteration: 1860, loss: -0.7842727961960091\n",
            "Iteration: 1861, loss: -0.7843125659114208\n",
            "Iteration: 1862, loss: -0.7842030961838732\n",
            "Iteration: 1863, loss: -0.7838650799365023\n",
            "Iteration: 1864, loss: -0.7832360462689812\n",
            "Iteration: 1865, loss: -0.7822792904903562\n",
            "Iteration: 1866, loss: -0.7808277873418699\n",
            "Iteration: 1867, loss: -0.7790941024571264\n",
            "Iteration: 1868, loss: -0.7771444921921252\n",
            "Iteration: 1869, loss: -0.7758217435576423\n",
            "Iteration: 1870, loss: -0.7755330342207881\n",
            "Iteration: 1871, loss: -0.777194328977001\n",
            "Iteration: 1872, loss: -0.7803557644007023\n",
            "Iteration: 1873, loss: -0.7842074189758655\n",
            "Iteration: 1874, loss: -0.7874246990858892\n",
            "Iteration: 1875, loss: -0.7892088672224427\n",
            "Iteration: 1876, loss: -0.7895136925022409\n",
            "Iteration: 1877, loss: -0.788701111940105\n",
            "Iteration: 1878, loss: -0.78739145168304\n",
            "Iteration: 1879, loss: -0.7860363456521541\n",
            "Iteration: 1880, loss: -0.7852160742226756\n",
            "Iteration: 1881, loss: -0.7852713434552147\n",
            "Iteration: 1882, loss: -0.7864080403135307\n",
            "Iteration: 1883, loss: -0.7881816811626592\n",
            "Iteration: 1884, loss: -0.7900863384208673\n",
            "Iteration: 1885, loss: -0.7917049580767476\n",
            "Iteration: 1886, loss: -0.7927143013989439\n",
            "Iteration: 1887, loss: -0.7930753532024815\n",
            "Iteration: 1888, loss: -0.7929700364675938\n",
            "Iteration: 1889, loss: -0.792583308826799\n",
            "Iteration: 1890, loss: -0.7921737863694243\n",
            "Iteration: 1891, loss: -0.7919098580312357\n",
            "Iteration: 1892, loss: -0.791903039473192\n",
            "Iteration: 1893, loss: -0.7921655267861025\n",
            "Iteration: 1894, loss: -0.7926908941458496\n",
            "Iteration: 1895, loss: -0.793427758387344\n",
            "Iteration: 1896, loss: -0.7943043121040192\n",
            "Iteration: 1897, loss: -0.7951339414666475\n",
            "Iteration: 1898, loss: -0.7959142971536248\n",
            "Iteration: 1899, loss: -0.7965385119553989\n",
            "Iteration: 1900, loss: -0.7970472197279964\n",
            "Iteration: 1901, loss: -0.7974566829167725\n",
            "Iteration: 1902, loss: -0.7978206062472376\n",
            "Iteration: 1903, loss: -0.7981101319520103\n",
            "Iteration: 1904, loss: -0.7983644823570664\n",
            "Iteration: 1905, loss: -0.7985881464571646\n",
            "Iteration: 1906, loss: -0.7987685017041547\n",
            "Iteration: 1907, loss: -0.7988566804918026\n",
            "Iteration: 1908, loss: -0.7988786842565722\n",
            "Iteration: 1909, loss: -0.7986809342592242\n",
            "Iteration: 1910, loss: -0.7982978630361901\n",
            "Iteration: 1911, loss: -0.7975423901813649\n",
            "Iteration: 1912, loss: -0.7965366696344203\n",
            "Iteration: 1913, loss: -0.7952624952901415\n",
            "Iteration: 1914, loss: -0.7938229731854416\n",
            "Iteration: 1915, loss: -0.792367712696254\n",
            "Iteration: 1916, loss: -0.7917799215689753\n",
            "Iteration: 1917, loss: -0.7921172289710391\n",
            "Iteration: 1918, loss: -0.7940532033752862\n",
            "Iteration: 1919, loss: -0.7968728850935693\n",
            "Iteration: 1920, loss: -0.7999610836685507\n",
            "Iteration: 1921, loss: -0.802409073329175\n",
            "Iteration: 1922, loss: -0.8037388365668936\n",
            "Iteration: 1923, loss: -0.8041287841165976\n",
            "Iteration: 1924, loss: -0.8037284145608488\n",
            "Iteration: 1925, loss: -0.8029470099969869\n",
            "Iteration: 1926, loss: -0.801947236900426\n",
            "Iteration: 1927, loss: -0.8012401965637989\n",
            "Iteration: 1928, loss: -0.8009472644757086\n",
            "Iteration: 1929, loss: -0.8012868852806365\n",
            "Iteration: 1930, loss: -0.8021226045641163\n",
            "Iteration: 1931, loss: -0.8034353474491995\n",
            "Iteration: 1932, loss: -0.8048441288460565\n",
            "Iteration: 1933, loss: -0.806163467511248\n",
            "Iteration: 1934, loss: -0.8071646302731286\n",
            "Iteration: 1935, loss: -0.8078329961071267\n",
            "Iteration: 1936, loss: -0.8081594781860745\n",
            "Iteration: 1937, loss: -0.8082667768112756\n",
            "Iteration: 1938, loss: -0.8082030995925304\n",
            "Iteration: 1939, loss: -0.8080304217200458\n",
            "Iteration: 1940, loss: -0.8077918123710847\n",
            "Iteration: 1941, loss: -0.8075028414998675\n",
            "Iteration: 1942, loss: -0.8072464118082955\n",
            "Iteration: 1943, loss: -0.8070231486138278\n",
            "Iteration: 1944, loss: -0.8069313195011819\n",
            "Iteration: 1945, loss: -0.806883459137481\n",
            "Iteration: 1946, loss: -0.8070954018824533\n",
            "Iteration: 1947, loss: -0.8074617031964478\n",
            "Iteration: 1948, loss: -0.8080983394325225\n",
            "Iteration: 1949, loss: -0.8089131964042836\n",
            "Iteration: 1950, loss: -0.8097962198819448\n",
            "Iteration: 1951, loss: -0.8107048979246023\n",
            "Iteration: 1952, loss: -0.8115605549796253\n",
            "Iteration: 1953, loss: -0.812339251856288\n",
            "Iteration: 1954, loss: -0.8130280802224716\n",
            "Iteration: 1955, loss: -0.81357704956584\n",
            "Iteration: 1956, loss: -0.8140434086828829\n",
            "Iteration: 1957, loss: -0.81442263499065\n",
            "Iteration: 1958, loss: -0.8147480341033463\n",
            "Iteration: 1959, loss: -0.815020767059205\n",
            "Iteration: 1960, loss: -0.8152598967507442\n",
            "Iteration: 1961, loss: -0.8154510029919576\n",
            "Iteration: 1962, loss: -0.81558108603937\n",
            "Iteration: 1963, loss: -0.8156230317535347\n",
            "Iteration: 1964, loss: -0.8155187660440033\n",
            "Iteration: 1965, loss: -0.8151715550801262\n",
            "Iteration: 1966, loss: -0.8144870852718028\n",
            "Iteration: 1967, loss: -0.813333954197567\n",
            "Iteration: 1968, loss: -0.8116104605030644\n",
            "Iteration: 1969, loss: -0.8094763068956499\n",
            "Iteration: 1970, loss: -0.8070710916863605\n",
            "Iteration: 1971, loss: -0.8052369610497453\n",
            "Iteration: 1972, loss: -0.8046144169401259\n",
            "Iteration: 1973, loss: -0.8063182719166512\n",
            "Iteration: 1974, loss: -0.8100556284026883\n",
            "Iteration: 1975, loss: -0.8146084193099595\n",
            "Iteration: 1976, loss: -0.8183228645286388\n",
            "Iteration: 1977, loss: -0.820167927622238\n",
            "Iteration: 1978, loss: -0.8201524909687599\n",
            "Iteration: 1979, loss: -0.8188506759762053\n",
            "Iteration: 1980, loss: -0.8170288775149147\n",
            "Iteration: 1981, loss: -0.8155372785014977\n",
            "Iteration: 1982, loss: -0.8150576445075947\n",
            "Iteration: 1983, loss: -0.8158566972609536\n",
            "Iteration: 1984, loss: -0.8176688415518265\n",
            "Iteration: 1985, loss: -0.819869590830168\n",
            "Iteration: 1986, loss: -0.821809072168726\n",
            "Iteration: 1987, loss: -0.8230043857923502\n",
            "Iteration: 1988, loss: -0.8234047034142209\n",
            "Iteration: 1989, loss: -0.8231570778774081\n",
            "Iteration: 1990, loss: -0.8225716613626365\n",
            "Iteration: 1991, loss: -0.8219334005541218\n",
            "Iteration: 1992, loss: -0.8215373971581281\n",
            "Iteration: 1993, loss: -0.8215383016348924\n",
            "Iteration: 1994, loss: -0.8219803476099193\n",
            "Iteration: 1995, loss: -0.8228668472718746\n",
            "Iteration: 1996, loss: -0.8239604744049698\n",
            "Iteration: 1997, loss: -0.8250534762426871\n",
            "Iteration: 1998, loss: -0.8259195757632011\n",
            "Iteration: 1999, loss: -0.8265435857040668\n",
            "Iteration: 2000, loss: -0.8269267996881119\n",
            "Iteration: 2001, loss: -0.8271137757430531\n",
            "Iteration: 2002, loss: -0.8272026116536938\n",
            "Iteration: 2003, loss: -0.8271831840376137\n",
            "Iteration: 2004, loss: -0.8271021809725869\n",
            "Iteration: 2005, loss: -0.8269590802883967\n",
            "Iteration: 2006, loss: -0.8268035889919171\n",
            "Iteration: 2007, loss: -0.8266306255682964\n",
            "Iteration: 2008, loss: -0.8264378243006245\n",
            "Iteration: 2009, loss: -0.8262655166916811\n",
            "Iteration: 2010, loss: -0.8261254101199317\n",
            "Iteration: 2011, loss: -0.8261985016187949\n",
            "Iteration: 2012, loss: -0.8263447312913587\n",
            "Iteration: 2013, loss: -0.8267458172196831\n",
            "Iteration: 2014, loss: -0.8272709069984698\n",
            "Iteration: 2015, loss: -0.8280197190120734\n",
            "Iteration: 2016, loss: -0.8287956317451203\n",
            "Iteration: 2017, loss: -0.8296760620614568\n",
            "Iteration: 2018, loss: -0.8305386234061984\n",
            "Iteration: 2019, loss: -0.8313054055909376\n",
            "Iteration: 2020, loss: -0.8319857572017768\n",
            "Iteration: 2021, loss: -0.8325518683441157\n",
            "Iteration: 2022, loss: -0.8330198055813828\n",
            "Iteration: 2023, loss: -0.8334173090944555\n",
            "Iteration: 2024, loss: -0.8337530986008951\n",
            "Iteration: 2025, loss: -0.8340518991186143\n",
            "Iteration: 2026, loss: -0.8343311414109648\n",
            "Iteration: 2027, loss: -0.8345772260506712\n",
            "Iteration: 2028, loss: -0.8348002794220954\n",
            "Iteration: 2029, loss: -0.8350182721142498\n",
            "Iteration: 2030, loss: -0.8351271459429824\n",
            "Iteration: 2031, loss: -0.8352026087144502\n",
            "Iteration: 2032, loss: -0.8351043611839735\n",
            "Iteration: 2033, loss: -0.8347244832652424\n",
            "Iteration: 2034, loss: -0.8339066198713303\n",
            "Iteration: 2035, loss: -0.8325089885235898\n",
            "Iteration: 2036, loss: -0.8303805520998291\n",
            "Iteration: 2037, loss: -0.8276805469949179\n",
            "Iteration: 2038, loss: -0.8248928836823961\n",
            "Iteration: 2039, loss: -0.8230076320474045\n",
            "Iteration: 2040, loss: -0.822752111278206\n",
            "Iteration: 2041, loss: -0.8251224703576283\n",
            "Iteration: 2042, loss: -0.8295069083319332\n",
            "Iteration: 2043, loss: -0.8345401941913169\n",
            "Iteration: 2044, loss: -0.8382482343679297\n",
            "Iteration: 2045, loss: -0.8395747587404528\n",
            "Iteration: 2046, loss: -0.8388243837676016\n",
            "Iteration: 2047, loss: -0.8368817466846256\n",
            "Iteration: 2048, loss: -0.8347585617481011\n",
            "Iteration: 2049, loss: -0.8335334518927001\n",
            "Iteration: 2050, loss: -0.8337267423978936\n",
            "Iteration: 2051, loss: -0.8353476674967081\n",
            "Iteration: 2052, loss: -0.8378204837092733\n",
            "Iteration: 2053, loss: -0.8402766030939669\n",
            "Iteration: 2054, loss: -0.8418280142304726\n",
            "Iteration: 2055, loss: -0.8423074996889055\n",
            "Iteration: 2056, loss: -0.8418940193731004\n",
            "Iteration: 2057, loss: -0.8410559610403953\n",
            "Iteration: 2058, loss: -0.8402989995780464\n",
            "Iteration: 2059, loss: -0.8400299599874574\n",
            "Iteration: 2060, loss: -0.840365805496742\n",
            "Iteration: 2061, loss: -0.8412416800955831\n",
            "Iteration: 2062, loss: -0.842435241418885\n",
            "Iteration: 2063, loss: -0.8436129132049753\n",
            "Iteration: 2064, loss: -0.8445387538401924\n",
            "Iteration: 2065, loss: -0.8450959148663842\n",
            "Iteration: 2066, loss: -0.845370746064166\n",
            "Iteration: 2067, loss: -0.8453745378401236\n",
            "Iteration: 2068, loss: -0.8452838860697455\n",
            "Iteration: 2069, loss: -0.8451320238910005\n",
            "Iteration: 2070, loss: -0.8450006742670332\n",
            "Iteration: 2071, loss: -0.8449101119469922\n",
            "Iteration: 2072, loss: -0.8448768629648762\n",
            "Iteration: 2073, loss: -0.8450057343579582\n",
            "Iteration: 2074, loss: -0.8452442826187536\n",
            "Iteration: 2075, loss: -0.8456486746908508\n",
            "Iteration: 2076, loss: -0.8460515910593063\n",
            "Iteration: 2077, loss: -0.8465454956014599\n",
            "Iteration: 2078, loss: -0.8470926546950079\n",
            "Iteration: 2079, loss: -0.8475855152898686\n",
            "Iteration: 2080, loss: -0.8480524617134048\n",
            "Iteration: 2081, loss: -0.8485119324465756\n",
            "Iteration: 2082, loss: -0.8489233800776488\n",
            "Iteration: 2083, loss: -0.8493428497461396\n",
            "Iteration: 2084, loss: -0.8497290074986141\n",
            "Iteration: 2085, loss: -0.8500819706520856\n",
            "Iteration: 2086, loss: -0.8504176492823624\n",
            "Iteration: 2087, loss: -0.8507420836753101\n",
            "Iteration: 2088, loss: -0.8510200848434881\n",
            "Iteration: 2089, loss: -0.8512664518813374\n",
            "Iteration: 2090, loss: -0.8514781229868127\n",
            "Iteration: 2091, loss: -0.8516420666824274\n",
            "Iteration: 2092, loss: -0.851670086185047\n",
            "Iteration: 2093, loss: -0.8515768923223129\n",
            "Iteration: 2094, loss: -0.8512063754651547\n",
            "Iteration: 2095, loss: -0.8505057454188608\n",
            "Iteration: 2096, loss: -0.8494331863545126\n",
            "Iteration: 2097, loss: -0.8478402916913825\n",
            "Iteration: 2098, loss: -0.8460909878052145\n",
            "Iteration: 2099, loss: -0.8442750949376232\n",
            "Iteration: 2100, loss: -0.8431255846162318\n",
            "Iteration: 2101, loss: -0.8430485400831447\n",
            "Iteration: 2102, loss: -0.8446921734774528\n",
            "Iteration: 2103, loss: -0.8475952914430227\n",
            "Iteration: 2104, loss: -0.8512046117496573\n",
            "Iteration: 2105, loss: -0.8542311012186045\n",
            "Iteration: 2106, loss: -0.8559999544154907\n",
            "Iteration: 2107, loss: -0.8565174710128631\n",
            "Iteration: 2108, loss: -0.8560555774878069\n",
            "Iteration: 2109, loss: -0.8550263843537901\n",
            "Iteration: 2110, loss: -0.8538118460275123\n",
            "Iteration: 2111, loss: -0.8528029550615881\n",
            "Iteration: 2112, loss: -0.8523456286337883\n",
            "Iteration: 2113, loss: -0.8526790055455824\n",
            "Iteration: 2114, loss: -0.8537108415780619\n",
            "Iteration: 2115, loss: -0.8553130168324026\n",
            "Iteration: 2116, loss: -0.8569660652172354\n",
            "Iteration: 2117, loss: -0.8583799116762504\n",
            "Iteration: 2118, loss: -0.859286284336706\n",
            "Iteration: 2119, loss: -0.8597386410584176\n",
            "Iteration: 2120, loss: -0.8598155034664803\n",
            "Iteration: 2121, loss: -0.8595562124444\n",
            "Iteration: 2122, loss: -0.8592352279743076\n",
            "Iteration: 2123, loss: -0.8588877446677158\n",
            "Iteration: 2124, loss: -0.8586144530880165\n",
            "Iteration: 2125, loss: -0.8584715632209277\n",
            "Iteration: 2126, loss: -0.8585393224066172\n",
            "Iteration: 2127, loss: -0.8588881664093175\n",
            "Iteration: 2128, loss: -0.8593652141137765\n",
            "Iteration: 2129, loss: -0.8600064050248943\n",
            "Iteration: 2130, loss: -0.8607020623546453\n",
            "Iteration: 2131, loss: -0.8613620782393238\n",
            "Iteration: 2132, loss: -0.862034889876411\n",
            "Iteration: 2133, loss: -0.8626258948229293\n",
            "Iteration: 2134, loss: -0.863165009078122\n",
            "Iteration: 2135, loss: -0.8635981372009608\n",
            "Iteration: 2136, loss: -0.8640274066448947\n",
            "Iteration: 2137, loss: -0.8643677116588069\n",
            "Iteration: 2138, loss: -0.8647065835950032\n",
            "Iteration: 2139, loss: -0.8649831586533078\n",
            "Iteration: 2140, loss: -0.8652832972353213\n",
            "Iteration: 2141, loss: -0.8655565777513184\n",
            "Iteration: 2142, loss: -0.8658410082802063\n",
            "Iteration: 2143, loss: -0.8661015048736003\n",
            "Iteration: 2144, loss: -0.8663583392242401\n",
            "Iteration: 2145, loss: -0.8666371599910843\n",
            "Iteration: 2146, loss: -0.8669040912210003\n",
            "Iteration: 2147, loss: -0.8671639224613107\n",
            "Iteration: 2148, loss: -0.8674055960711204\n",
            "Iteration: 2149, loss: -0.8676811836244664\n",
            "Iteration: 2150, loss: -0.8679260159981635\n",
            "Iteration: 2151, loss: -0.8681961295562997\n",
            "Iteration: 2152, loss: -0.8684180488782548\n",
            "Iteration: 2153, loss: -0.8685841477436153\n",
            "Iteration: 2154, loss: -0.8686469202499916\n",
            "Iteration: 2155, loss: -0.8684488225567161\n",
            "Iteration: 2156, loss: -0.8677055456786773\n",
            "Iteration: 2157, loss: -0.86620114873482\n",
            "Iteration: 2158, loss: -0.8635019193995451\n",
            "Iteration: 2159, loss: -0.8592335194347144\n",
            "Iteration: 2160, loss: -0.8535185804633388\n",
            "Iteration: 2161, loss: -0.847600003040706\n",
            "Iteration: 2162, loss: -0.8432695103687364\n",
            "Iteration: 2163, loss: -0.8444563926672586\n",
            "Iteration: 2164, loss: -0.8519732538189737\n",
            "Iteration: 2165, loss: -0.8629844308603497\n",
            "Iteration: 2166, loss: -0.8706708195866278\n",
            "Iteration: 2167, loss: -0.8713819119872828\n",
            "Iteration: 2168, loss: -0.8667251019038738\n",
            "Iteration: 2169, loss: -0.8609091180367143\n",
            "Iteration: 2170, loss: -0.858433442070056\n",
            "Iteration: 2171, loss: -0.8611443165949428\n",
            "Iteration: 2172, loss: -0.8671829794304433\n",
            "Iteration: 2173, loss: -0.8723458007231037\n",
            "Iteration: 2174, loss: -0.8737684281228019\n",
            "Iteration: 2175, loss: -0.8716855564610422\n",
            "Iteration: 2176, loss: -0.8685933654925408\n",
            "Iteration: 2177, loss: -0.8671472695038567\n",
            "Iteration: 2178, loss: -0.8686697387811273\n",
            "Iteration: 2179, loss: -0.8719702264892537\n",
            "Iteration: 2180, loss: -0.8747738090778878\n",
            "Iteration: 2181, loss: -0.8756132607077527\n",
            "Iteration: 2182, loss: -0.8746845797205685\n",
            "Iteration: 2183, loss: -0.8731758486515594\n",
            "Iteration: 2184, loss: -0.8724397300950486\n",
            "Iteration: 2185, loss: -0.8730485084080774\n",
            "Iteration: 2186, loss: -0.8746725051672388\n",
            "Iteration: 2187, loss: -0.8763938177497509\n",
            "Iteration: 2188, loss: -0.8773823429525615\n",
            "Iteration: 2189, loss: -0.8775219041340585\n",
            "Iteration: 2190, loss: -0.8770678369068292\n",
            "Iteration: 2191, loss: -0.8766004985983966\n",
            "Iteration: 2192, loss: -0.876501712145735\n",
            "Iteration: 2193, loss: -0.8769006495800196\n",
            "Iteration: 2194, loss: -0.8777488135927287\n",
            "Iteration: 2195, loss: -0.8786248597151991\n",
            "Iteration: 2196, loss: -0.8793379000306458\n",
            "Iteration: 2197, loss: -0.8797318541961329\n",
            "Iteration: 2198, loss: -0.8798422639250846\n",
            "Iteration: 2199, loss: -0.8798083908711231\n",
            "Iteration: 2200, loss: -0.8797091885037573\n",
            "Iteration: 2201, loss: -0.8796905836231467\n",
            "Iteration: 2202, loss: -0.8798282184545495\n",
            "Iteration: 2203, loss: -0.8801188386739655\n",
            "Iteration: 2204, loss: -0.8805419105613059\n",
            "Iteration: 2205, loss: -0.8810321058689558\n",
            "Iteration: 2206, loss: -0.8815110555188823\n",
            "Iteration: 2207, loss: -0.881962949034659\n",
            "Iteration: 2208, loss: -0.8823519374259541\n",
            "Iteration: 2209, loss: -0.8826998515164007\n",
            "Iteration: 2210, loss: -0.8830062210651728\n",
            "Iteration: 2211, loss: -0.8832540496352058\n",
            "Iteration: 2212, loss: -0.8834840757968875\n",
            "Iteration: 2213, loss: -0.8837101876668412\n",
            "Iteration: 2214, loss: -0.8838881693306659\n",
            "Iteration: 2215, loss: -0.8840458026079877\n",
            "Iteration: 2216, loss: -0.8841562158239675\n",
            "Iteration: 2217, loss: -0.884203595875816\n",
            "Iteration: 2218, loss: -0.8841355309783362\n",
            "Iteration: 2219, loss: -0.8840300915642159\n",
            "Iteration: 2220, loss: -0.883755573875802\n",
            "Iteration: 2221, loss: -0.8834383126262607\n",
            "Iteration: 2222, loss: -0.8830587248030249\n",
            "Iteration: 2223, loss: -0.8827405271736324\n",
            "Iteration: 2224, loss: -0.8824613636698679\n",
            "Iteration: 2225, loss: -0.8824117492036794\n",
            "Iteration: 2226, loss: -0.8826029714764497\n",
            "Iteration: 2227, loss: -0.8831530265898196\n",
            "Iteration: 2228, loss: -0.8839931306292851\n",
            "Iteration: 2229, loss: -0.884911678305681\n",
            "Iteration: 2230, loss: -0.885838498342923\n",
            "Iteration: 2231, loss: -0.8867874882713208\n",
            "Iteration: 2232, loss: -0.8875331171679514\n",
            "Iteration: 2233, loss: -0.8882405419596701\n",
            "Iteration: 2234, loss: -0.8886941614024679\n",
            "Iteration: 2235, loss: -0.8891466763358065\n",
            "Iteration: 2236, loss: -0.8893356983640534\n",
            "Iteration: 2237, loss: -0.8895902280020127\n",
            "Iteration: 2238, loss: -0.8896626314270306\n",
            "Iteration: 2239, loss: -0.8897464834215664\n",
            "Iteration: 2240, loss: -0.8896659425367061\n",
            "Iteration: 2241, loss: -0.8895553439349476\n",
            "Iteration: 2242, loss: -0.8893290498876681\n",
            "Iteration: 2243, loss: -0.8889811122805463\n",
            "Iteration: 2244, loss: -0.8885021131786928\n",
            "Iteration: 2245, loss: -0.887917252864745\n",
            "Iteration: 2246, loss: -0.8873134342328048\n",
            "Iteration: 2247, loss: -0.8867659062776946\n",
            "Iteration: 2248, loss: -0.8862840898607365\n",
            "Iteration: 2249, loss: -0.8861811556819239\n",
            "Iteration: 2250, loss: -0.8864602717438852\n",
            "Iteration: 2251, loss: -0.8872451204529666\n",
            "Iteration: 2252, loss: -0.8883142787858609\n",
            "Iteration: 2253, loss: -0.889764429914254\n",
            "Iteration: 2254, loss: -0.8912587168529323\n",
            "Iteration: 2255, loss: -0.8925485252005855\n",
            "Iteration: 2256, loss: -0.8935455875216435\n",
            "Iteration: 2257, loss: -0.8942659476100892\n",
            "Iteration: 2258, loss: -0.8947375863561533\n",
            "Iteration: 2259, loss: -0.8949858838177465\n",
            "Iteration: 2260, loss: -0.8950732185040786\n",
            "Iteration: 2261, loss: -0.8950331592111447\n",
            "Iteration: 2262, loss: -0.8948931634694579\n",
            "Iteration: 2263, loss: -0.8946629292113163\n",
            "Iteration: 2264, loss: -0.8943019627054348\n",
            "Iteration: 2265, loss: -0.893941777116667\n",
            "Iteration: 2266, loss: -0.893458798054951\n",
            "Iteration: 2267, loss: -0.8929543470057123\n",
            "Iteration: 2268, loss: -0.8923917510027148\n",
            "Iteration: 2269, loss: -0.8920670260473028\n",
            "Iteration: 2270, loss: -0.8919734892891054\n",
            "Iteration: 2271, loss: -0.8922206268864177\n",
            "Iteration: 2272, loss: -0.892807762468849\n",
            "Iteration: 2273, loss: -0.8937902615608861\n",
            "Iteration: 2274, loss: -0.8950056304186512\n",
            "Iteration: 2275, loss: -0.8962848426828857\n",
            "Iteration: 2276, loss: -0.8974476206887013\n",
            "Iteration: 2277, loss: -0.8984820368635983\n",
            "Iteration: 2278, loss: -0.8992352020157375\n",
            "Iteration: 2279, loss: -0.8997876482073954\n",
            "Iteration: 2280, loss: -0.9001057111107243\n",
            "Iteration: 2281, loss: -0.9002912514341384\n",
            "Iteration: 2282, loss: -0.9003800871087512\n",
            "Iteration: 2283, loss: -0.9003551891762152\n",
            "Iteration: 2284, loss: -0.9002842787059826\n",
            "Iteration: 2285, loss: -0.9000963566703527\n",
            "Iteration: 2286, loss: -0.8998168498911889\n",
            "Iteration: 2287, loss: -0.8993666257533375\n",
            "Iteration: 2288, loss: -0.8988661756681356\n",
            "Iteration: 2289, loss: -0.8981883182878722\n",
            "Iteration: 2290, loss: -0.8974048418268262\n",
            "Iteration: 2291, loss: -0.8966665558919461\n",
            "Iteration: 2292, loss: -0.8960475276221977\n",
            "Iteration: 2293, loss: -0.895751916801536\n",
            "Iteration: 2294, loss: -0.8958891291647951\n",
            "Iteration: 2295, loss: -0.8966643933595578\n",
            "Iteration: 2296, loss: -0.8979526570847304\n",
            "Iteration: 2297, loss: -0.8996302044738157\n",
            "Iteration: 2298, loss: -0.9014085320006359\n",
            "Iteration: 2299, loss: -0.9029737699182581\n",
            "Iteration: 2300, loss: -0.904150238434959\n",
            "Iteration: 2301, loss: -0.9049674150299843\n",
            "Iteration: 2302, loss: -0.9053851922599008\n",
            "Iteration: 2303, loss: -0.9055301805700544\n",
            "Iteration: 2304, loss: -0.9054507302955475\n",
            "Iteration: 2305, loss: -0.9052134326307444\n",
            "Iteration: 2306, loss: -0.9048526652816612\n",
            "Iteration: 2307, loss: -0.9044047393150134\n",
            "Iteration: 2308, loss: -0.9038753892799976\n",
            "Iteration: 2309, loss: -0.9034044705498749\n",
            "Iteration: 2310, loss: -0.9029827422421336\n",
            "Iteration: 2311, loss: -0.9026629864157033\n",
            "Iteration: 2312, loss: -0.9025027427680589\n",
            "Iteration: 2313, loss: -0.9026583700381501\n",
            "Iteration: 2314, loss: -0.9031822284389045\n",
            "Iteration: 2315, loss: -0.9039615738814952\n",
            "Iteration: 2316, loss: -0.9049991379550664\n",
            "Iteration: 2317, loss: -0.9061184902006789\n",
            "Iteration: 2318, loss: -0.9072601695598553\n",
            "Iteration: 2319, loss: -0.9082654943908836\n",
            "Iteration: 2320, loss: -0.909072785455307\n",
            "Iteration: 2321, loss: -0.9096980805498865\n",
            "Iteration: 2322, loss: -0.9100640974436827\n",
            "Iteration: 2323, loss: -0.9103378050836065\n",
            "Iteration: 2324, loss: -0.9103903147299185\n",
            "Iteration: 2325, loss: -0.9104405633096287\n",
            "Iteration: 2326, loss: -0.9103531155169087\n",
            "Iteration: 2327, loss: -0.9102035886763207\n",
            "Iteration: 2328, loss: -0.9099014440973376\n",
            "Iteration: 2329, loss: -0.9094198347594603\n",
            "Iteration: 2330, loss: -0.9088072243150175\n",
            "Iteration: 2331, loss: -0.9080426851627301\n",
            "Iteration: 2332, loss: -0.9071872303027879\n",
            "Iteration: 2333, loss: -0.9062164586317168\n",
            "Iteration: 2334, loss: -0.9054486688308829\n",
            "Iteration: 2335, loss: -0.905109746700332\n",
            "Iteration: 2336, loss: -0.9053170092497924\n",
            "Iteration: 2337, loss: -0.9062971791258618\n",
            "Iteration: 2338, loss: -0.9077312755353605\n",
            "Iteration: 2339, loss: -0.9095355104436321\n",
            "Iteration: 2340, loss: -0.9114574167053595\n",
            "Iteration: 2341, loss: -0.913061332924211\n",
            "Iteration: 2342, loss: -0.9142451469887027\n",
            "Iteration: 2343, loss: -0.9149267536166344\n",
            "Iteration: 2344, loss: -0.9151429090310778\n",
            "Iteration: 2345, loss: -0.9150679115619578\n",
            "Iteration: 2346, loss: -0.9147330732575383\n",
            "Iteration: 2347, loss: -0.9142796401905228\n",
            "Iteration: 2348, loss: -0.913770510830624\n",
            "Iteration: 2349, loss: -0.9132819953649075\n",
            "Iteration: 2350, loss: -0.9128561407520609\n",
            "Iteration: 2351, loss: -0.9126305495251941\n",
            "Iteration: 2352, loss: -0.9125738742785772\n",
            "Iteration: 2353, loss: -0.9127505019234613\n",
            "Iteration: 2354, loss: -0.9132316139669212\n",
            "Iteration: 2355, loss: -0.9140411182665281\n",
            "Iteration: 2356, loss: -0.9149603720298439\n",
            "Iteration: 2357, loss: -0.9159555967752104\n",
            "Iteration: 2358, loss: -0.9168811711363999\n",
            "Iteration: 2359, loss: -0.9177143970468606\n",
            "Iteration: 2360, loss: -0.9183995630697401\n",
            "Iteration: 2361, loss: -0.918946328889195\n",
            "Iteration: 2362, loss: -0.9193477401425241\n",
            "Iteration: 2363, loss: -0.9196333905868189\n",
            "Iteration: 2364, loss: -0.9198487592719187\n",
            "Iteration: 2365, loss: -0.9199887544041435\n",
            "Iteration: 2366, loss: -0.9200806828087762\n",
            "Iteration: 2367, loss: -0.9200770371840115\n",
            "Iteration: 2368, loss: -0.9199561482194267\n",
            "Iteration: 2369, loss: -0.9196585349400658\n",
            "Iteration: 2370, loss: -0.9191896815547452\n",
            "Iteration: 2371, loss: -0.9184323354294958\n",
            "Iteration: 2372, loss: -0.9173145602829134\n",
            "Iteration: 2373, loss: -0.9158764689036996\n",
            "Iteration: 2374, loss: -0.9143589926912057\n",
            "Iteration: 2375, loss: -0.9128764796401639\n",
            "Iteration: 2376, loss: -0.9118331228897222\n",
            "Iteration: 2377, loss: -0.9116334681936664\n",
            "Iteration: 2378, loss: -0.912778332830487\n",
            "Iteration: 2379, loss: -0.9150859588459177\n",
            "Iteration: 2380, loss: -0.9180194474472951\n",
            "Iteration: 2381, loss: -0.9208501139924781\n",
            "Iteration: 2382, loss: -0.9229193128749213\n",
            "Iteration: 2383, loss: -0.9240482323384581\n",
            "Iteration: 2384, loss: -0.9242897523721948\n",
            "Iteration: 2385, loss: -0.9239178490706521\n",
            "Iteration: 2386, loss: -0.9231813019860288\n",
            "Iteration: 2387, loss: -0.9222865175394295\n",
            "Iteration: 2388, loss: -0.9214531480205893\n",
            "Iteration: 2389, loss: -0.9209486362758321\n",
            "Iteration: 2390, loss: -0.920937130049359\n",
            "Iteration: 2391, loss: -0.9213903902766547\n",
            "Iteration: 2392, loss: -0.9222635615177472\n",
            "Iteration: 2393, loss: -0.9234184898940684\n",
            "Iteration: 2394, loss: -0.9246859594075238\n",
            "Iteration: 2395, loss: -0.9257777854832734\n",
            "Iteration: 2396, loss: -0.9266408352486285\n",
            "Iteration: 2397, loss: -0.9272079518344786\n",
            "Iteration: 2398, loss: -0.927544680641601\n",
            "Iteration: 2399, loss: -0.9276616129673637\n",
            "Iteration: 2400, loss: -0.9276814342284458\n",
            "Iteration: 2401, loss: -0.9275720789629512\n",
            "Iteration: 2402, loss: -0.9274342966143011\n",
            "Iteration: 2403, loss: -0.9272283111838412\n",
            "Iteration: 2404, loss: -0.9270021942266308\n",
            "Iteration: 2405, loss: -0.9268262092211599\n",
            "Iteration: 2406, loss: -0.9265826401128358\n",
            "Iteration: 2407, loss: -0.9264107944798377\n",
            "Iteration: 2408, loss: -0.926192037443998\n",
            "Iteration: 2409, loss: -0.925967369586147\n",
            "Iteration: 2410, loss: -0.9258496309052141\n",
            "Iteration: 2411, loss: -0.9258627944629448\n",
            "Iteration: 2412, loss: -0.9260367484989267\n",
            "Iteration: 2413, loss: -0.9263557267193756\n",
            "Iteration: 2414, loss: -0.9269726585398412\n",
            "Iteration: 2415, loss: -0.927750945417161\n",
            "Iteration: 2416, loss: -0.9286499167227638\n",
            "Iteration: 2417, loss: -0.9295559059585661\n",
            "Iteration: 2418, loss: -0.9304942258150766\n",
            "Iteration: 2419, loss: -0.9313073247591112\n",
            "Iteration: 2420, loss: -0.9319824148282982\n",
            "Iteration: 2421, loss: -0.9324702862405791\n",
            "Iteration: 2422, loss: -0.932856093136051\n",
            "Iteration: 2423, loss: -0.9330909593680429\n",
            "Iteration: 2424, loss: -0.9332765479066658\n",
            "Iteration: 2425, loss: -0.9333658376844898\n",
            "Iteration: 2426, loss: -0.933436107299837\n",
            "Iteration: 2427, loss: -0.9333922200186534\n",
            "Iteration: 2428, loss: -0.9333342806094633\n",
            "Iteration: 2429, loss: -0.933141617649186\n",
            "Iteration: 2430, loss: -0.9328850301333861\n",
            "Iteration: 2431, loss: -0.9324224026108296\n",
            "Iteration: 2432, loss: -0.9318072929372727\n",
            "Iteration: 2433, loss: -0.9310120649728643\n",
            "Iteration: 2434, loss: -0.929912199211182\n",
            "Iteration: 2435, loss: -0.9287325683512424\n",
            "Iteration: 2436, loss: -0.9276701455664952\n",
            "Iteration: 2437, loss: -0.9269399290041331\n",
            "Iteration: 2438, loss: -0.9267368504784207\n",
            "Iteration: 2439, loss: -0.9273942773678046\n",
            "Iteration: 2440, loss: -0.9290334426927809\n",
            "Iteration: 2441, loss: -0.931353380031672\n",
            "Iteration: 2442, loss: -0.9337523853504396\n",
            "Iteration: 2443, loss: -0.9357489843917265\n",
            "Iteration: 2444, loss: -0.9371910279312494\n",
            "Iteration: 2445, loss: -0.9379312819083323\n",
            "Iteration: 2446, loss: -0.9381362278823909\n",
            "Iteration: 2447, loss: -0.9379140487357808\n",
            "Iteration: 2448, loss: -0.937454188762867\n",
            "Iteration: 2449, loss: -0.9368703441871087\n",
            "Iteration: 2450, loss: -0.9362767176920236\n",
            "Iteration: 2451, loss: -0.9357839204774887\n",
            "Iteration: 2452, loss: -0.9354457096471125\n",
            "Iteration: 2453, loss: -0.9354460791235187\n",
            "Iteration: 2454, loss: -0.9356558138387105\n",
            "Iteration: 2455, loss: -0.9361654957224914\n",
            "Iteration: 2456, loss: -0.9368696863956687\n",
            "Iteration: 2457, loss: -0.9378086855753058\n",
            "Iteration: 2458, loss: -0.9387417045445848\n",
            "Iteration: 2459, loss: -0.9396482896997089\n",
            "Iteration: 2460, loss: -0.9403989694477373\n",
            "Iteration: 2461, loss: -0.9410588105272258\n",
            "Iteration: 2462, loss: -0.9415259866231904\n",
            "Iteration: 2463, loss: -0.9419140530151001\n",
            "Iteration: 2464, loss: -0.9421659420570517\n",
            "Iteration: 2465, loss: -0.942385543977273\n",
            "Iteration: 2466, loss: -0.9425197563437413\n",
            "Iteration: 2467, loss: -0.9426267018921801\n",
            "Iteration: 2468, loss: -0.9426539226805214\n",
            "Iteration: 2469, loss: -0.9426168054205837\n",
            "Iteration: 2470, loss: -0.9424463186276683\n",
            "Iteration: 2471, loss: -0.9421520056778359\n",
            "Iteration: 2472, loss: -0.9416102629289285\n",
            "Iteration: 2473, loss: -0.9407665649945427\n",
            "Iteration: 2474, loss: -0.9395977698544511\n",
            "Iteration: 2475, loss: -0.9381315898679156\n",
            "Iteration: 2476, loss: -0.9364877469648233\n",
            "Iteration: 2477, loss: -0.9350271015687737\n",
            "Iteration: 2478, loss: -0.9342165979510272\n",
            "Iteration: 2479, loss: -0.9344079014630576\n",
            "Iteration: 2480, loss: -0.9358614511585938\n",
            "Iteration: 2481, loss: -0.9381725724561402\n",
            "Iteration: 2482, loss: -0.9410784905380394\n",
            "Iteration: 2483, loss: -0.9436888351029084\n",
            "Iteration: 2484, loss: -0.9456241527402794\n",
            "Iteration: 2485, loss: -0.9466259509991994\n",
            "Iteration: 2486, loss: -0.9467585008626683\n",
            "Iteration: 2487, loss: -0.9463177707326504\n",
            "Iteration: 2488, loss: -0.945474641747114\n",
            "Iteration: 2489, loss: -0.9444325812984963\n",
            "Iteration: 2490, loss: -0.9434964683188014\n",
            "Iteration: 2491, loss: -0.9429401025725607\n",
            "Iteration: 2492, loss: -0.9427741754255213\n",
            "Iteration: 2493, loss: -0.94318898716794\n",
            "Iteration: 2494, loss: -0.9442014884330026\n",
            "Iteration: 2495, loss: -0.9455429304101367\n",
            "Iteration: 2496, loss: -0.9469321729652559\n",
            "Iteration: 2497, loss: -0.9481553563173575\n",
            "Iteration: 2498, loss: -0.9490263677216517\n",
            "Iteration: 2499, loss: -0.9496106917998829\n",
            "Iteration: 2500, loss: -0.9498332878324286\n",
            "Iteration: 2501, loss: -0.9498757194400886\n",
            "Iteration: 2502, loss: -0.9497369457722434\n",
            "Iteration: 2503, loss: -0.9495470282914827\n",
            "Iteration: 2504, loss: -0.9492619041615801\n",
            "Iteration: 2505, loss: -0.9490198852709586\n",
            "Iteration: 2506, loss: -0.9488314012739463\n",
            "Iteration: 2507, loss: -0.9486686438041466\n",
            "Iteration: 2508, loss: -0.9486342655562199\n",
            "Iteration: 2509, loss: -0.9486792912928681\n",
            "Iteration: 2510, loss: -0.9488307304541425\n",
            "Iteration: 2511, loss: -0.9490045535781019\n",
            "Iteration: 2512, loss: -0.9494144349335056\n",
            "Iteration: 2513, loss: -0.9497729799482942\n",
            "Iteration: 2514, loss: -0.9502726346423688\n",
            "Iteration: 2515, loss: -0.9506838284031444\n",
            "Iteration: 2516, loss: -0.9512109137919781\n",
            "Iteration: 2517, loss: -0.9516786344379661\n",
            "Iteration: 2518, loss: -0.9521891405131548\n",
            "Iteration: 2519, loss: -0.9526048929791132\n",
            "Iteration: 2520, loss: -0.9530251371300299\n",
            "Iteration: 2521, loss: -0.9534069771990078\n",
            "Iteration: 2522, loss: -0.9537528363072453\n",
            "Iteration: 2523, loss: -0.9540842444827731\n",
            "Iteration: 2524, loss: -0.9543752727101759\n",
            "Iteration: 2525, loss: -0.9546431736079675\n",
            "Iteration: 2526, loss: -0.9548624838324059\n",
            "Iteration: 2527, loss: -0.955038427987914\n",
            "Iteration: 2528, loss: -0.9551777005961039\n",
            "Iteration: 2529, loss: -0.9552466174997175\n",
            "Iteration: 2530, loss: -0.9552179093453803\n",
            "Iteration: 2531, loss: -0.955033376696475\n",
            "Iteration: 2532, loss: -0.9546185291242725\n",
            "Iteration: 2533, loss: -0.9538883437455945\n",
            "Iteration: 2534, loss: -0.952720590379331\n",
            "Iteration: 2535, loss: -0.9510809598811364\n",
            "Iteration: 2536, loss: -0.9490189918798658\n",
            "Iteration: 2537, loss: -0.9468979512298242\n",
            "Iteration: 2538, loss: -0.9452154661324285\n",
            "Iteration: 2539, loss: -0.9444775944043674\n",
            "Iteration: 2540, loss: -0.9454222781763115\n",
            "Iteration: 2541, loss: -0.9479196883919598\n",
            "Iteration: 2542, loss: -0.9515575391223244\n",
            "Iteration: 2543, loss: -0.955111382399169\n",
            "Iteration: 2544, loss: -0.957848556221909\n",
            "Iteration: 2545, loss: -0.9591459532393928\n",
            "Iteration: 2546, loss: -0.9592080944740674\n",
            "Iteration: 2547, loss: -0.9583403830530931\n",
            "Iteration: 2548, loss: -0.9570340937072642\n",
            "Iteration: 2549, loss: -0.9557484901932933\n",
            "Iteration: 2550, loss: -0.9547868589207552\n",
            "Iteration: 2551, loss: -0.95462173122064\n",
            "Iteration: 2552, loss: -0.9552764358788028\n",
            "Iteration: 2553, loss: -0.9566602238541462\n",
            "Iteration: 2554, loss: -0.9583395974684359\n",
            "Iteration: 2555, loss: -0.9598675620521611\n",
            "Iteration: 2556, loss: -0.9610461506377638\n",
            "Iteration: 2557, loss: -0.9616648652190714\n",
            "Iteration: 2558, loss: -0.9618438788589104\n",
            "Iteration: 2559, loss: -0.961634201169595\n",
            "Iteration: 2560, loss: -0.9612723003763942\n",
            "Iteration: 2561, loss: -0.9608715126469796\n",
            "Iteration: 2562, loss: -0.960581270234435\n",
            "Iteration: 2563, loss: -0.9604260753286511\n",
            "Iteration: 2564, loss: -0.9604935155047939\n",
            "Iteration: 2565, loss: -0.9607250020842509\n",
            "Iteration: 2566, loss: -0.9611916134670676\n",
            "Iteration: 2567, loss: -0.9616936928023924\n",
            "Iteration: 2568, loss: -0.9623320494096322\n",
            "Iteration: 2569, loss: -0.962874660054197\n",
            "Iteration: 2570, loss: -0.963424008828403\n",
            "Iteration: 2571, loss: -0.9639000214026845\n",
            "Iteration: 2572, loss: -0.9643408784826022\n",
            "Iteration: 2573, loss: -0.9646996519406891\n",
            "Iteration: 2574, loss: -0.9649946107585898\n",
            "Iteration: 2575, loss: -0.9652894130432006\n",
            "Iteration: 2576, loss: -0.9655315196606189\n",
            "Iteration: 2577, loss: -0.9657785001587915\n",
            "Iteration: 2578, loss: -0.9660023305732038\n",
            "Iteration: 2579, loss: -0.9662245357484899\n",
            "Iteration: 2580, loss: -0.9664372056396694\n",
            "Iteration: 2581, loss: -0.9666423272492889\n",
            "Iteration: 2582, loss: -0.9668577634183099\n",
            "Iteration: 2583, loss: -0.9670560567590359\n",
            "Iteration: 2584, loss: -0.9672751131829012\n",
            "Iteration: 2585, loss: -0.9674710942266949\n",
            "Iteration: 2586, loss: -0.9676562214629978\n",
            "Iteration: 2587, loss: -0.9678493494993504\n",
            "Iteration: 2588, loss: -0.9680034081880238\n",
            "Iteration: 2589, loss: -0.9681236386772802\n",
            "Iteration: 2590, loss: -0.9681865442593661\n",
            "Iteration: 2591, loss: -0.9680616614474266\n",
            "Iteration: 2592, loss: -0.967642768319462\n",
            "Iteration: 2593, loss: -0.9667821846431978\n",
            "Iteration: 2594, loss: -0.9651559207465955\n",
            "Iteration: 2595, loss: -0.9625370311020384\n",
            "Iteration: 2596, loss: -0.9589197313510495\n",
            "Iteration: 2597, loss: -0.9543312952715646\n",
            "Iteration: 2598, loss: -0.9496667789280953\n",
            "Iteration: 2599, loss: -0.9465817305491885\n",
            "Iteration: 2600, loss: -0.9469159235747943\n",
            "Iteration: 2601, loss: -0.9518029927810576\n",
            "Iteration: 2602, loss: -0.9593496538278462\n",
            "Iteration: 2603, loss: -0.9666930815881577\n",
            "Iteration: 2604, loss: -0.9707650326221239\n",
            "Iteration: 2605, loss: -0.9706149541987678\n",
            "Iteration: 2606, loss: -0.9674451806201084\n",
            "Iteration: 2607, loss: -0.9632819500668722\n",
            "Iteration: 2608, loss: -0.9605072867441132\n",
            "Iteration: 2609, loss: -0.9606613753234351\n",
            "Iteration: 2610, loss: -0.9638541947221612\n",
            "Iteration: 2611, loss: -0.9682293199080133\n",
            "Iteration: 2612, loss: -0.971703072931261\n",
            "Iteration: 2613, loss: -0.9729241106164666\n",
            "Iteration: 2614, loss: -0.9719488795380314\n",
            "Iteration: 2615, loss: -0.9699044746526732\n",
            "Iteration: 2616, loss: -0.9681918521536783\n",
            "Iteration: 2617, loss: -0.9678309878724153\n",
            "Iteration: 2618, loss: -0.968961528157678\n",
            "Iteration: 2619, loss: -0.9710884674457683\n",
            "Iteration: 2620, loss: -0.9731734475895915\n",
            "Iteration: 2621, loss: -0.9744341070843979\n",
            "Iteration: 2622, loss: -0.9745753350158154\n",
            "Iteration: 2623, loss: -0.9738850608443468\n",
            "Iteration: 2624, loss: -0.9729835271849022\n",
            "Iteration: 2625, loss: -0.9723933680229916\n",
            "Iteration: 2626, loss: -0.9724539880104018\n",
            "Iteration: 2627, loss: -0.9731511142957016\n",
            "Iteration: 2628, loss: -0.9742434591087217\n",
            "Iteration: 2629, loss: -0.9753354785662819\n",
            "Iteration: 2630, loss: -0.9761267998309117\n",
            "Iteration: 2631, loss: -0.9765159112296529\n",
            "Iteration: 2632, loss: -0.9765562106291037\n",
            "Iteration: 2633, loss: -0.9763867510949097\n",
            "Iteration: 2634, loss: -0.9761551788934086\n",
            "Iteration: 2635, loss: -0.975990554131422\n",
            "Iteration: 2636, loss: -0.9760013138353029\n",
            "Iteration: 2637, loss: -0.9762021134471854\n",
            "Iteration: 2638, loss: -0.9765146297202988\n",
            "Iteration: 2639, loss: -0.9769881342349543\n",
            "Iteration: 2640, loss: -0.9774470808981917\n",
            "Iteration: 2641, loss: -0.9779416443792761\n",
            "Iteration: 2642, loss: -0.9783503871585483\n",
            "Iteration: 2643, loss: -0.9787046245885425\n",
            "Iteration: 2644, loss: -0.97900827095775\n",
            "Iteration: 2645, loss: -0.9792434281452972\n",
            "Iteration: 2646, loss: -0.9794516652215799\n",
            "Iteration: 2647, loss: -0.9796095939952213\n",
            "Iteration: 2648, loss: -0.9797437187239175\n",
            "Iteration: 2649, loss: -0.9798642444854169\n",
            "Iteration: 2650, loss: -0.9799476619869514\n",
            "Iteration: 2651, loss: -0.9799962470423806\n",
            "Iteration: 2652, loss: -0.9799720320339969\n",
            "Iteration: 2653, loss: -0.9798620126389552\n",
            "Iteration: 2654, loss: -0.9796176510966205\n",
            "Iteration: 2655, loss: -0.9792429625672027\n",
            "Iteration: 2656, loss: -0.9787936563750255\n",
            "Iteration: 2657, loss: -0.9782261937943691\n",
            "Iteration: 2658, loss: -0.9776428225377413\n",
            "Iteration: 2659, loss: -0.977086866702053\n",
            "Iteration: 2660, loss: -0.9766387844676875\n",
            "Iteration: 2661, loss: -0.9765861228612684\n",
            "Iteration: 2662, loss: -0.9770265443613351\n",
            "Iteration: 2663, loss: -0.9777797730872789\n",
            "Iteration: 2664, loss: -0.9788450193152012\n",
            "Iteration: 2665, loss: -0.9799964815687292\n",
            "Iteration: 2666, loss: -0.9812189342327259\n",
            "Iteration: 2667, loss: -0.9822200321506109\n",
            "Iteration: 2668, loss: -0.9830322059294058\n",
            "Iteration: 2669, loss: -0.9836085103610227\n",
            "Iteration: 2670, loss: -0.9839840143504769\n",
            "Iteration: 2671, loss: -0.9842399435165595\n",
            "Iteration: 2672, loss: -0.984354879178072\n",
            "Iteration: 2673, loss: -0.9844306441842323\n",
            "Iteration: 2674, loss: -0.9843693212112276\n",
            "Iteration: 2675, loss: -0.9843154113462553\n",
            "Iteration: 2676, loss: -0.984148927550546\n",
            "Iteration: 2677, loss: -0.9839262832722065\n",
            "Iteration: 2678, loss: -0.9836056715936332\n",
            "Iteration: 2679, loss: -0.9832289197105574\n",
            "Iteration: 2680, loss: -0.9827551354829748\n",
            "Iteration: 2681, loss: -0.9821992462399565\n",
            "Iteration: 2682, loss: -0.9816107309500612\n",
            "Iteration: 2683, loss: -0.9810920842610241\n",
            "Iteration: 2684, loss: -0.9807456786888815\n",
            "Iteration: 2685, loss: -0.9808206767238521\n",
            "Iteration: 2686, loss: -0.9811243700371478\n",
            "Iteration: 2687, loss: -0.9819303417581081\n",
            "Iteration: 2688, loss: -0.9829535317039193\n",
            "Iteration: 2689, loss: -0.9842652173579016\n",
            "Iteration: 2690, loss: -0.9853879471383339\n",
            "Iteration: 2691, loss: -0.9864835072439417\n",
            "Iteration: 2692, loss: -0.9873310177493567\n",
            "Iteration: 2693, loss: -0.9880248082213912\n",
            "Iteration: 2694, loss: -0.9884598811319026\n",
            "Iteration: 2695, loss: -0.9887722784332502\n",
            "Iteration: 2696, loss: -0.9889270269770534\n",
            "Iteration: 2697, loss: -0.9889937192505501\n",
            "Iteration: 2698, loss: -0.9889824264324486\n",
            "Iteration: 2699, loss: -0.9888933759235261\n",
            "Iteration: 2700, loss: -0.988760615616049\n",
            "Iteration: 2701, loss: -0.98854577693852\n",
            "Iteration: 2702, loss: -0.9882496855120008\n",
            "Iteration: 2703, loss: -0.9878286733669586\n",
            "Iteration: 2704, loss: -0.9872741248574576\n",
            "Iteration: 2705, loss: -0.9866541700785229\n",
            "Iteration: 2706, loss: -0.9859767397828146\n",
            "Iteration: 2707, loss: -0.9852806855232316\n",
            "Iteration: 2708, loss: -0.9847394442245595\n",
            "Iteration: 2709, loss: -0.9844601973518624\n",
            "Iteration: 2710, loss: -0.9846077295753547\n",
            "Iteration: 2711, loss: -0.9850926923950151\n",
            "Iteration: 2712, loss: -0.9860254478309982\n",
            "Iteration: 2713, loss: -0.9873415685514235\n",
            "Iteration: 2714, loss: -0.9887922576809457\n",
            "Iteration: 2715, loss: -0.9901650857276566\n",
            "Iteration: 2716, loss: -0.9914017590374931\n",
            "Iteration: 2717, loss: -0.9922966128612531\n",
            "Iteration: 2718, loss: -0.9929024647597754\n",
            "Iteration: 2719, loss: -0.9932572271825101\n",
            "Iteration: 2720, loss: -0.9933771540915368\n",
            "Iteration: 2721, loss: -0.9933778354139761\n",
            "Iteration: 2722, loss: -0.9932427335624066\n",
            "Iteration: 2723, loss: -0.9930464852497644\n",
            "Iteration: 2724, loss: -0.9927564344220209\n",
            "Iteration: 2725, loss: -0.9924328197434267\n",
            "Iteration: 2726, loss: -0.9920339555959786\n",
            "Iteration: 2727, loss: -0.9915958129854099\n",
            "Iteration: 2728, loss: -0.9911589900933966\n",
            "Iteration: 2729, loss: -0.990787169047868\n",
            "Iteration: 2730, loss: -0.9904498535989357\n",
            "Iteration: 2731, loss: -0.9902744320951743\n",
            "Iteration: 2732, loss: -0.9903359282218833\n",
            "Iteration: 2733, loss: -0.9906332995857475\n",
            "Iteration: 2734, loss: -0.9910799694214802\n",
            "Iteration: 2735, loss: -0.9918811529781302\n",
            "Iteration: 2736, loss: -0.9927823817145982\n",
            "Iteration: 2737, loss: -0.9938796015001542\n",
            "Iteration: 2738, loss: -0.9947922073204315\n",
            "Iteration: 2739, loss: -0.9957298073905421\n",
            "Iteration: 2740, loss: -0.9963872768846697\n",
            "Iteration: 2741, loss: -0.9969996828306291\n",
            "Iteration: 2742, loss: -0.9973661784333395\n",
            "Iteration: 2743, loss: -0.9976346951224341\n",
            "Iteration: 2744, loss: -0.9978353493085483\n",
            "Iteration: 2745, loss: -0.9979348942747033\n",
            "Iteration: 2746, loss: -0.9979954243035636\n",
            "Iteration: 2747, loss: -0.9980159478406299\n",
            "Iteration: 2748, loss: -0.9979516496503149\n",
            "Iteration: 2749, loss: -0.9978730741824797\n",
            "Iteration: 2750, loss: -0.9976754902364191\n",
            "Iteration: 2751, loss: -0.9974064840315253\n",
            "Iteration: 2752, loss: -0.9969726925172364\n",
            "Iteration: 2753, loss: -0.9963941332548956\n",
            "Iteration: 2754, loss: -0.9955744513336113\n",
            "Iteration: 2755, loss: -0.9945076565525037\n",
            "Iteration: 2756, loss: -0.9933467160267617\n",
            "Iteration: 2757, loss: -0.9920367942475803\n",
            "Iteration: 2758, loss: -0.9910483563694793\n",
            "Iteration: 2759, loss: -0.9903648737739575\n",
            "Iteration: 2760, loss: -0.9905396556690643\n",
            "Iteration: 2761, loss: -0.9916129892766935\n",
            "Iteration: 2762, loss: -0.9936007844979377\n",
            "Iteration: 2763, loss: -0.9960183391268038\n",
            "Iteration: 2764, loss: -0.9984337971727134\n",
            "Iteration: 2765, loss: -1.0003228310935153\n",
            "Iteration: 2766, loss: -1.001521821087295\n",
            "Iteration: 2767, loss: -1.0020321072068243\n",
            "Iteration: 2768, loss: -1.0019904242594182\n",
            "Iteration: 2769, loss: -1.0015509648493572\n",
            "Iteration: 2770, loss: -1.0008903918124854\n",
            "Iteration: 2771, loss: -1.0001282149608786\n",
            "Iteration: 2772, loss: -0.9993927227165263\n",
            "Iteration: 2773, loss: -0.9988626871447663\n",
            "Iteration: 2774, loss: -0.9986273170400422\n",
            "Iteration: 2775, loss: -0.9987927815433248\n",
            "Iteration: 2776, loss: -0.9992797741949646\n",
            "Iteration: 2777, loss: -1.0001450816663067\n",
            "Iteration: 2778, loss: -1.0011850515380392\n",
            "Iteration: 2779, loss: -1.0022683815524311\n",
            "Iteration: 2780, loss: -1.0032283874333525\n",
            "Iteration: 2781, loss: -1.0039947792879047\n",
            "Iteration: 2782, loss: -1.0045632354874312\n",
            "Iteration: 2783, loss: -1.0049088735491192\n",
            "Iteration: 2784, loss: -1.005111004175172\n",
            "Iteration: 2785, loss: -1.0051899060293217\n",
            "Iteration: 2786, loss: -1.0051912676028896\n",
            "Iteration: 2787, loss: -1.0051061417844167\n",
            "Iteration: 2788, loss: -1.0050017648914518\n",
            "Iteration: 2789, loss: -1.0047932089489722\n",
            "Iteration: 2790, loss: -1.004559225885009\n",
            "Iteration: 2791, loss: -1.0042343203892026\n",
            "Iteration: 2792, loss: -1.0038542401976893\n",
            "Iteration: 2793, loss: -1.003409200903298\n",
            "Iteration: 2794, loss: -1.0029106179907974\n",
            "Iteration: 2795, loss: -1.002362016082837\n",
            "Iteration: 2796, loss: -1.0018915477805803\n",
            "Iteration: 2797, loss: -1.0014850692733892\n",
            "Iteration: 2798, loss: -1.0013729096726809\n",
            "Iteration: 2799, loss: -1.0015873811442244\n",
            "Iteration: 2800, loss: -1.002121896967112\n",
            "Iteration: 2801, loss: -1.0029676022899925\n",
            "Iteration: 2802, loss: -1.0041194159966653\n",
            "Iteration: 2803, loss: -1.005277502330928\n",
            "Iteration: 2804, loss: -1.006429744209993\n",
            "Iteration: 2805, loss: -1.007457977548093\n",
            "Iteration: 2806, loss: -1.0083082290564729\n",
            "Iteration: 2807, loss: -1.0089158016622677\n",
            "Iteration: 2808, loss: -1.0093307462366523\n",
            "Iteration: 2809, loss: -1.0095741806455008\n",
            "Iteration: 2810, loss: -1.0096892889629712\n",
            "Iteration: 2811, loss: -1.0097357939664142\n",
            "Iteration: 2812, loss: -1.0096816101423725\n",
            "Iteration: 2813, loss: -1.0095893768469615\n",
            "Iteration: 2814, loss: -1.0094241902137504\n",
            "Iteration: 2815, loss: -1.009202804035736\n",
            "Iteration: 2816, loss: -1.0088873485538241\n",
            "Iteration: 2817, loss: -1.0084917712531425\n",
            "Iteration: 2818, loss: -1.0080032992005286\n",
            "Iteration: 2819, loss: -1.0073892981506694\n",
            "Iteration: 2820, loss: -1.0067506612339676\n",
            "Iteration: 2821, loss: -1.0061290405924017\n",
            "Iteration: 2822, loss: -1.0056361244306142\n",
            "Iteration: 2823, loss: -1.005422090002029\n",
            "Iteration: 2824, loss: -1.0055093018524972\n",
            "Iteration: 2825, loss: -1.0060204425232677\n",
            "Iteration: 2826, loss: -1.0067978960154997\n",
            "Iteration: 2827, loss: -1.0078620685075799\n",
            "Iteration: 2828, loss: -1.0091285751217132\n",
            "Iteration: 2829, loss: -1.010474408553148\n",
            "Iteration: 2830, loss: -1.0116574309799362\n",
            "Iteration: 2831, loss: -1.0126121242806008\n",
            "Iteration: 2832, loss: -1.0133141294584973\n",
            "Iteration: 2833, loss: -1.0137705797532648\n",
            "Iteration: 2834, loss: -1.0140034639873432\n",
            "Iteration: 2835, loss: -1.014113258765765\n",
            "Iteration: 2836, loss: -1.0140951486150822\n",
            "Iteration: 2837, loss: -1.0140001616653092\n",
            "Iteration: 2838, loss: -1.0138305861117975\n",
            "Iteration: 2839, loss: -1.0136211551173435\n",
            "Iteration: 2840, loss: -1.013340585926885\n",
            "Iteration: 2841, loss: -1.0130387706871251\n",
            "Iteration: 2842, loss: -1.0126557075957754\n",
            "Iteration: 2843, loss: -1.0122289512769669\n",
            "Iteration: 2844, loss: -1.0117478802717108\n",
            "Iteration: 2845, loss: -1.0112965604028126\n",
            "Iteration: 2846, loss: -1.010929461143827\n",
            "Iteration: 2847, loss: -1.0106837911124065\n",
            "Iteration: 2848, loss: -1.0106912900302047\n",
            "Iteration: 2849, loss: -1.010908778118035\n",
            "Iteration: 2850, loss: -1.0114478764011698\n",
            "Iteration: 2851, loss: -1.0122149812278995\n",
            "Iteration: 2852, loss: -1.0131323180098355\n",
            "Iteration: 2853, loss: -1.0142472479967894\n",
            "Iteration: 2854, loss: -1.0153259573150908\n",
            "Iteration: 2855, loss: -1.0163033442954872\n",
            "Iteration: 2856, loss: -1.017079637634681\n",
            "Iteration: 2857, loss: -1.0177104562055965\n",
            "Iteration: 2858, loss: -1.018125109441\n",
            "Iteration: 2859, loss: -1.018386629511316\n",
            "Iteration: 2860, loss: -1.0184968243769885\n",
            "Iteration: 2861, loss: -1.018568752650407\n",
            "Iteration: 2862, loss: -1.0185064707278535\n",
            "Iteration: 2863, loss: -1.0184572349176568\n",
            "Iteration: 2864, loss: -1.0182608123828596\n",
            "Iteration: 2865, loss: -1.0180792425223772\n",
            "Iteration: 2866, loss: -1.0177631571943675\n",
            "Iteration: 2867, loss: -1.017365140496786\n",
            "Iteration: 2868, loss: -1.0168220569693287\n",
            "Iteration: 2869, loss: -1.0161157797392844\n",
            "Iteration: 2870, loss: -1.0153769520675944\n",
            "Iteration: 2871, loss: -1.0144879812202738\n",
            "Iteration: 2872, loss: -1.0135908145621577\n",
            "Iteration: 2873, loss: -1.012887804237651\n",
            "Iteration: 2874, loss: -1.0125006536046695\n",
            "Iteration: 2875, loss: -1.0125985657044252\n",
            "Iteration: 2876, loss: -1.013287746576062\n",
            "Iteration: 2877, loss: -1.014665237258249\n",
            "Iteration: 2878, loss: -1.0164175525602523\n",
            "Iteration: 2879, loss: -1.0182645208661738\n",
            "Iteration: 2880, loss: -1.019995993597718\n",
            "Iteration: 2881, loss: -1.021293706458562\n",
            "Iteration: 2882, loss: -1.0221652201398608\n",
            "Iteration: 2883, loss: -1.0225512147504243\n",
            "Iteration: 2884, loss: -1.0225593009290006\n",
            "Iteration: 2885, loss: -1.0223271079034972\n",
            "Iteration: 2886, loss: -1.0219112148517062\n",
            "Iteration: 2887, loss: -1.0214384453848404\n",
            "Iteration: 2888, loss: -1.0209579319570807\n",
            "Iteration: 2889, loss: -1.0204938650185853\n",
            "Iteration: 2890, loss: -1.020096655450513\n",
            "Iteration: 2891, loss: -1.0198730430293805\n",
            "Iteration: 2892, loss: -1.0198242913267261\n",
            "Iteration: 2893, loss: -1.0200497450961914\n",
            "Iteration: 2894, loss: -1.020510977894735\n",
            "Iteration: 2895, loss: -1.0211386570099346\n",
            "Iteration: 2896, loss: -1.0218465753857433\n",
            "Iteration: 2897, loss: -1.022647569691898\n",
            "Iteration: 2898, loss: -1.0234272432953282\n",
            "Iteration: 2899, loss: -1.0241560901484856\n",
            "Iteration: 2900, loss: -1.0247725577190934\n",
            "Iteration: 2901, loss: -1.0252674317134909\n",
            "Iteration: 2902, loss: -1.02561554908327\n",
            "Iteration: 2903, loss: -1.025938166918459\n",
            "Iteration: 2904, loss: -1.0261056199319392\n",
            "Iteration: 2905, loss: -1.0263102648903\n",
            "Iteration: 2906, loss: -1.0263796524548439\n",
            "Iteration: 2907, loss: -1.0264717777360015\n",
            "Iteration: 2908, loss: -1.0264392868835779\n",
            "Iteration: 2909, loss: -1.0263982155378402\n",
            "Iteration: 2910, loss: -1.026233538358129\n",
            "Iteration: 2911, loss: -1.0259575977937643\n",
            "Iteration: 2912, loss: -1.025462207367449\n",
            "Iteration: 2913, loss: -1.0247897468156988\n",
            "Iteration: 2914, loss: -1.0238471517102985\n",
            "Iteration: 2915, loss: -1.0226514225344723\n",
            "Iteration: 2916, loss: -1.02111351273256\n",
            "Iteration: 2917, loss: -1.0195422467933948\n",
            "Iteration: 2918, loss: -1.0180473473380947\n",
            "Iteration: 2919, loss: -1.017264565133469\n",
            "Iteration: 2920, loss: -1.0172615358128312\n",
            "Iteration: 2921, loss: -1.018441758758842\n",
            "Iteration: 2922, loss: -1.0205500428423249\n",
            "Iteration: 2923, loss: -1.0234677961619891\n",
            "Iteration: 2924, loss: -1.0261654697013367\n",
            "Iteration: 2925, loss: -1.0282990153922829\n",
            "Iteration: 2926, loss: -1.0295526032251472\n",
            "Iteration: 2927, loss: -1.0299622067330665\n",
            "Iteration: 2928, loss: -1.0296862632169568\n",
            "Iteration: 2929, loss: -1.0290221607231727\n",
            "Iteration: 2930, loss: -1.0281428120557197\n",
            "Iteration: 2931, loss: -1.0273117332607549\n",
            "Iteration: 2932, loss: -1.0266864497423558\n",
            "Iteration: 2933, loss: -1.0264671868728887\n",
            "Iteration: 2934, loss: -1.0266657960806729\n",
            "Iteration: 2935, loss: -1.0272484060262808\n",
            "Iteration: 2936, loss: -1.0281396837786843\n",
            "Iteration: 2937, loss: -1.0292076578048173\n",
            "Iteration: 2938, loss: -1.0302498595061016\n",
            "Iteration: 2939, loss: -1.031113367162947\n",
            "Iteration: 2940, loss: -1.0318104378127122\n",
            "Iteration: 2941, loss: -1.0322612863606515\n",
            "Iteration: 2942, loss: -1.0325419975415653\n",
            "Iteration: 2943, loss: -1.032676124127455\n",
            "Iteration: 2944, loss: -1.032719916858051\n",
            "Iteration: 2945, loss: -1.0326527071104203\n",
            "Iteration: 2946, loss: -1.0325524763824843\n",
            "Iteration: 2947, loss: -1.0323875753741185\n",
            "Iteration: 2948, loss: -1.0321642253927426\n",
            "Iteration: 2949, loss: -1.0319499120536892\n",
            "Iteration: 2950, loss: -1.0316489745222657\n",
            "Iteration: 2951, loss: -1.0313269247379317\n",
            "Iteration: 2952, loss: -1.0309683041956248\n",
            "Iteration: 2953, loss: -1.0305995648138477\n",
            "Iteration: 2954, loss: -1.0302469273353223\n",
            "Iteration: 2955, loss: -1.0298510242579517\n",
            "Iteration: 2956, loss: -1.0297323562072802\n",
            "Iteration: 2957, loss: -1.0297232299743997\n",
            "Iteration: 2958, loss: -1.0299370465687339\n",
            "Iteration: 2959, loss: -1.030268406975836\n",
            "Iteration: 2960, loss: -1.030960621936136\n",
            "Iteration: 2961, loss: -1.0317285146690796\n",
            "Iteration: 2962, loss: -1.032608731899471\n",
            "Iteration: 2963, loss: -1.0335250307788038\n",
            "Iteration: 2964, loss: -1.0344493017666687\n",
            "Iteration: 2965, loss: -1.0352121178093459\n",
            "Iteration: 2966, loss: -1.0358371299860463\n",
            "Iteration: 2967, loss: -1.0363633110275605\n",
            "Iteration: 2968, loss: -1.0367337960907876\n",
            "Iteration: 2969, loss: -1.0370240196279767\n",
            "Iteration: 2970, loss: -1.0372475532175978\n",
            "Iteration: 2971, loss: -1.0374079988880822\n",
            "Iteration: 2972, loss: -1.0375301728996797\n",
            "Iteration: 2973, loss: -1.0376410698645278\n",
            "Iteration: 2974, loss: -1.0377186006473318\n",
            "Iteration: 2975, loss: -1.0377244697066221\n",
            "Iteration: 2976, loss: -1.037682754980593\n",
            "Iteration: 2977, loss: -1.037499605704968\n",
            "Iteration: 2978, loss: -1.0371611107085272\n",
            "Iteration: 2979, loss: -1.036651283068908\n",
            "Iteration: 2980, loss: -1.0358416120788858\n",
            "Iteration: 2981, loss: -1.0346752587078234\n",
            "Iteration: 2982, loss: -1.0331144588095957\n",
            "Iteration: 2983, loss: -1.0313167447235931\n",
            "Iteration: 2984, loss: -1.0294632396059482\n",
            "Iteration: 2985, loss: -1.027838649872588\n",
            "Iteration: 2986, loss: -1.0272264562681135\n",
            "Iteration: 2987, loss: -1.0277310181987194\n",
            "Iteration: 2988, loss: -1.0297335190504187\n",
            "Iteration: 2989, loss: -1.0324065637410147\n",
            "Iteration: 2990, loss: -1.0355260756056723\n",
            "Iteration: 2991, loss: -1.0381983794087495\n",
            "Iteration: 2992, loss: -1.0400830618055625\n",
            "Iteration: 2993, loss: -1.0408860024711712\n",
            "Iteration: 2994, loss: -1.0409274795995342\n",
            "Iteration: 2995, loss: -1.040315719492203\n",
            "Iteration: 2996, loss: -1.039429424213762\n",
            "Iteration: 2997, loss: -1.038428018218214\n",
            "Iteration: 2998, loss: -1.037637771999801\n",
            "Iteration: 2999, loss: -1.0372495121969418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQX0BJ6tdUjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, mu, sigma = model(data_test.query, data_test.num_total_points,\n",
        "                     data_test.num_context_points)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.initializers.global_variables().run()\n",
        "predictions = mu[0,:,0].eval()\n",
        "true_y = y_test_1.eval()\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Ru78BrBMHR",
        "colab_type": "code",
        "outputId": "ae987842-639e-476d-8f0e-d1aa45a206f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#r2 of splicing predictions on test dataset\n",
        "print('predictions: ' + str(predictions))\n",
        "print('true y values: ' + str(true_y))\n",
        "print('mean squared error: ' + str(mean_squared_error(true_y, predictions)))\n",
        "print('r2: ' + str(r2_score(true_y, predictions)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions: [0.1917588  0.17465036 0.40791608 ... 0.35786017 0.47494458 0.27357091]\n",
            "true y values: [1.         1.         0.75       ... 0.97727273 0.05882353 0.        ]\n",
            "mean squared error: 0.19148582632827937\n",
            "r2: -0.34211512959374346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r01YAa4ccB1I",
        "colab_type": "code",
        "outputId": "940567cb-165b-4b80-b68e-d3400a4ae111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Calculating MSE when predicting all 0's as a Benchmark\n",
        "test_zeroes = np.zeros(predictions.shape[0])\n",
        "print('mean squared error when predicting exclusively 0: ' + str(mean_squared_error(true_y, test_zeroes)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean squared error when predicting exclusively 0: 0.24100631597304542\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}